{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from json import loads\n",
    "from re import findall,UNICODE\n",
    "import sys\n",
    "sys.path.append(\"/Users/andyreagan/tools/python\")\n",
    "from kitchentable.dogtoys import *\n",
    "from labMTsimple.labMTsimple.speedy import *\n",
    "my_labMT = LabMT()\n",
    "from labMTsimple.labMTsimple.storyLab import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"id', 'polarity', 'tweet\"']\n"
     ]
    }
   ],
   "source": [
    "f = open(\"../data/twitter-labeled-tweets/scrounged/STS-Gold/sts_gold_tweet.csv\",\"r\")\n",
    "print(f.readline().rstrip().split(\"\\\";\\\"\"))\n",
    "pos_tweets = []\n",
    "neg_tweets = []\n",
    "for line in f:\n",
    "    a = line.rstrip().split(\"\\\";\\\"\")\n",
    "    if not len(a) == 3:\n",
    "        print(a)\n",
    "    tweet_id = a[0].lstrip(\"\\\"\")\n",
    "    tweet_pol = a[1]\n",
    "    tweet_text = a[2].rstrip(\"\\\"\")\n",
    "    if int(tweet_pol) == 0:\n",
    "        neg_tweets.append(tweet_text)\n",
    "    elif int(tweet_pol) == 4:\n",
    "        pos_tweets.append(tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "632"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1402"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@ mcdonalds with my litto sis aka cuzin lol cristyyyyy ',\n",
       " '@AnnaSaccone Love your new cards!   I would definitely hire you ;).',\n",
       " \"@supricky06 that was one of the most enjoyable experiences I've had on YouTube. Well done  http://bit.ly/1a7zPw\",\n",
       " 'Dallas vegas goodness  http://twitpic.com/3lzt1 On my way to to the SusCon ',\n",
       " '@JBsFanArgentina Hey I luv this pic!!! was amazing of the last CHAT of The JB in FACEBOOK! ',\n",
       " \"@llmatticusll yuuuup! 9 miles  it's for cancer! for a good deed!\",\n",
       " \"@seanmurphymusic I'am in love with Taylor Swift's &quot;Crazier&quot; and I think you should definitely do a cover of it, it'll be beautiful  &lt;3\",\n",
       " 'Had a piece of fried chicken, some PSP luv and now off to bed. ',\n",
       " \"@mileycyrus BRAZIL LOVES U SO MUCH!! YOU ARE AWESOME, MILEY! COME TO BRAZIL.. WE CAN'T FOR THIS DAY COME \",\n",
       " \"@SilMuri yeah that'd be awesome... You're gonna be the new Oprah! Freakin' yeah  *ninja-rolls over to Sil to eat chocolate*\",\n",
       " \"@DanniAsheOnline Awwww she called me sexy  nice! you rock Danni! you just made my night gurl! muah! I'll give you props anytime! \",\n",
       " \"@McChelsea get a glass of wine, a nice book, and just chillax out there.  oh wait, it's seattle, i'll probably rain. hah \",\n",
       " \"Had to happen, @Oprah is on twitter, and only after 24 hours, she's got 260,371 followers and counting...  You go girlfriend!\",\n",
       " 'THE LAKERS ARE SO GOING TO WIN AND ADVANCE TO THE FINALS ',\n",
       " '@lightinthesky wow. enjoy! ',\n",
       " 'i drew a cute baby zebra i think he is my favorite so far ',\n",
       " 'Went to Calgary with some of the lads (Gaspar had to sit in the boot  ) to watch the Flames versus LA Kings. Calgary won 4-1. Nice night.',\n",
       " '@batistini21 lols you go for the cavs. orlando owned them today ',\n",
       " '@CCArquette ps. please try to see if you get Cougar Town to be broadcast internationally!!! im brazilian but such a great fan of yours! ',\n",
       " 'I L&lt;3VE Taylor Swift.She is just so amazing and her music is wonderfull and makes me so happy. ']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tweets[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['@',\n",
       "  'mcdonalds',\n",
       "  'with',\n",
       "  'my',\n",
       "  'litto',\n",
       "  'sis',\n",
       "  'aka',\n",
       "  'cuzin',\n",
       "  'lol',\n",
       "  'cristyyyyy'],\n",
       " ['@annasaccone',\n",
       "  'love',\n",
       "  'your',\n",
       "  'new',\n",
       "  'cards',\n",
       "  'i',\n",
       "  'would',\n",
       "  'definitely',\n",
       "  'hire',\n",
       "  'you',\n",
       "  ';)'],\n",
       " ['@supricky06',\n",
       "  'that',\n",
       "  'was',\n",
       "  'one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'most',\n",
       "  'enjoyable',\n",
       "  'experiences',\n",
       "  \"i've\",\n",
       "  'had',\n",
       "  'on',\n",
       "  'youtube',\n",
       "  'well',\n",
       "  'done',\n",
       "  'http://bit.ly/1a7zpw'],\n",
       " ['dallas',\n",
       "  'vegas',\n",
       "  'goodness',\n",
       "  'http://twitpic.com/3lzt1',\n",
       "  'on',\n",
       "  'my',\n",
       "  'way',\n",
       "  'to',\n",
       "  'to',\n",
       "  'the',\n",
       "  'suscon'],\n",
       " ['@jbsfanargentina',\n",
       "  'hey',\n",
       "  'i',\n",
       "  'luv',\n",
       "  'this',\n",
       "  'pic',\n",
       "  'was',\n",
       "  'amazing',\n",
       "  'of',\n",
       "  'the',\n",
       "  'last',\n",
       "  'chat',\n",
       "  'of',\n",
       "  'the',\n",
       "  'jb',\n",
       "  'in',\n",
       "  'facebook'],\n",
       " ['@llmatticusll',\n",
       "  'yuuuup',\n",
       "  '9',\n",
       "  'miles',\n",
       "  \"it's\",\n",
       "  'for',\n",
       "  'cancer',\n",
       "  'for',\n",
       "  'a',\n",
       "  'good',\n",
       "  'deed'],\n",
       " ['@seanmurphymusic',\n",
       "  \"i'am\",\n",
       "  'in',\n",
       "  'love',\n",
       "  'with',\n",
       "  'taylor',\n",
       "  \"swift's\",\n",
       "  '&quot',\n",
       "  ';',\n",
       "  'crazier&quot',\n",
       "  ';',\n",
       "  'and',\n",
       "  'i',\n",
       "  'think',\n",
       "  'you',\n",
       "  'should',\n",
       "  'definitely',\n",
       "  'do',\n",
       "  'a',\n",
       "  'cover',\n",
       "  'of',\n",
       "  'it',\n",
       "  \"it'll\",\n",
       "  'be',\n",
       "  'beautiful',\n",
       "  '&lt',\n",
       "  ';3'],\n",
       " ['had',\n",
       "  'a',\n",
       "  'piece',\n",
       "  'of',\n",
       "  'fried',\n",
       "  'chicken',\n",
       "  'some',\n",
       "  'psp',\n",
       "  'luv',\n",
       "  'and',\n",
       "  'now',\n",
       "  'off',\n",
       "  'to',\n",
       "  'bed'],\n",
       " ['@mileycyrus',\n",
       "  'brazil',\n",
       "  'loves',\n",
       "  'u',\n",
       "  'so',\n",
       "  'much',\n",
       "  'you',\n",
       "  'are',\n",
       "  'awesome',\n",
       "  'miley',\n",
       "  'come',\n",
       "  'to',\n",
       "  'brazil',\n",
       "  'we',\n",
       "  \"can't\",\n",
       "  'for',\n",
       "  'this',\n",
       "  'day',\n",
       "  'come'],\n",
       " ['@silmuri',\n",
       "  'yeah',\n",
       "  \"that'd\",\n",
       "  'be',\n",
       "  'awesome',\n",
       "  \"you're\",\n",
       "  'gonna',\n",
       "  'be',\n",
       "  'the',\n",
       "  'new',\n",
       "  'oprah',\n",
       "  \"freakin'\",\n",
       "  'yeah',\n",
       "  '*',\n",
       "  'ninja',\n",
       "  '-',\n",
       "  'rolls',\n",
       "  'over',\n",
       "  'to',\n",
       "  'sil',\n",
       "  'to',\n",
       "  'eat',\n",
       "  'chocolate',\n",
       "  '*'],\n",
       " ['@danniasheonline',\n",
       "  'awwww',\n",
       "  'she',\n",
       "  'called',\n",
       "  'me',\n",
       "  'sexy',\n",
       "  'nice',\n",
       "  'you',\n",
       "  'rock',\n",
       "  'danni',\n",
       "  'you',\n",
       "  'just',\n",
       "  'made',\n",
       "  'my',\n",
       "  'night',\n",
       "  'gurl',\n",
       "  'muah',\n",
       "  \"i'll\",\n",
       "  'give',\n",
       "  'you',\n",
       "  'props',\n",
       "  'anytime'],\n",
       " ['@mcchelsea',\n",
       "  'get',\n",
       "  'a',\n",
       "  'glass',\n",
       "  'of',\n",
       "  'wine',\n",
       "  'a',\n",
       "  'nice',\n",
       "  'book',\n",
       "  'and',\n",
       "  'just',\n",
       "  'chillax',\n",
       "  'out',\n",
       "  'there',\n",
       "  'oh',\n",
       "  'wait',\n",
       "  \"it's\",\n",
       "  'seattle',\n",
       "  \"i'll\",\n",
       "  'probably',\n",
       "  'rain',\n",
       "  'hah'],\n",
       " ['had',\n",
       "  'to',\n",
       "  'happen',\n",
       "  '@oprah',\n",
       "  'is',\n",
       "  'on',\n",
       "  'twitter',\n",
       "  'and',\n",
       "  'only',\n",
       "  'after',\n",
       "  '24',\n",
       "  'hours',\n",
       "  \"she's\",\n",
       "  'got',\n",
       "  '260,371',\n",
       "  'followers',\n",
       "  'and',\n",
       "  'counting',\n",
       "  'you',\n",
       "  'go',\n",
       "  'girlfriend'],\n",
       " ['the',\n",
       "  'lakers',\n",
       "  'are',\n",
       "  'so',\n",
       "  'going',\n",
       "  'to',\n",
       "  'win',\n",
       "  'and',\n",
       "  'advance',\n",
       "  'to',\n",
       "  'the',\n",
       "  'finals'],\n",
       " ['@lightinthesky', 'wow', 'enjoy'],\n",
       " ['i',\n",
       "  'drew',\n",
       "  'a',\n",
       "  'cute',\n",
       "  'baby',\n",
       "  'zebra',\n",
       "  'i',\n",
       "  'think',\n",
       "  'he',\n",
       "  'is',\n",
       "  'my',\n",
       "  'favorite',\n",
       "  'so',\n",
       "  'far'],\n",
       " ['went',\n",
       "  'to',\n",
       "  'calgary',\n",
       "  'with',\n",
       "  'some',\n",
       "  'of',\n",
       "  'the',\n",
       "  'lads',\n",
       "  '(',\n",
       "  'gaspar',\n",
       "  'had',\n",
       "  'to',\n",
       "  'sit',\n",
       "  'in',\n",
       "  'the',\n",
       "  'boot',\n",
       "  ')',\n",
       "  'to',\n",
       "  'watch',\n",
       "  'the',\n",
       "  'flames',\n",
       "  'versus',\n",
       "  'la',\n",
       "  'kings',\n",
       "  'calgary',\n",
       "  'won',\n",
       "  '4',\n",
       "  '-',\n",
       "  '1',\n",
       "  'nice',\n",
       "  'night'],\n",
       " ['@batistini21',\n",
       "  'lols',\n",
       "  'you',\n",
       "  'go',\n",
       "  'for',\n",
       "  'the',\n",
       "  'cavs',\n",
       "  'orlando',\n",
       "  'owned',\n",
       "  'them',\n",
       "  'today'],\n",
       " ['@ccarquette',\n",
       "  'ps',\n",
       "  'please',\n",
       "  'try',\n",
       "  'to',\n",
       "  'see',\n",
       "  'if',\n",
       "  'you',\n",
       "  'get',\n",
       "  'cougar',\n",
       "  'town',\n",
       "  'to',\n",
       "  'be',\n",
       "  'broadcast',\n",
       "  'internationally',\n",
       "  'im',\n",
       "  'brazilian',\n",
       "  'but',\n",
       "  'such',\n",
       "  'a',\n",
       "  'great',\n",
       "  'fan',\n",
       "  'of',\n",
       "  'yours'],\n",
       " ['i',\n",
       "  'l&lt',\n",
       "  ';3',\n",
       "  've',\n",
       "  'taylor',\n",
       "  'swift',\n",
       "  'she',\n",
       "  'is',\n",
       "  'just',\n",
       "  'so',\n",
       "  'amazing',\n",
       "  'and',\n",
       "  'her',\n",
       "  'music',\n",
       "  'is',\n",
       "  'wonderfull',\n",
       "  'and',\n",
       "  'makes',\n",
       "  'me',\n",
       "  'so',\n",
       "  'happy']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[listify(x) for x in pos_tweets[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the angel is going to miss the athlete this weekend ',\n",
       " \"It looks as though Shaq is getting traded to Cleveland to play w/ LeBron... Too bad for Suns' fans. The Big Cactus is no more \",\n",
       " \"@clarianne APRIL 9TH ISN'T COMING SOON ENOUGH \",\n",
       " 'drinking a McDonalds coffee and not understanding why someone would hurt me for no apparent reason. ',\n",
       " 'So dissapointed Taylor Swift doesnt have a Twitter ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_tweets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_senti_dicts = [LabMT(),ANEW(),WK(),MPQA(),\n",
    "                   LIWC01(),LIWC07(),LIWC15(),\n",
    "                   OL(),PANASX(),Pattern(),SentiWordNet(),\n",
    "                   AFINN(),GI(),WDAL(),EmoLex(),MaxDiff(),\n",
    "                   HashtagSent(),Sent140Lex(),SOCAL(),\n",
    "                   SenticNet(),Emoticons(),SentiStrength(),\n",
    "                   VADER(),Umigon(),USent(),EmoSenticNet()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_senti_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5.0, 5.3752396791234593)\n",
      "(5.0, 5.1493520309477763)\n",
      "(5.0, 5.0638469277757823)\n",
      "(0.0, -0.27099555061179087)\n",
      "(0.0, -0.033591731266149873)\n",
      "(0.0, -0.020968101717599823)\n",
      "(0.0, -0.015880287066727745)\n",
      "(0.0, -0.40931878501916841)\n",
      "(0.0, 0.0)\n",
      "(0.0, -0.014117423080119412)\n",
      "(0.0, -0.01176069800289377)\n",
      "(0.0, -0.5894226887363746)\n",
      "(0.0, -0.10112978782033619)\n",
      "(1.5, 1.8397483243737849)\n",
      "(0.0, -0.071358059512057537)\n",
      "(0.0, 0.00025610561056105019)\n",
      "(0.0, 0.063984296772524887)\n",
      "(0.0, 0.35669409937888202)\n",
      "(0.0, -0.13250311724152655)\n",
      "(0.0, -0.030085266666666662)\n",
      "(0.0, 0.07575757575757576)\n",
      "(0.0, -1.2795411089866158)\n",
      "(0.0, -0.18193814982671289)\n",
      "(0.0, -0.27939590075512405)\n",
      "(0.0, -0.78716216216216217)\n",
      "(0.0, 0.59531392174704278)\n"
     ]
    }
   ],
   "source": [
    "for x in [(x.center,x.scorelist.mean()) for x in all_senti_dicts]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'@': 1, 'with': 1, 'lol': 1, 'aka': 1, 'litto': 1, 'sis': 1, 'my': 1, 'cuzin': 1, 'mcdonalds': 1, 'cristyyyyy': 1}\n",
      "6.01 6.01 True \n",
      "622\n",
      "5.00 5.15 False \n",
      "396\n",
      "4.63 4.63 True \n",
      "601\n",
      "0.00 -0.27 False \n",
      "441\n",
      "0.00 0.00 True \n",
      "383\n",
      "0.25 0.25 True \n",
      "453\n",
      "0.25 0.25 True \n",
      "462\n",
      "0.00 -0.41 False \n",
      "424\n",
      "0.00 0.00 False \n",
      "13\n",
      "0.80 0.80 True \n",
      "438\n",
      "0.00 0.00 True \n",
      "476\n",
      "3.00 3.00 True \n",
      "469\n",
      "0.00 -0.10 False \n",
      "374\n",
      "2.11 2.11 True \n",
      "628\n",
      "0.00 -0.07 False \n",
      "353\n",
      "0.25 0.25 True \n",
      "579\n",
      "0.24 0.24 True \n",
      "467\n",
      "0.06 0.06 True \n",
      "545\n",
      "0.00 -0.13 False \n",
      "487\n",
      "0.11 0.11 True \n",
      "511\n",
      "0.00 0.08 False \n",
      "12\n",
      "2.00 2.00 True \n",
      "463\n",
      "1.80 1.80 True \n",
      "510\n",
      "1.00 1.00 True \n",
      "404\n",
      "0.00 -0.79 False \n",
      "11\n",
      "1.00 1.00 True \n",
      "585\n"
     ]
    }
   ],
   "source": [
    "pos_tweet_dicts = [dictify(listify(x)) for x in pos_tweets]\n",
    "print(pos_tweet_dicts[0])\n",
    "for x in all_senti_dicts:\n",
    "    print(\"{0:.2f} {2:.2f} {1} \".format(x.score(pos_tweet_dicts[0],center=x.center),\n",
    "          x.score(pos_tweet_dicts[0],center=1000.0)!=1000.0,\n",
    "          x.score(pos_tweet_dicts[0],center=x.scorelist.mean())))\n",
    "    a = np.array([x.score(y) for y in pos_tweet_dicts])\n",
    "    # print(a)\n",
    "    b = a[a>x.center]\n",
    "    c = a[a<x.center]\n",
    "    print(len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifier_perf(conf_mat,v=False):\n",
    "    \"\"\"Given the confusion matrix, produce precision, recall, and f1-score.\n",
    "    Actual going down, predicited going across.\"\"\"\n",
    "    \n",
    "    N = conf_mat.shape[0]\n",
    "    # could do these computations using matrix math...\n",
    "    R = np.array([conf_mat[i,i]/conf_mat[i,:].sum() for i in range(N)])\n",
    "    P = np.array([conf_mat[i,i]/conf_mat[:,i].sum() for i in range(N)])\n",
    "\n",
    "    F1 = np.array([2*R[i]*P[i]/(R[i]+P[i]) for i in range(N)])\n",
    "    if v:\n",
    "        print(R)\n",
    "        print(P)\n",
    "        print(F1)\n",
    "    return F1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_tweet_dicts = [dictify(listify(x)) for x in pos_tweets]\n",
    "neg_tweet_dicts = [dictify(listify(x)) for x in neg_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_tweets = len(pos_tweet_dicts)+len(neg_tweet_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:8: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:9: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  440.    99.]\n",
      " [  265.  1036.]]\n",
      "[[ 589.   43.]\n",
      " [ 570.  832.]]\n",
      "[[  622.    10.]\n",
      " [ 1134.   267.]]\n",
      "labMT 0.694259616042 0.419587009342 0.778985105518\n",
      "[[ 298.   44.]\n",
      " [ 317.  506.]]\n",
      "[[ 392.   19.]\n",
      " [ 433.  459.]]\n",
      "[[ 396.   15.]\n",
      " [ 464.  428.]]\n",
      "ANEW 0.65218859991 0.632164947238 0.679925812106\n",
      "[[ 403.  120.]\n",
      " [ 340.  913.]]\n",
      "[[ 598.   18.]\n",
      " [ 894.  453.]]\n",
      "[[ 601.   15.]\n",
      " [ 928.  417.]]\n",
      "WK 0.532856131913 0.514851646197 0.717713010992\n",
      "[[ 355.  133.]\n",
      " [ 290.  932.]]\n",
      "[[ 546.   29.]\n",
      " [ 739.  569.]]\n",
      "[[ 441.   40.]\n",
      " [ 357.  650.]]\n",
      "MPQA 0.592079341976 0.727829499973 0.720848218817\n",
      "[[  324.   214.]\n",
      " [  163.  1141.]]\n",
      "[[ 622.   10.]\n",
      " [ 867.  533.]]\n",
      "[[ 383.   10.]\n",
      " [ 178.  533.]]\n",
      "LIWC01 0.567575962066 0.826507377649 0.745206248452\n",
      "[[  315.   223.]\n",
      " [  136.  1169.]]\n",
      "[[ 623.    9.]\n",
      " [ 876.  526.]]\n",
      "[[ 453.    9.]\n",
      " [ 238.  526.]]\n",
      "LIWC07 0.563904958322 0.797814984774 0.751948106967\n",
      "[[  397.   143.]\n",
      " [  207.  1097.]]\n",
      "[[ 624.    8.]\n",
      " [ 840.  562.]]\n",
      "[[ 462.    8.]\n",
      " [ 224.  562.]]\n",
      "LIWC15 0.582699781676 0.814108256525 0.778238663852\n",
      "[[ 354.   47.]\n",
      " [ 183.  817.]]\n",
      "[[ 457.   20.]\n",
      " [ 425.  646.]]\n",
      "[[ 424.   23.]\n",
      " [ 198.  712.]]\n",
      "OL 0.708182258366 0.829459114418 0.815703441712\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[[ 13.   0.]\n",
      " [  4.  17.]]\n",
      "[[ 13.   0.]\n",
      " [  4.  17.]]\n",
      "PANAS-X 0.880701754386 0.880701754386 nan\n",
      "[[ 339.   94.]\n",
      " [ 220.  735.]]\n",
      "[[ 474.   41.]\n",
      " [ 477.  546.]]\n",
      "[[ 438.   42.]\n",
      " [ 367.  557.]]\n",
      "Pattern 0.662459220594 0.706581572822 0.753729386663\n",
      "[[ 348.  189.]\n",
      " [ 332.  970.]]\n",
      "[[ 536.   95.]\n",
      " [ 627.  771.]]\n",
      "[[ 476.  144.]\n",
      " [ 478.  908.]]\n",
      "SentiWordNet 0.639321393258 0.674850654556 0.680097775086\n",
      "[[ 392.   33.]\n",
      " [ 230.  768.]]\n",
      "[[ 479.   19.]\n",
      " [ 417.  655.]]\n",
      "[[ 469.   22.]\n",
      " [ 300.  706.]]\n",
      "AFINN 0.7187586794 0.779373317955 0.801306891816\n",
      "[[ 304.  121.]\n",
      " [ 276.  783.]]\n",
      "[[ 440.   66.]\n",
      " [ 503.  629.]]\n",
      "[[ 374.   66.]\n",
      " [ 307.  629.]]\n",
      "GI 0.647937935794 0.719283660523 0.701366828618\n",
      "[[ 345.  195.]\n",
      " [ 412.  893.]]\n",
      "[[ 525.  106.]\n",
      " [ 747.  654.]]\n",
      "[[  628.     3.]\n",
      " [ 1373.    28.]]\n",
      "WDAL 0.578517856921 0.258154896334 0.63917020892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 306.  193.]\n",
      " [ 253.  943.]]\n",
      "[[ 538.   46.]\n",
      " [ 750.  536.]]\n",
      "[[ 353.   46.]\n",
      " [ 274.  536.]]\n",
      "EmoLex 0.574331063892 0.729112052161 0.6935988807\n",
      "[[ 430.   87.]\n",
      " [ 416.  796.]]\n",
      "[[ 579.   29.]\n",
      " [ 776.  530.]]\n",
      "[[ 579.   29.]\n",
      " [ 777.  525.]]\n",
      "MaxDiff 0.57913900456 0.577672896622 0.695432824897\n",
      "[[ 386.  155.]\n",
      " [ 462.  845.]]\n",
      "[[  399.   233.]\n",
      " [  375.  1027.]]\n",
      "[[ 467.  165.]\n",
      " [ 576.  826.]]\n",
      "HashtagSent 0.669583934047 0.623979392631 0.64417431781\n",
      "[[  489.    46.]\n",
      " [  150.  1155.]]\n",
      "[[  122.   510.]\n",
      " [    3.  1399.]]\n",
      "[[  545.    87.]\n",
      " [   99.  1303.]]\n",
      "Sent140Lex 0.583693440902 0.893806531873 0.877418556623\n",
      "[[ 376.   83.]\n",
      " [ 280.  806.]]\n",
      "[[ 494.   47.]\n",
      " [ 486.  679.]]\n",
      "[[ 487.   48.]\n",
      " [ 436.  700.]]\n",
      "SOCAL 0.683855600302 0.705569098232 0.745320996764\n",
      "[[ 344.  183.]\n",
      " [ 408.  857.]]\n",
      "[[ 521.   99.]\n",
      " [ 724.  636.]]\n",
      "[[ 511.  109.]\n",
      " [ 661.  699.]]\n",
      "SenticNet 0.582936520632 0.60757322417 0.640760558937\n",
      "[[ 10.   0.]\n",
      " [  5.   7.]]\n",
      "[[ 12.   1.]\n",
      " [  7.   9.]]\n",
      "[[ 12.   0.]\n",
      " [  7.   5.]]\n",
      "Emoticons 0.721153846154 0.681214421252 0.768421052632\n",
      "[[ 394.   42.]\n",
      " [ 211.  785.]]\n",
      "[[ 492.   19.]\n",
      " [ 440.  631.]]\n",
      "[[ 463.   30.]\n",
      " [ 219.  792.]]\n",
      "SentiStrength 0.707603639039 0.82612111293 0.809091115077\n",
      "[[ 409.   40.]\n",
      " [ 222.  828.]]\n",
      "[[ 516.   22.]\n",
      " [ 418.  706.]]\n",
      "[[ 510.   28.]\n",
      " [ 369.  746.]]\n",
      "VADER 0.731752981501 0.754833260047 0.810403390878\n",
      "[[ 346.   58.]\n",
      " [ 121.  843.]]\n",
      "[[ 443.   34.]\n",
      " [ 251.  791.]]\n",
      "[[ 404.   34.]\n",
      " [ 134.  792.]]\n",
      "Umigon 0.801983481356 0.86598922075 0.849255270359\n",
      "[[ 10.   5.]\n",
      " [ 10.  55.]]\n",
      "[[ 12.   6.]\n",
      " [ 10.  64.]]\n",
      "[[ 11.   6.]\n",
      " [ 10.  64.]]\n",
      "USent 0.744444444444 0.733918128655 0.725714285714\n",
      "[[ 385.  135.]\n",
      " [ 567.  683.]]\n",
      "[[ 521.   91.]\n",
      " [ 768.  572.]]\n",
      "[[  585.    10.]\n",
      " [ 1137.    89.]]\n",
      "EmoSenticNet 0.559637923441 0.319651468636 0.59181970608\n"
     ]
    }
   ],
   "source": [
    "tweet_f1 = []\n",
    "twitter_calibrated_f1 = []\n",
    "conf_mats = []\n",
    "for x in all_senti_dicts:\n",
    "    a = np.array([x.score(y,center=1000.0) for y in pos_tweet_dicts])\n",
    "    b = np.array([x.score(y,center=1000.0) for y in neg_tweet_dicts])\n",
    "    total_n_samples = len(a)+len(b)\n",
    "    a_training = np.random.choice(np.arange(len(a)),size=.05*total_n_samples)\n",
    "    b_training = np.random.choice(np.arange(len(b)),size=.05*total_n_samples)\n",
    "    a_mask = np.zeros(len(a))\n",
    "    b_mask = np.zeros(len(b))\n",
    "    a_mask[a_training] = 1\n",
    "    b_mask[b_training] = 1\n",
    "    avg = np.concatenate((a[(a!=1000.0) & (a_mask>0)],b[(b!=1000.0) & (b_mask>0)])).mean()\n",
    "    # print(a)\n",
    "    # b = a[a>x.center]\n",
    "    # c = a[a<x.center]\n",
    "    # print(len(b))\n",
    "    cal_conf_mat = np.zeros((2,2))\n",
    "    cal_conf_mat[0,0] = len(a[(a!=1000.0) & (a>avg) & (a_mask<1)])\n",
    "    cal_conf_mat[0,1] = len(a[(a!=1000.0) & (a<avg) & (a_mask<1)])\n",
    "    cal_conf_mat[1,0] = len(b[(b!=1000.0) & (b>avg) & (b_mask<1)])\n",
    "    cal_conf_mat[1,1] = len(b[(b!=1000.0) & (b<avg) & (b_mask<1)])\n",
    "    # classifier_perf(this_conf_mat,v=True)\n",
    "    # print(a)\n",
    "    print(cal_conf_mat)\n",
    "    calibrated_f1 = classifier_perf(cal_conf_mat)\n",
    "    twitter_calibrated_f1.append(calibrated_f1)\n",
    "    \n",
    "    a = np.array([x.score(y,center=1000.0) for y in pos_tweet_dicts])\n",
    "    # print(a)\n",
    "    # b = a[a>x.center]\n",
    "    # c = a[a<x.center]\n",
    "    # print(len(b))\n",
    "    this_conf_mat = np.zeros((2,2))\n",
    "    this_conf_mat[0,0] = len(a[(a!=1000.0) & (a>x.scorelist.mean())])\n",
    "    this_conf_mat[0,1] = len(a[(a!=1000.0) & (a<x.scorelist.mean())])\n",
    "    a = np.array([x.score(y,center=1000.0) for y in neg_tweet_dicts])\n",
    "    this_conf_mat[1,0] = len(a[(a!=1000.0) & (a>x.scorelist.mean())])\n",
    "    this_conf_mat[1,1] = len(a[(a!=1000.0) & (a<x.scorelist.mean())])\n",
    "    # classifier_perf(this_conf_mat,v=True)\n",
    "    # print(a)\n",
    "    print(this_conf_mat)\n",
    "    mean_f1 = classifier_perf(this_conf_mat)\n",
    "    center_conf_mat = np.zeros((2,2))\n",
    "    a = np.array([x.score(y,center=1000.0) for y in pos_tweet_dicts])\n",
    "    center_conf_mat[0,0] = len(a[(a!=1000.0) & (a>x.center)])\n",
    "    center_conf_mat[0,1] = len(a[(a!=1000.0) & (a<x.center)])\n",
    "    a = np.array([x.score(y,center=1000.0) for y in neg_tweet_dicts])\n",
    "    center_conf_mat[1,0] = len(a[(a!=1000.0) & (a>x.center)])\n",
    "    center_conf_mat[1,1] = len(a[(a!=1000.0) & (a<x.center)])\n",
    "    center_f1 = classifier_perf(center_conf_mat)\n",
    "    print(center_conf_mat)\n",
    "    print(x.title,mean_f1,center_f1,calibrated_f1)\n",
    "    if not np.isnan(center_f1):\n",
    "        if center_f1>mean_f1:\n",
    "            conf_mats.append(center_conf_mat)\n",
    "            tweet_f1.append(center_f1)\n",
    "            continue\n",
    "    conf_mats.append(this_conf_mat)\n",
    "    tweet_f1.append(mean_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('WK', 0.53285613191300163, 0.71771301099210938)\n",
      "('EmoSenticNet', 0.55963792344098273, 0.59181970608022882)\n",
      "('WDAL', 0.57851785692140067, 0.63917020892019605)\n",
      "('MaxDiff', 0.5791390045602357, 0.69543282489664626)\n",
      "('SenticNet', 0.60757322416974169, 0.64076055893721184)\n",
      "('ANEW', 0.65218859991023548, 0.67992581210553427)\n",
      "('HashtagSent', 0.66958393404674399, 0.64417431781010182)\n",
      "('SentiWordNet', 0.67485065455572668, 0.68009777508591718)\n",
      "('labMT', 0.69425961604244324, 0.77898510551798061)\n",
      "('SOCAL', 0.70556909823246228, 0.74532099676448882)\n",
      "('Pattern', 0.70658157282242962, 0.7537293866628092)\n",
      "('GI', 0.71928366052251458, 0.70136682861798993)\n",
      "('Emoticons', 0.72115384615384603, 0.76842105263157889)\n",
      "('MPQA', 0.72782949997304724, 0.72084821881689787)\n",
      "('EmoLex', 0.72911205216105401, 0.69359888069985443)\n",
      "('USent', 0.74444444444444446, 0.72571428571428576)\n",
      "('VADER', 0.75483326004693074, 0.81040339087784341)\n",
      "('AFINN', 0.77937331795463272, 0.8013068918156272)\n",
      "('LIWC07', 0.79781498477379698, 0.75194810696677172)\n",
      "('LIWC15', 0.81410825652488983, 0.77823866385187135)\n",
      "('SentiStrength', 0.82612111292962354, 0.80909111507722598)\n",
      "('LIWC01', 0.82650737764938365, 0.7452062484521047)\n",
      "('OL', 0.82945911441821329, 0.81570344171234566)\n",
      "('Umigon', 0.86598922075005613, 0.84925527035886761)\n",
      "('PANAS-X', 0.88070175438596487, nan)\n",
      "('Sent140Lex', 0.89380653187342241, 0.87741855662253865)\n"
     ]
    }
   ],
   "source": [
    "for x in sorted(zip([x.title for x in all_senti_dicts],tweet_f1,twitter_calibrated_f1),key=lambda x: x[1]):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 589.,   43.],\n",
       "       [ 570.,  832.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2034.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mats[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WK 0.532856131913 0.965093411996 0.514255942451\n",
      "EmoSenticNet 0.559637923441 0.959685349066 0.537076315908\n",
      "WDAL 0.578517856921 0.999016715831 0.577949009471\n",
      "MaxDiff 0.57913900456 0.941002949853 0.544971511666\n",
      "SenticNet 0.60757322417 0.973451327434 0.591442961581\n",
      "ANEW 0.65218859991 0.640609636185 0.417798301712\n",
      "HashtagSent 0.669583934047 1.0 0.669583934047\n",
      "SentiWordNet 0.674850654556 0.986234021632 0.665560675044\n",
      "labMT 0.694259616042 1.0 0.694259616042\n",
      "SOCAL 0.705569098232 0.821533923304 0.579648949433\n",
      "Pattern 0.706581572822 0.690265486726 0.487728873276\n",
      "GI 0.719283660523 0.676499508358 0.486595042713\n",
      "Emoticons 0.721153846154 0.0142576204523 0.0102819378262\n",
      "MPQA 0.727829499973 0.731563421829 0.532453439508\n",
      "EmoLex 0.729112052161 0.594395280236 0.433380762568\n",
      "USent 0.744444444444 0.0452310717797 0.0336720201027\n",
      "VADER 0.754833260047 0.812684365782 0.613441189212\n",
      "AFINN 0.779373317955 0.73598820059 0.573609565869\n",
      "LIWC07 0.797814984774 0.602753195674 0.480885531629\n",
      "LIWC15 0.814108256525 0.61750245821 0.502713849654\n",
      "SentiStrength 0.82612111293 0.739429695182 0.610858482717\n",
      "LIWC01 0.826507377649 0.542772861357 0.448605774299\n",
      "OL 0.829459114418 0.667158308751 0.553380539954\n",
      "Umigon 0.86598922075 0.670599803343 0.580732201132\n",
      "PANAS-X 0.880701754386 0.0167158308751 0.0147216615777\n",
      "Sent140Lex 0.893806531873 1.0 0.893806531873\n"
     ]
    }
   ],
   "source": [
    "d = []\n",
    "for x in sorted(zip([x.title for x in all_senti_dicts],tweet_f1,conf_mats,twitter_calibrated_f1),key=lambda x: x[1]):\n",
    "    title=x[0]\n",
    "    acc=x[1]\n",
    "    perc=x[2].sum()/float(total_tweets)\n",
    "    acc_total=acc*perc\n",
    "    print(x[0],x[1],x[2].sum()/float(total_tweets),acc_total)\n",
    "    d.append([x[0],x[1],x[2].sum()/float(total_tweets),acc_total,x[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['WK',\n",
       "  0.53285613191300163,\n",
       "  0.96509341199606691,\n",
       "  0.51425594245094508,\n",
       "  0.71771301099210938],\n",
       " ['EmoSenticNet',\n",
       "  0.55963792344098273,\n",
       "  0.95968534906587999,\n",
       "  0.53707631590796379,\n",
       "  0.59181970608022882],\n",
       " ['WDAL',\n",
       "  0.57851785692140067,\n",
       "  0.99901671583087515,\n",
       "  0.57794900947113381,\n",
       "  0.63917020892019605],\n",
       " ['MaxDiff',\n",
       "  0.5791390045602357,\n",
       "  0.94100294985250732,\n",
       "  0.54497151166582647,\n",
       "  0.69543282489664626],\n",
       " ['SenticNet',\n",
       "  0.60757322416974169,\n",
       "  0.97345132743362828,\n",
       "  0.59144296158116449,\n",
       "  0.64076055893721184],\n",
       " ['ANEW',\n",
       "  0.65218859991023548,\n",
       "  0.64060963618485744,\n",
       "  0.4177983017124075,\n",
       "  0.67992581210553427],\n",
       " ['HashtagSent',\n",
       "  0.66958393404674399,\n",
       "  1.0,\n",
       "  0.66958393404674399,\n",
       "  0.64417431781010182],\n",
       " ['SentiWordNet',\n",
       "  0.67485065455572668,\n",
       "  0.98623402163225171,\n",
       "  0.66556067504365179,\n",
       "  0.68009777508591718],\n",
       " ['labMT', 0.69425961604244324, 1.0, 0.69425961604244324, 0.77898510551798061],\n",
       " ['SOCAL',\n",
       "  0.70556909823246228,\n",
       "  0.82153392330383479,\n",
       "  0.57964894943286349,\n",
       "  0.74532099676448882],\n",
       " ['Pattern',\n",
       "  0.70658157282242962,\n",
       "  0.69026548672566368,\n",
       "  0.48772887327565934,\n",
       "  0.7537293866628092],\n",
       " ['GI',\n",
       "  0.71928366052251458,\n",
       "  0.67649950835791539,\n",
       "  0.48659504271336285,\n",
       "  0.70136682861798993],\n",
       " ['Emoticons',\n",
       "  0.72115384615384603,\n",
       "  0.014257620452310717,\n",
       "  0.010281937826185611,\n",
       "  0.76842105263157889],\n",
       " ['MPQA',\n",
       "  0.72782949997304724,\n",
       "  0.73156342182890854,\n",
       "  0.53245343950830593,\n",
       "  0.72084821881689787],\n",
       " ['EmoLex',\n",
       "  0.72911205216105401,\n",
       "  0.5943952802359882,\n",
       "  0.43338076256770613,\n",
       "  0.69359888069985443],\n",
       " ['USent',\n",
       "  0.74444444444444446,\n",
       "  0.045231071779744343,\n",
       "  0.033672020102698569,\n",
       "  0.72571428571428576],\n",
       " ['VADER',\n",
       "  0.75483326004693074,\n",
       "  0.81268436578171088,\n",
       "  0.61344118921218116,\n",
       "  0.81040339087784341],\n",
       " ['AFINN',\n",
       "  0.77937331795463272,\n",
       "  0.7359882005899705,\n",
       "  0.57360956586926504,\n",
       "  0.8013068918156272],\n",
       " ['LIWC07',\n",
       "  0.79781498477379698,\n",
       "  0.60275319567354968,\n",
       "  0.4808855316286505,\n",
       "  0.75194810696677172],\n",
       " ['LIWC15',\n",
       "  0.81410825652488983,\n",
       "  0.61750245821042282,\n",
       "  0.50271384965352095,\n",
       "  0.77823866385187135],\n",
       " ['SentiStrength',\n",
       "  0.82612111292962354,\n",
       "  0.7394296951819076,\n",
       "  0.61085848271688981,\n",
       "  0.80909111507722598],\n",
       " ['LIWC01',\n",
       "  0.82650737764938365,\n",
       "  0.54277286135693215,\n",
       "  0.44860577429937049,\n",
       "  0.7452062484521047],\n",
       " ['OL',\n",
       "  0.82945911441821329,\n",
       "  0.66715830875122906,\n",
       "  0.55338053995354741,\n",
       "  0.81570344171234566],\n",
       " ['Umigon',\n",
       "  0.86598922075005613,\n",
       "  0.67059980334316616,\n",
       "  0.58073220113228929,\n",
       "  0.84925527035886761],\n",
       " ['PANAS-X',\n",
       "  0.88070175438596487,\n",
       "  0.016715830875122909,\n",
       "  0.014721661577739825,\n",
       "  nan],\n",
       " ['Sent140Lex',\n",
       "  0.89380653187342241,\n",
       "  1.0,\n",
       "  0.89380653187342241,\n",
       "  0.87741855662253865]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Sent140Lex',\n",
       "  0.89380653187342241,\n",
       "  1.0,\n",
       "  0.89380653187342241,\n",
       "  0.87741855662253865],\n",
       " ['labMT', 0.69425961604244324, 1.0, 0.69425961604244324, 0.77898510551798061],\n",
       " ['HashtagSent',\n",
       "  0.66958393404674399,\n",
       "  1.0,\n",
       "  0.66958393404674399,\n",
       "  0.64417431781010182],\n",
       " ['SentiWordNet',\n",
       "  0.67485065455572668,\n",
       "  0.98623402163225171,\n",
       "  0.66556067504365179,\n",
       "  0.68009777508591718],\n",
       " ['VADER',\n",
       "  0.75483326004693074,\n",
       "  0.81268436578171088,\n",
       "  0.61344118921218116,\n",
       "  0.81040339087784341],\n",
       " ['SentiStrength',\n",
       "  0.82612111292962354,\n",
       "  0.7394296951819076,\n",
       "  0.61085848271688981,\n",
       "  0.80909111507722598],\n",
       " ['SenticNet',\n",
       "  0.60757322416974169,\n",
       "  0.97345132743362828,\n",
       "  0.59144296158116449,\n",
       "  0.64076055893721184],\n",
       " ['Umigon',\n",
       "  0.86598922075005613,\n",
       "  0.67059980334316616,\n",
       "  0.58073220113228929,\n",
       "  0.84925527035886761],\n",
       " ['SOCAL',\n",
       "  0.70556909823246228,\n",
       "  0.82153392330383479,\n",
       "  0.57964894943286349,\n",
       "  0.74532099676448882],\n",
       " ['WDAL',\n",
       "  0.57851785692140067,\n",
       "  0.99901671583087515,\n",
       "  0.57794900947113381,\n",
       "  0.63917020892019605],\n",
       " ['AFINN',\n",
       "  0.77937331795463272,\n",
       "  0.7359882005899705,\n",
       "  0.57360956586926504,\n",
       "  0.8013068918156272],\n",
       " ['OL',\n",
       "  0.82945911441821329,\n",
       "  0.66715830875122906,\n",
       "  0.55338053995354741,\n",
       "  0.81570344171234566],\n",
       " ['MaxDiff',\n",
       "  0.5791390045602357,\n",
       "  0.94100294985250732,\n",
       "  0.54497151166582647,\n",
       "  0.69543282489664626],\n",
       " ['EmoSenticNet',\n",
       "  0.55963792344098273,\n",
       "  0.95968534906587999,\n",
       "  0.53707631590796379,\n",
       "  0.59181970608022882],\n",
       " ['MPQA',\n",
       "  0.72782949997304724,\n",
       "  0.73156342182890854,\n",
       "  0.53245343950830593,\n",
       "  0.72084821881689787],\n",
       " ['WK',\n",
       "  0.53285613191300163,\n",
       "  0.96509341199606691,\n",
       "  0.51425594245094508,\n",
       "  0.71771301099210938],\n",
       " ['LIWC15',\n",
       "  0.81410825652488983,\n",
       "  0.61750245821042282,\n",
       "  0.50271384965352095,\n",
       "  0.77823866385187135],\n",
       " ['Pattern',\n",
       "  0.70658157282242962,\n",
       "  0.69026548672566368,\n",
       "  0.48772887327565934,\n",
       "  0.7537293866628092],\n",
       " ['GI',\n",
       "  0.71928366052251458,\n",
       "  0.67649950835791539,\n",
       "  0.48659504271336285,\n",
       "  0.70136682861798993],\n",
       " ['LIWC07',\n",
       "  0.79781498477379698,\n",
       "  0.60275319567354968,\n",
       "  0.4808855316286505,\n",
       "  0.75194810696677172],\n",
       " ['LIWC01',\n",
       "  0.82650737764938365,\n",
       "  0.54277286135693215,\n",
       "  0.44860577429937049,\n",
       "  0.7452062484521047],\n",
       " ['EmoLex',\n",
       "  0.72911205216105401,\n",
       "  0.5943952802359882,\n",
       "  0.43338076256770613,\n",
       "  0.69359888069985443],\n",
       " ['ANEW',\n",
       "  0.65218859991023548,\n",
       "  0.64060963618485744,\n",
       "  0.4177983017124075,\n",
       "  0.67992581210553427],\n",
       " ['USent',\n",
       "  0.74444444444444446,\n",
       "  0.045231071779744343,\n",
       "  0.033672020102698569,\n",
       "  0.72571428571428576],\n",
       " ['PANAS-X',\n",
       "  0.88070175438596487,\n",
       "  0.016715830875122909,\n",
       "  0.014721661577739825,\n",
       "  nan],\n",
       " ['Emoticons',\n",
       "  0.72115384615384603,\n",
       "  0.014257620452310717,\n",
       "  0.010281937826185611,\n",
       "  0.76842105263157889]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(d,key=lambda x: x[3],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_results = sorted(d,key=lambda x: x[3],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. & Sent140Lex & 100.0 & 0.89 & 0.88 & 0.89\\\\\n",
      "2. & labMT & 100.0 & 0.69 & 0.78 & 0.69\\\\\n",
      "3. & HashtagSent & 100.0 & 0.67 & 0.64 & 0.67\\\\\n",
      "4. & SentiWordNet & 98.6 & 0.67 & 0.68 & 0.67\\\\\n",
      "5. & VADER & 81.3 & 0.75 & 0.81 & 0.61\\\\\n",
      "6. & SentiStrength & 73.9 & 0.83 & 0.81 & 0.61\\\\\n",
      "7. & SenticNet & 97.3 & 0.61 & 0.64 & 0.59\\\\\n",
      "8. & Umigon & 67.1 & 0.87 & 0.85 & 0.58\\\\\n",
      "9. & SOCAL & 82.2 & 0.71 & 0.75 & 0.58\\\\\n",
      "10. & WDAL & 99.9 & 0.58 & 0.64 & 0.58\\\\\n",
      "11. & AFINN & 73.6 & 0.78 & 0.80 & 0.57\\\\\n",
      "12. & OL & 66.7 & 0.83 & 0.82 & 0.55\\\\\n",
      "13. & MaxDiff & 94.1 & 0.58 & 0.70 & 0.54\\\\\n",
      "14. & EmoSenticNet & 96.0 & 0.56 & 0.59 & 0.54\\\\\n",
      "15. & MPQA & 73.2 & 0.73 & 0.72 & 0.53\\\\\n",
      "16. & WK & 96.5 & 0.53 & 0.72 & 0.51\\\\\n",
      "17. & LIWC15 & 61.8 & 0.81 & 0.78 & 0.50\\\\\n",
      "18. & Pattern & 69.0 & 0.71 & 0.75 & 0.49\\\\\n",
      "19. & GI & 67.6 & 0.72 & 0.70 & 0.49\\\\\n",
      "20. & LIWC07 & 60.3 & 0.80 & 0.75 & 0.48\\\\\n",
      "21. & LIWC01 & 54.3 & 0.83 & 0.75 & 0.45\\\\\n",
      "22. & EmoLex & 59.4 & 0.73 & 0.69 & 0.43\\\\\n",
      "23. & ANEW & 64.1 & 0.65 & 0.68 & 0.42\\\\\n",
      "24. & USent & 4.5 & 0.74 & 0.73 & 0.03\\\\\n",
      "25. & PANAS-X & 1.7 & 0.88 & -- & 0.01\\\\\n",
      "26. & Emoticons & 1.4 & 0.72 & 0.77 & 0.01\\\\\n"
     ]
    }
   ],
   "source": [
    "f = open(\"tables/STS-Gold-results.tex\",\"w\")\n",
    "f.write(r\"\"\"\\begin{tabular}{l | l | c | c | c | c}\n",
    "Rank & Dictionary & \\% tweets scored & F1 of tweets scored & Calibrated F1 & Overall F1\\\\\n",
    "\\hline\n",
    "\"\"\")\n",
    "# strings = []\n",
    "for i,x in enumerate(sorted_results):\n",
    "    for j in [1,3,4]:\n",
    "        if np.isnan(x[j]):\n",
    "            x[j] = \"--\"\n",
    "        else:\n",
    "            x[j] = \"{0:.2f}\".format(x[j])\n",
    "    # strings.append(\"{4}. & {0} & {1:.1f} & {2:.2f} & {3:.2f}\\\\\\\\\".format(x[0],x[2]*100,x[1],x[3],i+1))\n",
    "    f.write(\"{4}. & {0} & {1:.1f} & {2} & {5} & {3}\\\\\\\\\\n\".format(x[0],x[2]*100,x[1],x[3],i+1,x[4]))\n",
    "    print(\"{4}. & {0} & {1:.1f} & {2} & {5} & {3}\\\\\\\\\".format(x[0],x[2]*100,x[1],x[3],i+1,x[4]))\n",
    "# f.write(\"\\n\".join(strings))\n",
    "f.write(\"\\\\end{tabular}\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_results = np.array([x[3] for x in d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50190056235638225"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54102391378689507"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_perf(np.ones((2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
