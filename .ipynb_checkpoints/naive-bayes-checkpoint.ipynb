{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement and test Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/andyreagan/tools/python/labMTsimple/\")\n",
    "from labMTsimple.speedy import *\n",
    "from labMTsimple.storyLab import *\n",
    "\n",
    "import re\n",
    "import codecs\n",
    "from os import listdir,mkdir\n",
    "from os.path import isfile,isdir\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import rc,rcParams\n",
    "rc(\"xtick\", labelsize=8)\n",
    "rc(\"ytick\", labelsize=8)\n",
    "rc(\"font\",**{\"family\":\"serif\",\"serif\":[\"cmr10\"]})\n",
    "# rc(\"text\", usetex=True)\n",
    "figwidth_onecol = 8.5\n",
    "figwidth_twocol = figwidth_onecol/2\n",
    "\n",
    "import numpy as np\n",
    "from json import loads\n",
    "import csv\n",
    "from datetime import datetime,timedelta\n",
    "import pickle\n",
    "\n",
    "error_logging = True\n",
    "sys.path.append(\"/Users/andyreagan/tools/python/kitchentable\")\n",
    "from dogtoys import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadMovieReviews():\n",
    "    posfiles = [\"../data/moviereviews/txt_sentoken/pos/\"+x for x in listdir(\"../data/moviereviews/txt_sentoken/pos\") if \".txt\" in x]\n",
    "    negfiles = [\"../data/moviereviews/txt_sentoken/neg/\"+x for x in listdir(\"../data/moviereviews/txt_sentoken/neg\") if \".txt\" in x]\n",
    "    poswordcounts = dict()\n",
    "    allwordcounts = dict()\n",
    "    for file in posfiles:\n",
    "        f = open(file,\"r\")\n",
    "        postext = f.read() + \" \"\n",
    "        f.close()\n",
    "        dictify_general(postext,poswordcounts)\n",
    "        dictify_general(postext,allwordcounts)\n",
    "    negwordcounts = dict()\n",
    "    for file in negfiles:\n",
    "        f = open(file,\"r\")\n",
    "        negtext = f.read() + \" \"\n",
    "        f.close()\n",
    "        dictify_general(negtext,negwordcounts)\n",
    "        dictify_general(negtext,allwordcounts)\n",
    "\n",
    "    print(\"there are {0} unique words in this corpus\".format(len(allwordcounts)))\n",
    "\n",
    "    # rip those dictionaries into lists for sorting\n",
    "    allwordsList = [word for word in allwordcounts]\n",
    "    allcountsList = [allwordcounts[allwordsList[i]] for i in range(len(allwordsList))]\n",
    "\n",
    "    # sort them\n",
    "    indexer = sorted(range(len(allcountsList)), key=lambda k: allcountsList[k], reverse=True)\n",
    "    allcountsListSorted = np.array([float(allcountsList[i]) for i in indexer])\n",
    "    allwordsListSorted = [allwordsList[i] for i in indexer]\n",
    "\n",
    "    return allcountsListSorted,allwordsListSorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bayes_score(all_counts,pos_params,neg_params,my_counts):\n",
    "    # uses all_counts_1k\n",
    "    # uses pos_params\n",
    "    # uses neg_params\n",
    "\n",
    "    # compute conditional\n",
    "    prob_pos = my_counts*pos_params/(all_counts/sum(all_counts))\n",
    "    prob_neg = my_counts*neg_params/(all_counts/sum(all_counts))\n",
    "    # add these things up carefulls\n",
    "    log_prob_pos = 0.0\n",
    "    for p in prob_pos:\n",
    "        if p > 0:\n",
    "            log_prob_pos += np.log(p)\n",
    "    log_prob_neg = 0.0\n",
    "    for p in prob_neg:\n",
    "        if p > 0:\n",
    "            log_prob_neg += np.log(p)\n",
    "\n",
    "    # normalize post-hoc\n",
    "    p_d = log_prob_pos+log_prob_neg\n",
    "\n",
    "    # return the difference\n",
    "    # if > 0, positive review\n",
    "    return log_prob_pos/p_d - log_prob_neg/p_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_bayes(all_words_1k,poswordcounts,negwordcounts):\n",
    "    \"\"\"Train bayes classifier using sorted word list, and dictionaries of counts from the two classes. Return the parameters\"\"\"\n",
    "    pos_counts = np.array([float(poswordcounts[word]) if word in poswordcounts else 0.0 for word in all_words_1k])\n",
    "    neg_counts = np.array([float(negwordcounts[word]) if word in negwordcounts else 0.0 for word in all_words_1k])\n",
    "    # print(pos_counts[:10])\n",
    "\n",
    "    # smoothing parameter\n",
    "    alpha = 1.0\n",
    "    pos_params = (pos_counts+alpha)/(np.sum(pos_counts)+alpha*len(pos_counts))\n",
    "    # print(pos_params[:10])\n",
    "\n",
    "    neg_params = (neg_counts+alpha)/(np.sum(neg_counts)+alpha*len(neg_counts))\n",
    "    # print(neg_params[:10])\n",
    "\n",
    "    return pos_params,neg_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naive_bayes():\n",
    "    posfiles = [\"../data/moviereviews/txt_sentoken/pos/\"+x for x in listdir(\"../data/moviereviews/txt_sentoken/pos\") if \".txt\" in x]\n",
    "    negfiles = [\"../data/moviereviews/txt_sentoken/neg/\"+x for x in listdir(\"../data/moviereviews/txt_sentoken/neg\") if \".txt\" in x]\n",
    "\n",
    "    print(len(posfiles))\n",
    "    print(len(negfiles))\n",
    "    # randomly select N reviews for training\n",
    "    training_size = 1400\n",
    "\n",
    "    pos_files_training = np.random.choice(np.arange(len(posfiles)),size=training_size/2,replace=False)\n",
    "    print(pos_files_training)\n",
    "    # go add up all the words for training\n",
    "    poswordcounts = dict()\n",
    "    for file in [posfiles[i] for i in pos_files_training]:\n",
    "        f = open(file,\"r\")\n",
    "        postext = f.read()\n",
    "        f.close()\n",
    "        dictify_general(postext,poswordcounts)\n",
    "    # select the negative reviews for training\n",
    "    neg_files_training = np.random.choice(np.arange(len(negfiles)),size=training_size/2,replace=False)\n",
    "    print(neg_files_training)\n",
    "    # go add up the words\n",
    "    negwordcounts = dict()\n",
    "    for file in [negfiles[i] for i in neg_files_training]:\n",
    "        f = open(file,\"r\")\n",
    "        negtext = f.read()\n",
    "        f.close()\n",
    "        dictify_general(negtext,negwordcounts)\n",
    "\n",
    "    all_counts,all_words = loadMovieReviews()\n",
    "    # take the top 1000 words\n",
    "    all_counts_1k = all_counts[30:5000]\n",
    "    all_words_1k = all_words[30:5000]\n",
    "    # build this for speed later\n",
    "    all_words_1k_dict = dict()\n",
    "    for i,word in enumerate(all_words_1k):\n",
    "        all_words_1k_dict[word] = i\n",
    "\n",
    "    pos_params,neg_params = train_bayes(all_words_1k,poswordcounts,negwordcounts)\n",
    "\n",
    "    # classify all positive reviews\n",
    "    print(\"positive reviews\")\n",
    "    correct_pos = np.zeros(len(posfiles)-training_size/2)\n",
    "    for i,file in enumerate([posfiles[i] for i in range(len(posfiles)) if i not in pos_files_training]):\n",
    "        f = open(file,\"r\")\n",
    "        postext = f.read()\n",
    "        f.close()\n",
    "        my_poswordcounts = dict()\n",
    "        dictify_general(postext,my_poswordcounts)\n",
    "        my_pos_counts = np.array([float(my_poswordcounts[word]) if word in my_poswordcounts else 0.0 for word in all_words_1k])\n",
    "\n",
    "        score = bayes_score(all_counts_1k,pos_params,neg_params,my_pos_counts)\n",
    "\n",
    "        if score > 0:\n",
    "            correct_pos[i] = 1\n",
    "\n",
    "    pos_accuracy = sum(correct_pos)/len(correct_pos)\n",
    "    print(sum(correct_pos)/len(correct_pos))\n",
    "\n",
    "    # classify all negative reviews\n",
    "    print(\"negative reviews\")\n",
    "    correct_neg = np.zeros(len(negfiles)-training_size/2)\n",
    "    for i,file in enumerate([negfiles[i] for i in range(len(negfiles)) if i not in neg_files_training]):\n",
    "        f = open(file,\"r\")\n",
    "        negtext = f.read()\n",
    "        f.close()\n",
    "        my_wordcounts = dict()\n",
    "        dictify_general(negtext,my_wordcounts)\n",
    "        my_counts = np.array([float(my_wordcounts[word]) if word in my_wordcounts else 0.0 for word in all_words_1k])\n",
    "\n",
    "        score = bayes_score(all_counts_1k,pos_params,neg_params,my_counts)\n",
    "\n",
    "        if score < 0:\n",
    "            # print(\"correct, score={0}\".format(score))\n",
    "            correct_neg[i] = 1\n",
    "\n",
    "    neg_accuracy = sum(correct_neg)/len(correct_neg)\n",
    "    print(sum(correct_neg)/len(correct_neg))\n",
    "\n",
    "    print(\"overall accuracy: {0:.1f}\".format((pos_accuracy+neg_accuracy)/2*100))\n",
    "    \n",
    "\n",
    "    allLengths = [                   1,   2,   3,   5,   7, 10, 15, 25, 40, 60, 80,100,150,250,400,600,900,]\n",
    "    allSamples = [int(np.floor(1000-training_size/2)),2000,1500,1500,1000,900,750,600,500,250,100, 75, 75, 40, 25, 15,  1,]\n",
    "    # allLengths = []\n",
    "\n",
    "    files = [posfiles[i] for i in range(len(posfiles)) if i not in pos_files_training]\n",
    "\n",
    "    # store all of the scores here\n",
    "    pos_results_all = [[] for i in range(len(allLengths))]\n",
    "    # store the mean of those here\n",
    "    pos_means = [0.0 for i in range(len(allLengths))]\n",
    "    # store the std of those here\n",
    "    pos_std = [0.0 for i in range(len(allLengths))]\n",
    "\n",
    "    for k,numReviews in enumerate(allLengths):\n",
    "        numSamples = allSamples[k]\n",
    "        print(\"pos: taking {0} samples of {1} reviews\".format(numSamples,numReviews))\n",
    "\n",
    "        if numReviews == 1:\n",
    "            choose_randomly = False\n",
    "        else:\n",
    "            choose_randomly = True\n",
    "\n",
    "        scores = [0.0 for i in range(numSamples)]\n",
    "        for i in range(numSamples):\n",
    "            # print(\"on sample {0}\".format(i))\n",
    "\n",
    "            if choose_randomly:\n",
    "                if len(files) > numReviews:\n",
    "                    print(\"WARN the number of files {} is greater than reviews {}\".format(len(files),numReviews))\n",
    "                my_files = np.random.choice(files,size=min([numReviews,len(files)]),replace=False)\n",
    "            else:\n",
    "                my_files = [files[i]]\n",
    "\n",
    "            # forget the string expansion\n",
    "            # let\"s store them as a dict\n",
    "            # allwordcounts = dict()\n",
    "            # nah, let's store them as in the vector\n",
    "            allwordcounts = np.zeros(len(all_counts_1k))\n",
    "            for file in my_files:\n",
    "                ########################################\n",
    "                # this loads the files\n",
    "                f = open(file,\"r\")\n",
    "                rawtext = f.read()\n",
    "                f.close()\n",
    "                # add to the full dict\n",
    "                # dictify_general(rawtext,allwordcounts)\n",
    "                # add it to the vector\n",
    "                my_list = listify(rawtext)\n",
    "                for word in my_list:\n",
    "                    if word in all_words_1k_dict:\n",
    "                        allwordcounts[all_words_1k_dict[word]] += 1\n",
    "\n",
    "            scores[i] = bayes_score(all_counts_1k,pos_params,neg_params,allwordcounts)\n",
    "\n",
    "        # now save those scores\n",
    "        pos_results_all[k] = scores\n",
    "        pos_means[k] = np.mean(scores)\n",
    "        print(\"mean score is {0}\".format(pos_means[k]))\n",
    "        pos_std[k] = np.std(scores)\n",
    "\n",
    "    files = [negfiles[i] for i in range(len(negfiles)) if i not in neg_files_training]\n",
    "\n",
    "    # store all of the scores here\n",
    "    neg_results_all = [[] for i in range(len(allLengths))]\n",
    "    # store the mean of those here\n",
    "    neg_means = [0.0 for i in range(len(allLengths))]\n",
    "    # store the std of those here\n",
    "    neg_std = [0.0 for i in range(len(allLengths))]\n",
    "\n",
    "    for k,numReviews in enumerate(allLengths):\n",
    "        numSamples = allSamples[k]\n",
    "        print(\"neg: taking {0} samples of {1} reviews\".format(numSamples,numReviews))\n",
    "\n",
    "        if numReviews == 1:\n",
    "            choose_randomly = False\n",
    "        else:\n",
    "            choose_randomly = True\n",
    "\n",
    "        scores = [0.0 for i in range(numSamples)]\n",
    "        for i in range(numSamples):\n",
    "            # print(\"on sample {0}\".format(i))\n",
    "\n",
    "            if choose_randomly:\n",
    "                if len(files) > numReviews:\n",
    "                    print(\"WARN the number of files {} is greater than reviews {}\".format(len(files),numReviews))\n",
    "                my_files = np.random.choice(files,size=min([numReviews,len(files)]),replace=False)\n",
    "            else:\n",
    "                my_files = [files[i]]\n",
    "\n",
    "\n",
    "            # forget the string expansion\n",
    "            # let\"s store them as a dict\n",
    "            # allwordcounts = dict()\n",
    "            # nah, let's store them as in the vector\n",
    "            allwordcounts = np.zeros(len(all_counts_1k))\n",
    "            for file in my_files:\n",
    "                ########################################\n",
    "                # this loads the files\n",
    "                f = open(file,\"r\")\n",
    "                rawtext = f.read()\n",
    "                f.close()\n",
    "                # add to the full dict\n",
    "                # dictify_general(rawtext,allwordcounts)\n",
    "                # add it to the vector\n",
    "                my_list = listify(rawtext)\n",
    "                for word in my_list:\n",
    "                    if word in all_words_1k_dict:\n",
    "                        allwordcounts[all_words_1k_dict[word]] += 1\n",
    "\n",
    "            scores[i] = bayes_score(all_counts_1k,pos_params,neg_params,allwordcounts)\n",
    "\n",
    "        # now save those scores\n",
    "        neg_results_all[k] = scores\n",
    "        neg_means[k] = np.mean(scores)\n",
    "        print(\"mean score is {0}\".format(neg_means[k]))\n",
    "        neg_std[k] = np.std(scores)\n",
    "\n",
    "    # print(neg_results_all)\n",
    "    # print(pos_results_all)\n",
    "    print(neg_means)\n",
    "    print(pos_means)\n",
    "    print(neg_std)\n",
    "    print(pos_std)\n",
    "\n",
    "    overlapping = np.zeros(len(allLengths))\n",
    "    for i in range(len(allLengths)):\n",
    "        average_score = np.mean(neg_results_all[i]+pos_results_all[i])\n",
    "        overlapping[i] = float(len(np.where(pos_results_all[i] < average_score)[0]) + len(np.where(neg_results_all[i] > average_score)[0])) / ( len( neg_results_all[i] ) + len( pos_results_all[i] ) )\n",
    "\n",
    "    print(overlapping)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_axes([0.15,0.2,0.7,0.7])\n",
    "    ax1.plot(np.log10(allLengths),overlapping,linewidth=2,color=\"#ef8a62\",)\n",
    "    ax1.set_xlabel(\"log10(Number of Reviews)\")\n",
    "    ax1.set_ylabel(\"Fraction overlapping\")\n",
    "    # xlim with a little space\n",
    "    ax1.set_xlim(np.log10([allLengths[0]-.1,allLengths[-1]+.1])) \n",
    "    ax1.set_title(\"Sentiment over many random samples for {0}\".format(\"Naive Bayes\"))\n",
    "    # mysavefig(\"moviereviews-scores-{0}.png\".format(\"naive-bayes\"))\n",
    "    mysavefig(\"moviereviews-scores-{0}.pdf\".format(\"naive-bayes\"),folder=\"../figures/NB\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_axes([0.15,0.2,0.7,0.7])\n",
    "    ax1.errorbar(np.log10(allLengths),pos_means,pos_std,linewidth=2,color=\"#ef8a62\",)\n",
    "    ax1.errorbar(np.log10(allLengths),neg_means,neg_std,linewidth=2,color=\"#2b8cbe\",)\n",
    "    ax1.legend([\"Positive reviews\",\"Negative reviews\"],loc=\"best\")\n",
    "    ax1.set_xlabel(\"log10(Number of Reviews)\")\n",
    "    ax1.set_ylabel(\"Sentiment\")\n",
    "    # xlim with a little space\n",
    "    ax1.set_xlim(np.log10([allLengths[0]-.1,allLengths[-1]+.1])) \n",
    "    # ax1.set_ylim([0,24])\n",
    "    # plt.xticks([float(i)+0.5 for i in range(4)])\n",
    "    # plt.yticks([float(i)+0.5 for i in range(3)])\n",
    "    # ax1.set_xticklabels([1,5,25,50])\n",
    "    # ax1.set_yticklabels([22,28,35])\n",
    "    ax1.set_title(\"Sentiment over many random samples for {0}\".format(\"Naive Bayes\"))\n",
    "    # mysavefig(\"moviereviews-{0}.png\".format(\"naive-bayes\"))\n",
    "    mysavefig(\"moviereviews-{0}.pdf\".format(\"naive-bayes\"),folder=\"../figures/NB\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NB_on_NYT():\n",
    "    \"\"\"Train NB on movie reviews, test it on NYT Society section.\"\"\"\n",
    "\n",
    "    print(\"training\")\n",
    "    posfiles = [\"../data/moviereviews/txt_sentoken/pos/\"+x for x in listdir(\"../data/moviereviews/txt_sentoken/pos\") if \".txt\" in x]\n",
    "    negfiles = [\"../data/moviereviews/txt_sentoken/neg/\"+x for x in listdir(\"../data/moviereviews/txt_sentoken/neg\") if \".txt\" in x]\n",
    "\n",
    "    # randomly select N reviews for training\n",
    "    training_size = 1400\n",
    "\n",
    "    pos_files_training = np.random.choice(np.arange(len(posfiles)),size=training_size/2,replace=False)\n",
    "    # go add up all the words for training\n",
    "    poswordcounts = dict()\n",
    "    for file in [posfiles[i] for i in pos_files_training]:\n",
    "        f = open(file,\"r\")\n",
    "        postext = f.read()\n",
    "        f.close()\n",
    "        dictify_general(postext,poswordcounts)\n",
    "    # select the negative reviews for training\n",
    "    neg_files_training = np.random.choice(np.arange(len(negfiles)),size=training_size/2,replace=False)\n",
    "    # go add up the words\n",
    "    negwordcounts = dict()\n",
    "    for file in [negfiles[i] for i in neg_files_training]:\n",
    "        f = open(file,\"r\")\n",
    "        negtext = f.read()\n",
    "        f.close()\n",
    "        dictify_general(negtext,negwordcounts)\n",
    "\n",
    "    all_counts,all_words = loadMovieReviews()\n",
    "    # take the top 1000 words\n",
    "    all_counts_1k = all_counts[30:5000]\n",
    "    all_words_1k = all_words[30:5000]\n",
    "    # build this for speed later\n",
    "    all_words_1k_dict = dict()\n",
    "    for i,word in enumerate(all_words_1k):\n",
    "        all_words_1k_dict[word] = i\n",
    "\n",
    "    pos_params,neg_params = train_bayes(all_words_1k,poswordcounts,negwordcounts)\n",
    "\n",
    "    # above from naive_bayes() function\n",
    "    # now get the NYT section loaded\n",
    "    print(\"loading NYT\")\n",
    "    sections = [\"arts\",\"books\",\"classified\",\"cultural\",\"editorial\",\"education\",\"financial\",\"foreign\",\"home\",\"leisure\",\"living\",\"magazine\",\"metropolitan\",\"movies\",\"national\",\"regional\",\"science\",\"society\",\"sports\",\"style\",\"television\",\"travel\",\"week-in-review\",\"weekend\",]\n",
    "    scores = np.zeros(len(sections))\n",
    "    for i,section in enumerate(sections):\n",
    "        a = pickle.load( open(\"../data/nyt/sections/NYT_{0}.dict\".format(section), \"rb\") )\n",
    "\n",
    "        allwordcounts = np.zeros(len(all_counts_1k))\n",
    "        for word in a:\n",
    "            if word in all_words_1k_dict:\n",
    "                allwordcounts[all_words_1k_dict[word]] = a[word]\n",
    "\n",
    "        print(\"scoring\")\n",
    "        score = bayes_score(all_counts_1k,pos_params,neg_params,allwordcounts)\n",
    "        print(\"{0} score={1}\".format(section,score))\n",
    "        scores[i] = score\n",
    "    indexer = sorted(range(len(scores)),key=lambda k: scores[k],reverse=True)\n",
    "    sections_sorted = [sections[i] for i in indexer]\n",
    "    my_happs_sorted = np.array([scores[i] for i in indexer])\n",
    "    print(sections_sorted)\n",
    "\n",
    "    avg_happs_unweighted = my_happs_sorted.mean()\n",
    "\n",
    "    fig = plt.figure(figsize=(12,10))\n",
    "    ax = fig.add_axes([.2,.2,.7,.7])\n",
    "    # ax.bar(np.arange(len(sections)),np.array(my_happs_sorted),orientation=\"vertical\")\n",
    "    # ax.bar(0,.8,np.array(my_happs_sorted)-avg_happs,bottom=np.arange(len(sections)),orientation=\"horizontal\")\n",
    "    bar_height = 0.8\n",
    "    happs_diff = np.array(my_happs_sorted)-avg_happs_unweighted\n",
    "    rects1 = ax.bar(0,bar_height,happs_diff,bottom=np.arange(len(sections)),orientation=\"horizontal\")\n",
    "    ax.set_ylim([bar_height-1,len(sections)])\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xlabel(\"Happs diff from unweighted average\")\n",
    "    ax.set_title(\"Ranking of NYT Sections by {0}\".format(\"NB\"))\n",
    "\n",
    "    def autolabel(rects):\n",
    "        # attach some text labels\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            width = rect.get_width()\n",
    "            y = int(np.floor(rect.get_y()))\n",
    "\n",
    "            x = rect.get_x()\n",
    "            if happs_diff[y] > 0:\n",
    "                ax.text(-0.0005, y+bar_height/2., \"{0}. {1}\".format(y+1,sections_sorted[y].capitalize()),\n",
    "                        ha=\"right\", va=\"center\")\n",
    "            else:\n",
    "                ax.text(.0005, y+bar_height/2., \"{0}. {1}\".format(y+1,sections_sorted[y].capitalize()),\n",
    "                        ha=\"left\", va=\"center\")\n",
    "\n",
    "    autolabel(rects1)\n",
    "\n",
    "    # mysavefig(\"NYT-sorted-{0}.png\".format(\"NB\"))\n",
    "    mysavefig(\"NYT-sorted-{0}.pdf\".format(\"NB\"),folder=\"../figures/nyt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NB_wordshift(q=False):\n",
    "    print(\"training\")\n",
    "    posfiles = [\"../data/moviereviews/txt_sentoken/pos/\"+x for x in listdir(\"../data/moviereviews/txt_sentoken/pos\") if \".txt\" in x]\n",
    "    negfiles = [\"../data/moviereviews/txt_sentoken/neg/\"+x for x in listdir(\"../data/moviereviews/txt_sentoken/neg\") if \".txt\" in x]\n",
    "\n",
    "    # randomly select N reviews for training\n",
    "    training_size = 1400\n",
    "\n",
    "    pos_files_training = np.random.choice(np.arange(len(posfiles)),size=training_size/2,replace=False)\n",
    "    # go add up all the words for training\n",
    "    poswordcounts = dict()\n",
    "    for file in [posfiles[i] for i in pos_files_training]:\n",
    "        f = open(file,\"r\")\n",
    "        postext = f.read()\n",
    "        f.close()\n",
    "        dictify_general(postext,poswordcounts)\n",
    "    # select the negative reviews for training\n",
    "    neg_files_training = np.random.choice(np.arange(len(negfiles)),size=training_size/2,replace=False)\n",
    "    # go add up the words\n",
    "    negwordcounts = dict()\n",
    "    for file in [negfiles[i] for i in neg_files_training]:\n",
    "        f = open(file,\"r\")\n",
    "        negtext = f.read()\n",
    "        f.close()\n",
    "        dictify_general(negtext,negwordcounts)\n",
    "\n",
    "    all_counts,all_words = loadMovieReviews()\n",
    "    # take the top 1000 words\n",
    "    all_counts_1k = all_counts[30:5000]\n",
    "    all_words_1k = all_words[30:5000]\n",
    "    # build this for speed later\n",
    "    all_words_1k_dict = dict()\n",
    "    for i,word in enumerate(all_words_1k):\n",
    "        all_words_1k_dict[word] = i\n",
    "\n",
    "    pos_params,neg_params = train_bayes(all_words_1k,poswordcounts,negwordcounts)\n",
    "\n",
    "    my_counts = np.ones(len(all_counts_1k))\n",
    "    # given a text word vector my_counts, the unweighted probabilities are this\n",
    "    prob_pos = my_counts*pos_params/(all_counts_1k/sum(all_counts_1k))\n",
    "    prob_neg = my_counts*neg_params/(all_counts_1k/sum(all_counts_1k))\n",
    "    # really just each vector multiplied together\n",
    "    # but we take the log, and sum (for floating point-ness)\n",
    "    \n",
    "    # now let's normalize this, per peer request:\n",
    "    prob_pos = prob_pos/prob_pos.sum()\n",
    "    prob_neg = prob_neg/prob_neg.sum()\n",
    "\n",
    "    # using 1's, and taking the max of the ratios will give the most informative\n",
    "    ratio_pos = prob_pos/prob_neg\n",
    "    ratio_neg = prob_neg/prob_pos\n",
    "\n",
    "    indexer = sorted(range(len(ratio_pos)),key=lambda k: ratio_pos[k],reverse=True)\n",
    "    ratio_pos_sorted = [ratio_pos[i] for i in indexer]\n",
    "    words_pos_sorted = [all_words_1k[i] for i in indexer]\n",
    "    indexer = sorted(range(len(ratio_pos)),key=lambda k: ratio_neg[k],reverse=True)\n",
    "    ratio_neg_sorted = [ratio_neg[i] for i in indexer]\n",
    "    words_neg_sorted = [all_words_1k[i] for i in indexer]\n",
    "    \n",
    "    movie_results = [[],[],[],[]]\n",
    "\n",
    "    if not q:\n",
    "        print(\"-\"*80)\n",
    "        print(\"-\"*80)\n",
    "        print(\"top positive words:\")\n",
    "    for i in range(10):\n",
    "        movie_results[0].append([ratio_pos_sorted[i],words_pos_sorted[i]])\n",
    "        if not q:\n",
    "            print(\"{1}: {0}\".format(ratio_pos_sorted[i],words_pos_sorted[i]))\n",
    "    if not q:\n",
    "        print(\"-\"*80)\n",
    "        print(\"top negative words:\")\n",
    "    for i in range(10):\n",
    "        movie_results[1].append([ratio_neg_sorted[i],words_neg_sorted[i]])\n",
    "        if not q:\n",
    "            print(\"{1}: {0}\".format(ratio_neg_sorted[i],words_neg_sorted[i]))\n",
    "    if not q:\n",
    "        print(\"-\"*80)\n",
    "        print(\"-\"*80)\n",
    "\n",
    "    # using 1's, and taking the max of the ratios will give the most informative\n",
    "    diff_pos = prob_pos-prob_neg\n",
    "    diff_neg = prob_neg-prob_pos\n",
    "\n",
    "    indexer = sorted(range(len(diff_pos)),key=lambda k: diff_pos[k],reverse=True)\n",
    "    diff_pos_sorted = [diff_pos[i] for i in indexer]\n",
    "    words_pos_sorted = [all_words_1k[i] for i in indexer]\n",
    "    indexer = sorted(range(len(diff_neg)),key=lambda k: diff_neg[k],reverse=True)\n",
    "    diff_neg_sorted = [diff_neg[i] for i in indexer]\n",
    "    words_neg_sorted = [all_words_1k[i] for i in indexer]\n",
    "\n",
    "    if not q:\n",
    "        print(\"top positive words:\")\n",
    "    for i in range(10):\n",
    "        movie_results[2].append([diff_pos_sorted[i],words_pos_sorted[i]])\n",
    "        if not q:\n",
    "            print(\"{1}: {0}\".format(diff_pos_sorted[i],words_pos_sorted[i]))\n",
    "    if not q:\n",
    "        print(\"-\"*80)\n",
    "        print(\"top negative words:\")\n",
    "    for i in range(10):\n",
    "        movie_results[3].append([diff_neg_sorted[i],words_neg_sorted[i]])\n",
    "        if not q:\n",
    "            print(\"{1}: {0}\".format(diff_neg_sorted[i],words_neg_sorted[i]))\n",
    "    if not q:\n",
    "        print(\"-\"*80)\n",
    "        print(\"-\"*80)\n",
    "    \n",
    "    section=\"society\"\n",
    "    a = pickle.load( open(\"../data/nyt/sections/NYT_{0}.dict\".format(section), \"rb\") )\n",
    "\n",
    "    allwordcounts = np.zeros(len(all_counts_1k))\n",
    "    for word in a:\n",
    "        if word in all_words_1k_dict:\n",
    "            allwordcounts[all_words_1k_dict[word]] = a[word]\n",
    "\n",
    "    prob_pos = allwordcounts*pos_params/(all_counts_1k/sum(all_counts_1k))\n",
    "    prob_neg = allwordcounts*neg_params/(all_counts_1k/sum(all_counts_1k))\n",
    "    \n",
    "    # now let's normalize this, per peer request:\n",
    "    prob_pos = prob_pos/prob_pos.sum()\n",
    "    prob_neg = prob_neg/prob_neg.sum()\n",
    "\n",
    "    # using 1's, and taking the max of the ratios will give the most informative\n",
    "    ratio_pos = prob_pos/prob_neg\n",
    "    ratio_neg = prob_neg/prob_pos\n",
    "\n",
    "    indexer = sorted(range(len(ratio_pos)),key=lambda k: ratio_pos[k],reverse=True)\n",
    "    ratio_pos_sorted = [ratio_pos[i] for i in indexer]\n",
    "    words_pos_sorted = [all_words_1k[i] for i in indexer]\n",
    "    indexer = sorted(range(len(ratio_pos)),key=lambda k: ratio_neg[k],reverse=True)\n",
    "    ratio_neg_sorted = [ratio_neg[i] for i in indexer]\n",
    "    words_neg_sorted = [all_words_1k[i] for i in indexer]\n",
    "    \n",
    "    times_results = [[],[],[],[]]\n",
    "\n",
    "    if not q:\n",
    "        print(\"-\"*80)\n",
    "        print(\"-\"*80)\n",
    "        print(\"top positive words:\")\n",
    "    for i in range(10):\n",
    "        times_results[0].append([ratio_pos_sorted[i],words_pos_sorted[i]])\n",
    "        if not q:\n",
    "            print(\"{1}: {0}\".format(ratio_pos_sorted[i],words_pos_sorted[i]))\n",
    "    if not q:\n",
    "        print(\"-\"*80)\n",
    "        print(\"top negative words:\")\n",
    "    for i in range(10):\n",
    "        times_results[1].append([ratio_neg_sorted[i],words_neg_sorted[i]])\n",
    "        if not q:\n",
    "            print(\"{1}: {0}\".format(ratio_neg_sorted[i],words_neg_sorted[i]))\n",
    "    if not q:\n",
    "        print(\"-\"*80)\n",
    "        print(\"-\"*80)\n",
    "\n",
    "    # using 1's, and taking the max of the ratios will give the most informative\n",
    "    diff_pos = prob_pos-prob_neg\n",
    "    diff_neg = prob_neg-prob_pos\n",
    "\n",
    "    indexer = sorted(range(len(diff_pos)),key=lambda k: diff_pos[k],reverse=True)\n",
    "    diff_pos_sorted = [diff_pos[i] for i in indexer]\n",
    "    words_pos_sorted = [all_words_1k[i] for i in indexer]\n",
    "    indexer = sorted(range(len(diff_neg)),key=lambda k: diff_neg[k],reverse=True)\n",
    "    diff_neg_sorted = [diff_neg[i] for i in indexer]\n",
    "    words_neg_sorted = [all_words_1k[i] for i in indexer]\n",
    "\n",
    "    if not q:\n",
    "        print(\"top positive words:\")\n",
    "    for i in range(10):\n",
    "        times_results[2].append([diff_pos_sorted[i],words_pos_sorted[i]])\n",
    "        if not q:\n",
    "            print(\"{1}: {0}\".format(diff_pos_sorted[i],words_pos_sorted[i]))\n",
    "    if not q:\n",
    "        print(\"-\"*80)\n",
    "        print(\"top negative words:\")\n",
    "    for i in range(10):\n",
    "        times_results[3].append([diff_neg_sorted[i],words_neg_sorted[i]])\n",
    "        if not q:\n",
    "            print(\"{1}: {0}\".format(diff_neg_sorted[i],words_neg_sorted[i]))\n",
    "    if not q:\n",
    "        print(\"-\"*80)\n",
    "        print(\"-\"*80)\n",
    "\n",
    "    score = bayes_score(all_counts_1k,pos_params,neg_params,allwordcounts)\n",
    "\n",
    "    if not q:\n",
    "        print(\"{0} score={1}\".format(section,score))\n",
    "    \n",
    "    return movie_results,times_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bayes_many_samples(n_trials):\n",
    "    posfiles = [\"../data/moviereviews/txt_sentoken/pos/\"+x for x in listdir(\"../data/moviereviews/txt_sentoken/pos\") if \".txt\" in x]\n",
    "    negfiles = [\"../data/moviereviews/txt_sentoken/neg/\"+x for x in listdir(\"../data/moviereviews/txt_sentoken/neg\") if \".txt\" in x]\n",
    "\n",
    "    results = [0.0 for i in range(n_trials)]\n",
    "    n_stopwords = 0\n",
    "\n",
    "    for trial_i in range(n_trials):\n",
    "\n",
    "        print(len(posfiles))\n",
    "        print(len(negfiles))\n",
    "        # randomly select N reviews for training\n",
    "        training_size = 200\n",
    "    \n",
    "        pos_files_training = np.random.choice(np.arange(len(posfiles)),size=training_size/2,replace=False)\n",
    "        print(pos_files_training)\n",
    "        # go add up all the words for training\n",
    "        poswordcounts = dict()\n",
    "        for file in [posfiles[i] for i in pos_files_training]:\n",
    "            f = open(file,\"r\")\n",
    "            postext = f.read()\n",
    "            f.close()\n",
    "            dictify_general(postext,poswordcounts)\n",
    "        # select the negative reviews for training\n",
    "        neg_files_training = np.random.choice(np.arange(len(negfiles)),size=training_size/2,replace=False)\n",
    "        print(neg_files_training)\n",
    "        # go add up the words\n",
    "        negwordcounts = dict()\n",
    "        for file in [negfiles[i] for i in neg_files_training]:\n",
    "            f = open(file,\"r\")\n",
    "            negtext = f.read()\n",
    "            f.close()\n",
    "            dictify_general(negtext,negwordcounts)\n",
    "    \n",
    "        all_counts,all_words = loadMovieReviews()\n",
    "        # take the top 1000 words\n",
    "        all_counts_1k = all_counts[n_stopwords:5000]\n",
    "        all_words_1k = all_words[n_stopwords:5000]\n",
    "        # build this for speed later\n",
    "        all_words_1k_dict = dict()\n",
    "        for i,word in enumerate(all_words_1k):\n",
    "            all_words_1k_dict[word] = i\n",
    "    \n",
    "        pos_params,neg_params = train_bayes(all_words_1k,poswordcounts,negwordcounts)\n",
    "    \n",
    "        # classify all positive reviews\n",
    "        print(\"positive reviews\")\n",
    "        correct_pos = np.zeros(len(posfiles)-training_size/2)\n",
    "        for i,file in enumerate([posfiles[i] for i in range(len(posfiles)) if i not in pos_files_training]):\n",
    "            f = open(file,\"r\")\n",
    "            postext = f.read()\n",
    "            f.close()\n",
    "            my_poswordcounts = dict()\n",
    "            dictify_general(postext,my_poswordcounts)\n",
    "            my_pos_counts = np.array([float(my_poswordcounts[word]) if word in my_poswordcounts else 0.0 for word in all_words_1k])\n",
    "    \n",
    "            score = bayes_score(all_counts_1k,pos_params,neg_params,my_pos_counts)\n",
    "    \n",
    "            if score > 0:\n",
    "                correct_pos[i] = 1\n",
    "    \n",
    "        pos_accuracy = sum(correct_pos)/len(correct_pos)\n",
    "        print(sum(correct_pos)/len(correct_pos))\n",
    "    \n",
    "        # classify all negative reviews\n",
    "        print(\"negative reviews\")\n",
    "        correct_neg = np.zeros(len(negfiles)-training_size/2)\n",
    "        for i,file in enumerate([negfiles[i] for i in range(len(negfiles)) if i not in neg_files_training]):\n",
    "            f = open(file,\"r\")\n",
    "            negtext = f.read()\n",
    "            f.close()\n",
    "            my_wordcounts = dict()\n",
    "            dictify_general(negtext,my_wordcounts)\n",
    "            my_counts = np.array([float(my_wordcounts[word]) if word in my_wordcounts else 0.0 for word in all_words_1k])\n",
    "    \n",
    "            score = bayes_score(all_counts_1k,pos_params,neg_params,my_counts)\n",
    "    \n",
    "            if score < 0:\n",
    "                # print(\"correct, score={0}\".format(score))\n",
    "                correct_neg[i] = 1\n",
    "    \n",
    "        neg_accuracy = sum(correct_neg)/len(correct_neg)\n",
    "        print(sum(correct_neg)/len(correct_neg))\n",
    "    \n",
    "        print(\"overall accuracy: {0:.1f}\".format((pos_accuracy+neg_accuracy)/2*100))\n",
    "        results[trial_i] = (pos_accuracy+neg_accuracy)/2*100\n",
    "        \n",
    "    print(np.mean(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "[ 50 332  61 108 911 869 907 855 287 104 715 463 396 676 360 174 306  23\n",
      " 746 350 421 893 924 725 525 771 829  10 951 561 912 409 521 791 329 905\n",
      "  57 109 344 783 910 239  15 316 336 391 237  36 304  54 380 425  39 150\n",
      " 854 217 423 250 169 379 164  62 978 983 963 229 590  29   6 471 682 913\n",
      " 681 690  94 858  91 751 949  26 114 530 609 916  24 467 318 885 976  98\n",
      " 480 187 112 151 132 514 866 802 620 524 752 223 662 877 705 921 347 311\n",
      " 625 429 645 309 339 498  18 482 389 148 646 215  59  84 527  33 558 205\n",
      " 272 477 553 902 445 671 119  81 133 440 915 177 592 330 999 582 608 823\n",
      " 874 932 770 190 712 518 979 696 692 180 265 134 914 576 879  67 750 299\n",
      " 635 300 761 183 772 673 678  42 247 126 737 397 341 985 570 709 624 515\n",
      " 629 936 663 792 224 850 125 568 457  49 880 953 956 232 967 365 691  85\n",
      " 734 356 405 896  90 756 594 377 935 722 744 260 768 175 947  40 333 157\n",
      " 809 404 354 142 426 825 728 438 549 578 776 923 909  63 711 460 543 455\n",
      " 495 123 542  16 714 610 129 595 989 721 442 886 406 786 418 349 906 944\n",
      "  47   0 614 470 189 465 131 637 943 547 656 816 331 238 424 173 512 158\n",
      " 851 292 925 357 501 865 787  72 101  32 288 733 163 812 390 642 454 589\n",
      " 281 317 307 698 274 685 264 933 441 674 510 227 655 617 631 416 236 736\n",
      " 420 257 895 117 862 962 355 833 602 204 248 118 220 484 740 649 263 520\n",
      " 668  53 221 878 233 781 838 612  52 193 587 988 788 167 627 361 363 343\n",
      " 144 882 115 228 675   7  68 511 273 504 765  80 773  82 147 687 166 555\n",
      " 876 738 965 665 417 242 124 640 603 209 644 200 388 135 186 822 732 839\n",
      " 998 735 968 563 145 847 531 616 461 407 717 523 323 903  12 679 305 301\n",
      " 130 769 652 660 596 422 707 376 160 716 845  13 872 230 138 946 661 102\n",
      " 106 342 371 683 516 564 826 122 491 386 871 255 996 938 672 289 234 487\n",
      " 532 571 964 605 701 170 459 986 848 427 308 348 984 991 883 479 918 718\n",
      " 585 485 719 759 372 249 766  78 606 451 382 820 437 942 548 689 818 214\n",
      " 680 528 982 181 860 941 544 569 201 572 210 695 700 483  51 664 797 235\n",
      " 351  48 904  69 573 435 312 154 554 269 650 384 615 182  30 324 922 216\n",
      " 103 937 607   9 981 231 730 446 337 408 222 476  79  86 334 994 313 203\n",
      " 496  43 474 207 472 168 840 599 297 731 653 199 927 908 488 892 412 370\n",
      "  73 469 280  76 891 176 321 387 853 507 458 775 760 340 817  71 805 198\n",
      " 782 975 267 503 747 139 443 969 632 804 785  22 275  60 897 800 185 385\n",
      " 208 881 793 803 997 961 540 618  44 415  46 326 824  31 931  35 811 141\n",
      " 245 754  28 657 213 832 322 890  37 159  99 155 748 767 917 934 219 279\n",
      " 268 849 704 398 584  56 794 970 411 107 626 546 541 601 475 383 684  75\n",
      " 694 256 901 842 345 954 613 303   2 393 315 837 807 282 191 276 834 741\n",
      " 729 727 394 831 801 846  92 225 764 566 121 254  11   1 502 286 266 808\n",
      " 431 559 178 314 667 859 466 813 359 864 493 212 863 702 400 971 591 258\n",
      " 367 611  25  27 723 244 110 259 552 575 196 368 358 857 373 432]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:10: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:20: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[685 310 726 758 657 302 432 710 207 608 424 383 446 354 605 515 471 991\n",
      "  11 107 663 693 952 771 277 925 443 967 372 417 136 303 102 542 901 978\n",
      " 865 528 951 384 986 634  65 735 485 686 220 607 772 885 274 947 705 153\n",
      " 692 932 465 550  43 479 422 418 996 714 350 680 416 503 218  84 237 230\n",
      "   6 177 829 850  41   3 308 992 238 451 834 147 994 561 352 813 176  87\n",
      " 995 700  75 468 210 777 512 588 190 963 556 325 154 653 555 239 143 669\n",
      " 201 842 821 161 344  52 839  80 926 174 221  81 367  62 775  12 275  89\n",
      " 779 142 211 884 621 314 670 413 618 435 349 903 114 856 427  51 927 825\n",
      " 345 724 944 338 320 844 487 254 317 981 339 674 498 708 436 306 453 869\n",
      " 565 261 116 440 209 729 364 104 938 864 566 571 182  93 300 493 523 584\n",
      " 815 636 233  59 887 284  47 199 545 386 880 703  19 326 313 589 799 833\n",
      " 172 259 551 149  77 695 666   5 905 534 731 699 343  39 366 334  40 581\n",
      " 678 629 444  55 100  23 288 878 494  50 838 539 433  31 613 508 906 140\n",
      " 658 321 573 393 150 808 623 617 910 513 762 390 537  38 659  94 175 507\n",
      " 476 647 890 888 701 916 723 626 506 972 677  68  88 304 939 378 458 774\n",
      " 681 293 738  97 894 215 679 912 490 272 578 950 696 990 389 181 501 861\n",
      " 423 516 489 921 781 139 388  34 291 554 519 347 590 682 826  26 985 544\n",
      " 106 205 522 637 743 980 698 392  66 802 859 509 227 445 586 200 504 335\n",
      " 803 428 809 919 159 530 450 341 315 368  30 316 401 900 945 111 148 652\n",
      " 619 290 311 540 430 135 750 720 120 817 736 298  46 213 242 684  82 707\n",
      " 576 818 217 184 187 999 462 330   9 983 602 600 582 722 913 553 229 164\n",
      " 327  60 870 112 971 278 375 965 716 255 760 730 752 648 749 847 641  25\n",
      " 262 146 757 598 941 412 612 611 457 517 929 322 804 948 406 295 739 745\n",
      "   8 868 332 961 609 673 610  45 216 668 289 226 270 644 896 998 688  37\n",
      "  14 923 822 866 646 299 746  29 301 382  49 560 505 492 266 798 356 928\n",
      "  85 478 396 633 362 841  91 482 649 725 122 269 398 222 845 532  99 466\n",
      " 531 552 930 246 409  20 286 263 877 956 836 535  22 285 497 178 810 470\n",
      " 399   2 336 630 225 533 253  53 186 256 917 846 862 728  92 543 977 133\n",
      " 892 937 591 271 898 755  35 502 788 379 579 784 675 234 853 721 665  74\n",
      " 891 387  70   7 672 265 702 740 993 949 342 208 568 882 361 374 138 546\n",
      " 594 895 719 800 639  71 793 848  72 296 307 168 704 791 805 811 655 570\n",
      " 258 472 875 380 180 638 643 946 712 814 940 475 662 224 130 624  18 603\n",
      " 405 569 962 595 627 260 854 954 796 456 365 101 402  63 109 179  98 908\n",
      " 441 283 189 744  95 960 469 969 103 970 480 640 902 987 651 754 860 448\n",
      " 768 858 935 294 324 463  67 105 128 411 369 789 267 377 353 481 328 936\n",
      " 333 631  79 907 474 871 783 273 410 160 765 776 419 671 157 251 718 564\n",
      " 108 371 574 173 460 126 521 563 155 683 934 694 117 123 403 192 455 548\n",
      " 572 312 477 318 837 876 437 711 449 363 329 264 801 792 622 162  33 577\n",
      " 351 997 989 661 467 464  10 915 486 151 447  96 873 144 557 843]\n",
      "there are 43903 unique words in this corpus\n",
      "positive reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:43: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.833333333333\n",
      "negative reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:62: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82\n",
      "overall accuracy: 82.7\n",
      "pos: taking 300 samples of 1 reviews\n",
      "mean score is 0.1474464455226564\n",
      "pos: taking 2000 samples of 2 reviews\n",
      "mean score is 0.11286729935097702\n",
      "pos: taking 1500 samples of 3 reviews\n",
      "mean score is 0.09819679173908433\n",
      "pos: taking 1500 samples of 5 reviews\n",
      "mean score is 0.08367538454249544\n",
      "pos: taking 1000 samples of 7 reviews\n",
      "mean score is 0.07602263048412164\n",
      "pos: taking 900 samples of 10 reviews\n",
      "mean score is 0.06944925660992114\n",
      "pos: taking 750 samples of 15 reviews\n",
      "mean score is 0.061885085333698575\n",
      "pos: taking 600 samples of 25 reviews\n",
      "mean score is 0.05224466570555474\n",
      "pos: taking 500 samples of 40 reviews\n",
      "mean score is 0.043211585607523174\n",
      "pos: taking 250 samples of 60 reviews\n",
      "mean score is 0.035977902787568024\n",
      "pos: taking 100 samples of 80 reviews\n",
      "mean score is 0.030362444029556287\n",
      "pos: taking 75 samples of 100 reviews\n",
      "mean score is 0.026553705976707326\n",
      "pos: taking 75 samples of 150 reviews\n",
      "mean score is 0.020717014203355272\n",
      "pos: taking 40 samples of 250 reviews\n",
      "mean score is 0.014539362378088366\n",
      "pos: taking 25 samples of 400 reviews\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot take a larger sample than population when 'replace=False'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-471fe89c40d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnaive_bayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-f626ab0b3650>\u001b[0m in \u001b[0;36mnaive_bayes\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mchoose_randomly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                 \u001b[0mmy_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumReviews\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0mmy_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice (numpy/random/mtrand/mtrand.c:17481)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot take a larger sample than population when 'replace=False'"
     ]
    }
   ],
   "source": [
    "naive_bayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "[958 130 238 765 982 992 728 359 173 192 976 410 233 865 847 576 923 163\n",
      " 385  91 445 186 487 864 936 329 285 915 592 772 706 144 135 124 867 254\n",
      " 986 539 449 589 700 175 350 566 932 217 160 139 973 882 713  10 502 605\n",
      " 523 546 632 107 549 719 726 714 309 604 802  39 427 783 306  84 227 425\n",
      " 679 612 521 509  75 969 836 677 729 717 271 250 944 835 390 694 402 759\n",
      " 366 716 642 902 280  38 952 655 196 909]\n",
      "[285 874 249 518 365 321 659 988 457 848 710 701 668  83 780 211 696 888\n",
      " 191 331 387 136  37 200 690 794  19 382 986 939 327 975 815 648 784 797\n",
      " 454 915 231 930 397 970  72 595 218 851 551 704 626 538 452 695 349 116\n",
      " 730 528 643 287 998 973 126 284 905 127 164  26 732 286 786 442 236 854\n",
      " 158 361 159   5 836 844 346 541 258 421 558  36 402 411 783 304 610 810\n",
      " 981 679 711 737 432 302 373 357 311  84]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:15: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:25: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 43903 unique words in this corpus\n",
      "positive reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:48: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.618888888889\n",
      "negative reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:67: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.878888888889\n",
      "overall accuracy: 74.9\n",
      "1000\n",
      "1000\n",
      "[871 483 145 837 329 993  73 287 301 612 153 367 989 234 591 107 495  24\n",
      " 135  95 359 583 700 723 823 256 768 546 572 647 632 750 406  41 731 674\n",
      " 967 694 164 443 340 889 911  21 286 707 338 692 173 236 362   7 474   6\n",
      " 549 524 458  10 846 985 268 598 669 469  93 192 706 182 202 635 705 339\n",
      " 684 315 174 282 800   0 746 171 938 424 881   1 895 942 880 180  89 650\n",
      "  23  12 481 854 207 318 275 160 660 636]\n",
      "[546  10 808 860 134 617 887 751 981  16 468 938 102 561 256 221   2 229\n",
      "  50 595 839 519 538 110 721 402 647 105 624 433 626 724 764 108 146 719\n",
      " 283 697 112 381 961 898 552 143  25 696 532  68 535 490 450 213 214 896\n",
      " 805 194 398 239 325 135 908 627  51 384 200 412 792 357 849 988 720 527\n",
      " 339 722 512  36  31 834 435 837 843  81 787 474 550 990 266 886 681 953\n",
      " 730 671 427 121 996  22 931 201 673 560]\n",
      "there are 43903 unique words in this corpus\n",
      "positive reviews\n",
      "0.64\n",
      "negative reviews\n",
      "0.878888888889\n",
      "overall accuracy: 75.9\n",
      "1000\n",
      "1000\n",
      "[739 330 229 978 840 779 871 580 799 967 244 133 958 953 994 919 404 214\n",
      " 241 667 651 119  88  58 420 572 768 964 350 278 267 307 296 726 621 188\n",
      " 369 279 582 115 269 302  60 423 816 911  13  39  26 992 573 653 209 928\n",
      " 590 760 560 533 559 383 558 656 843 103 974 552 393 740 200 731 712 158\n",
      " 265 553 183 990 876 429 623  57 457 513 333 463 684 340 236 356  69 234\n",
      " 254 828 184 118  86  23 167 781 586 630]\n",
      "[ 67 684 155   5 890 868 284 933  94 186 922 302 183 824 196 365 965 766\n",
      "  60 546 800  91 334  64 792 236 576 115 488 158 973 655 172 567 178 337\n",
      " 981 345 389   6  93 884 491 775 392 240 656 708 291 430  10  99  34 716\n",
      " 604 857 650 558 869 435 367 588 744 352 998 404 132  41 675 753 615 443\n",
      "  83   8 598 413 429 161 390 689  42 681 195 930 224 283 520 916 902 227\n",
      " 634 364 298 244 851 248 530 368 296  84]\n",
      "there are 43903 unique words in this corpus\n",
      "positive reviews\n",
      "0.65\n",
      "negative reviews\n",
      "0.863333333333\n",
      "overall accuracy: 75.7\n",
      "1000\n",
      "1000\n",
      "[839 468 667 620 994 557 937 345 927 657 908 764  62 876  64 223 847 702\n",
      " 866 992 932   3 638 164 313 717 427  34 775 399 487 473 800 315 537 201\n",
      " 347 914 929 361 813 239 679 550 165  45 244 118 203 770 772 664 579 245\n",
      "  11 572 983 166 777   2  91 721 237 206 673 336 449 451 663 566 680 886\n",
      " 597 688 267 846 706 563 463 896 228  79 102 595 369 569 831  10 969  36\n",
      " 227 472 103 975 974 756 734   8   6 949]\n",
      "[607 119 265 289 821 325 592 476 177 879 322 849 343 302  38 372 470 396\n",
      " 780 461  65  79 889 544 614 305 398 488 976 542   0 657 570 192  31 705\n",
      " 648 304 260 143  67 594  26 197 788 860 591 131 703 108 164 870 234 532\n",
      "  91 163 826 921 787 111 374 266 801  56 646 507  11 758 887 174 545 827\n",
      " 237 996 415 114 270 691 774 528 165 565 916 729  47 859 873 357 667 123\n",
      " 161  97 991   8 559 220 561 978 782 598]\n",
      "there are 43903 unique words in this corpus\n",
      "positive reviews\n",
      "0.668888888889\n",
      "negative reviews\n",
      "0.851111111111\n",
      "overall accuracy: 76.0\n",
      "1000\n",
      "1000\n",
      "[365 100 322 225 367 881 754  31 784 370 137 123 658 333 837 409 898 133\n",
      " 469 849 280 402  76 220 153   6 936 912 169 531 802 530 823 835  10 832\n",
      " 697 993 855 576 276 602 439 414  45 375 299  73 424 762 923 435 992 345\n",
      "  59 242 788 665   7 596 790 391 412 125  71 321  49 407 604 378 400 544\n",
      " 914 741  97 213 446 262 875  46 491 978 916 826 880 363 798 110 668 516\n",
      " 105 673 948 445 431 195 161 620 539 594]\n",
      "[179 624 691 787 631 193 902 790 305 234 706 986 773 512 782 840 866  76\n",
      " 152 330 750 702 894  38 269 741 352 723  11 264 765 299 142 109 397 520\n",
      "  57 435 726 915 491 261 707 546 129 586 526  49 224 965 701 543 571 402\n",
      " 987 282  36 724 969 653 369 158  79   7 284  30  24 673 386 135 497 669\n",
      " 443 373 769 831 849 410 134 884 482 263 457  98 785 714 770   4  56 464\n",
      "   6 633 975  91 496 141 734 110 641 136]\n",
      "there are 43903 unique words in this corpus\n",
      "positive reviews\n",
      "0.61\n",
      "negative reviews\n",
      "0.862222222222\n",
      "overall accuracy: 73.6\n",
      "1000\n",
      "1000\n",
      "[829 764 633  58 845 498 652 467 181  83  27 624 329 392 135 515 221 646\n",
      " 490 718 990 285 528 334 505   3 641 388  45 282   0 318 917 159  51 615\n",
      "  42 155 870 943 545 276 850 586 439 479 611  90 229 937 499 254 554 170\n",
      " 974 281 233 600 279 428  52 373 739 519 127 154 684 188 731 734 609 817\n",
      " 552 341 605 464 557 922 675 387 698  19 995 151  30 218 512 124 801 103\n",
      " 905  21 563  96 500 184 549 856 380 149]\n",
      "[672 140 939  19 184 793 532 395  34 952 779 489 450 870 524 555  86 258\n",
      "  22 249 515  12   1  95 204 988 247 810  98 377 446 732  41 149   8 348\n",
      " 812 330 507 344 598  80  67  28 257 915 747  25 552 422 467 980 794 927\n",
      "  48 246 711 523 449 474 728 127 930 298 739 259 241 960 443 462 706 234\n",
      " 912 583 432 910 290 685  15 961 316 242 864 126 167 746 827 987 283 263\n",
      " 445  10 676 331 196  72 627 522 673 905]\n",
      "there are 43903 unique words in this corpus\n",
      "positive reviews\n",
      "0.603333333333\n",
      "negative reviews\n",
      "0.877777777778\n",
      "overall accuracy: 74.1\n",
      "1000\n",
      "1000\n",
      "[ 72 412 316 220 513  48 436 706 120 815  56 441 653 856  55 861 574 248\n",
      "  69 782 740 966 864 963 613  99  58 828 577 816 349 517 405 359 965 259\n",
      "  73 136 137 188 746 526 328 460 778 699 584 421 917 234 829 445 766 855\n",
      " 600 830 777 617 595 998 573 323 207  64 892 201 982 175  39 914 589 538\n",
      " 578 289 757 812 779 337 346 449 140 618 149 226 126 111 473 497 430 876\n",
      "  12 649 742 956 269 557 218 661 382  80]\n",
      "[418 688 193 915 771 907 373 323  86 861 757 152 852 122 712  61 279 941\n",
      " 974 426 548 126 685 653 922 403 517  30 516 943 529 251 217 197 158 744\n",
      " 356 563 954 346 824 881 592 690 564 602  96 187 894 919  97 236 462 761\n",
      " 926 581 567 457 970 238 956 713 300 453 428 121  15 743 715 169 878 278\n",
      " 755 519 357 603 627 365  78 742 261 277 569 896 171 129 138   6 528 416\n",
      "   3 216 314 209 155 576 456  82 335  40]\n",
      "there are 43903 unique words in this corpus\n",
      "positive reviews\n",
      "0.657777777778\n",
      "negative reviews\n",
      "0.824444444444\n",
      "overall accuracy: 74.1\n",
      "1000\n",
      "1000\n",
      "[595  83 554 686  29 788 727 126 531 415 767 492 281 176 707 712 236 412\n",
      "  38 458 473 478 396 387 417 789 206 820 451 269 123  65 618 394 839 143\n",
      " 530 582 261 249 992 650 429 193 938 158 652 552 112 282 324 826 278 489\n",
      " 544  25 581 754 796 268 619 373 884 881 248 838 918 781 254 145 461 469\n",
      " 119 465 401 490 951 188  92 998  40 854   1 189 984 791 588 680 861 343\n",
      " 704 360 610 970 309 661 634 993  30 613]\n",
      "[334  75 243 183 927 674 309 973 826 268 830 637 329 386 796 619 622 171\n",
      " 601 536 287 556 167 977 401 380 416   9 175 498  35 384 666 360 640 926\n",
      " 108 606 336 422 610 161 840 285 340 274 216 305 772 528 223 778 110 440\n",
      " 869 576 936 511 659 813 981 290 695  63 941 751 308 166  20 487 523 237\n",
      " 236 624 562 214 203  16 103 843 104 432 391 224 451  48 959  49  52 134\n",
      " 776 436 333 878 877 855 909 867 473 646]\n",
      "there are 43903 unique words in this corpus\n",
      "positive reviews\n",
      "0.661111111111\n",
      "negative reviews\n",
      "0.848888888889\n",
      "overall accuracy: 75.5\n",
      "1000\n",
      "1000\n",
      "[776 146  19 344 176 721 300 667 402 275 745 797  34 726 948 984 957 111\n",
      " 479 108 435  50 528 925 138 565  78  42 935 647 451 414 723 608 421 606\n",
      "  90 202 503 949 892  14 244 376 881 519 677 910 674  48  31 265 373 126\n",
      " 168 547 268 824 415 842 729 179 460 614 387 572 450 333  10 494 742 445\n",
      " 798 442 929 983 585 198  23 374 237 998 961 993 800 634 968 212 515 570\n",
      " 232 209 137 200 241 360 305 637 339 188]\n",
      "[218 829 148  38 824 405 440 169 613   7 395 553 651 433 756 847 606 880\n",
      " 944  41  50 637   6 900 859 708 715 896 738 337  46 776 392  40 277 516\n",
      " 254 163 587  88 505 639 751 524 550 480 226 659 348  67 559 699 704 744\n",
      " 816 899 149 917 554 106 248 771 910 336 841 805 789 384 310 939 670 502\n",
      " 579 299 287 180 441 477 849 398 750 786 664 214  43 446 235 364 221 581\n",
      " 453 693 700 331 465  25 893 206 599 741]\n",
      "there are 43903 unique words in this corpus\n",
      "positive reviews\n",
      "0.667777777778\n",
      "negative reviews\n",
      "0.833333333333\n",
      "overall accuracy: 75.1\n",
      "1000\n",
      "1000\n",
      "[967 559 725 650 846 433 636 241 996 286 968 478 989 666 255 200 487 807\n",
      " 798 824 575 423 729  54  56 589 446  32 541 817  42 279 239 105 802 756\n",
      " 268 644 272 330  49 516 641 229 273 884 249 524 317 384 327 898 502 750\n",
      " 370 235 600 605  79 296 668 780 357 882 580   9 321 217 333 405 560  22\n",
      " 377 703 717 257 564 548 786 467 144 339 871  78 488 941 209 342  61 471\n",
      "   8 612 213 153 248  59 100 237 845 716]\n",
      "[800  27 455 291 964  79 581 435 181 403 395  43 818 744 448 804 628 338\n",
      " 924 759 348 582 867  80   5 535 220 928 292 858 618 331 697 841 570 150\n",
      " 878 269 814 913  89 529  41  70 696 526 607 749 666 663 110  10 998 572\n",
      " 972 386 208 378 335 847 155 903 559 143 912 615 182 522 279 501 257  23\n",
      " 151  98 648 462 465 432 862 206 742 791 889 764 908 909 531 902 534 425\n",
      " 946 851 553  58 419 576 290  45 146 540]\n",
      "there are 43903 unique words in this corpus\n",
      "positive reviews\n",
      "0.6\n",
      "negative reviews\n",
      "0.873333333333\n",
      "overall accuracy: 73.7\n",
      "74.85\n"
     ]
    }
   ],
   "source": [
    "bayes_many_samples(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:13: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:22: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 49910 unique words in this corpus\n",
      "loading NYT\n",
      "scoring\n",
      "arts score=0.0011159275640186261\n",
      "scoring\n",
      "books score=0.0012200187812716479\n",
      "scoring\n",
      "classified score=0.0015993922267443228\n",
      "scoring\n",
      "cultural score=0.001261138308587706\n",
      "scoring\n",
      "editorial score=0.0012472466907665902\n",
      "scoring\n",
      "education score=0.011667700557933758\n",
      "scoring\n",
      "financial score=0.0011334191940998717\n",
      "scoring\n",
      "foreign score=0.0011420095448818057\n",
      "scoring\n",
      "home score=0.0017423632375913045\n",
      "scoring\n",
      "leisure score=0.0015426128702282216\n",
      "scoring\n",
      "living score=0.001899736991112455\n",
      "scoring\n",
      "magazine score=0.0013857617664865818\n",
      "scoring\n",
      "metropolitan score=0.0010941508039481462\n",
      "scoring\n",
      "movies score=0.001398978836208764\n",
      "scoring\n",
      "national score=0.0011498957790482822\n",
      "scoring\n",
      "regional score=0.001148435935058978\n",
      "scoring\n",
      "science score=0.0016449263342612763\n",
      "scoring\n",
      "society score=0.0016969451543734082\n",
      "scoring\n",
      "sports score=0.0011298850088227175\n",
      "scoring\n",
      "style score=0.0014925479144223819\n",
      "scoring\n",
      "television score=0.07792182991749835\n",
      "scoring\n",
      "travel score=0.038806614494318714\n",
      "scoring\n",
      "week-in-review score=0.0015283724947402488\n",
      "scoring\n",
      "weekend score=0.0014304607439555306\n",
      "['television', 'travel', 'education', 'living', 'home', 'society', 'science', 'classified', 'leisure', 'week-in-review', 'style', 'weekend', 'movies', 'magazine', 'cultural', 'editorial', 'books', 'national', 'regional', 'foreign', 'financial', 'sports', 'arts', 'metropolitan']\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnwAAAIwCAYAAAAcd7ZBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnBI+EREREQ/3/wG2skITfxGAsgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x107b833c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NB_on_NYT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:9: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:18: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 49910 unique words in this corpus\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "top positive words:\n",
      "trek: 22.23155849829559\n",
      "rocky: 22.231558498295588\n",
      "mallory: 19.452613686008643\n",
      "shrek: 17.599983811150675\n",
      "contact: 16.673668873721688\n",
      "marie: 16.673668873721688\n",
      "altman: 14.821038998863726\n",
      "julianne: 14.821038998863724\n",
      "animated: 11.115779249147796\n",
      "lebowski: 11.115779249147796\n",
      "--------------------------------------------------------------------------------\n",
      "top negative words:\n",
      "mars: 18.35228960809382\n",
      "horrible: 17.27274316055889\n",
      "gorilla: 16.193196713023966\n",
      "cindy: 16.193196713023966\n",
      "batman: 13.674255002109119\n",
      "fault: 12.95455737041917\n",
      "bats: 12.954557370419169\n",
      "eastwood: 12.954557370419169\n",
      "godzilla: 12.414784146651701\n",
      "martha: 11.87501092288424\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "top positive words:\n",
      "mallory: 0.0023184565606258548\n",
      "marie: 0.001476974524356019\n",
      "altman: 0.0014371265400795635\n",
      "julianne: 0.0013892223220769112\n",
      "dinosaurs: 0.001173216901082185\n",
      "apostle: 0.0010833532309152843\n",
      "friend's: 0.001044187783182919\n",
      "fiona: 0.0010098516458492538\n",
      "donkey: 0.001005630306932223\n",
      "mickey: 0.000949805022893053\n",
      "--------------------------------------------------------------------------------\n",
      "top negative words:\n",
      "gorilla: 0.0013689842437767507\n",
      "cindy: 0.0013689842437767507\n",
      "squad: 0.0012020041196654269\n",
      "eastwood: 0.0010435049291860903\n",
      "suzie: 0.0010144084962846044\n",
      "poison: 0.0009738321564332202\n",
      "freeze: 0.0009562156304816331\n",
      "bats: 0.0009540616495415682\n",
      "martha: 0.0008934325429288263\n",
      "varsity: 0.0007963000592209096\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "top positive words:\n",
      "trek: 20.65798228822203\n",
      "rocky: 20.65798228822203\n",
      "contact: 15.493486716166519\n",
      "younger: 8.607492620092511\n",
      "goodman: 8.607492620092511\n",
      "masterpiece: 8.60749262009251\n",
      "subtle: 7.74674335808326\n",
      "mature: 7.74674335808326\n",
      "cuba: 7.74674335808326\n",
      "religion: 7.746743358083259\n",
      "--------------------------------------------------------------------------------\n",
      "top negative words:\n",
      "mars: 19.75023476676218\n",
      "horrible: 18.58845625107029\n",
      "batman: 14.715861198763976\n",
      "schumacher: 11.617785156918933\n",
      "product: 11.61778515691893\n",
      "poorly: 11.61778515691893\n",
      "arnold: 10.456006641227036\n",
      "blade: 9.87511738338109\n",
      "decent: 9.294228125535145\n",
      "coach: 9.294228125535145\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "top positive words:\n",
      "performed: 0.01123388374186558\n",
      "mrs: 0.010485540655490002\n",
      "degree: 0.009588007754898523\n",
      "president: 0.007565684593222708\n",
      "father: 0.00723685511905485\n",
      "j: 0.0058295708952366965\n",
      "college: 0.0046096088996417095\n",
      "church: 0.004150110245260737\n",
      "st: 0.004137585853175096\n",
      "mother: 0.003540597850654588\n",
      "--------------------------------------------------------------------------------\n",
      "top negative words:\n",
      "york: 0.018547998873680714\n",
      "n: 0.017382963511169386\n",
      "university: 0.015043267117001272\n",
      "received: 0.00890507909363634\n",
      "mr: 0.005026620895149894\n",
      "manhattan: 0.004572419673205527\n",
      "new: 0.004540998315428337\n",
      "dr: 0.004331090216340459\n",
      "medical: 0.00402728199709851\n",
      "evening: 0.0033536739166692927\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "society score=0.002358306454522563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:117: RuntimeWarning: invalid value encountered in true_divide\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:118: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "mr,tr = NB_wordshift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[22.231558498295591, 'trek'],\n",
       "  [22.231558498295588, 'rocky'],\n",
       "  [19.452613686008643, 'mallory'],\n",
       "  [17.599983811150675, 'shrek'],\n",
       "  [16.673668873721688, 'contact'],\n",
       "  [16.673668873721688, 'marie'],\n",
       "  [14.821038998863726, 'altman'],\n",
       "  [14.821038998863724, 'julianne'],\n",
       "  [11.115779249147796, 'animated'],\n",
       "  [11.115779249147796, 'lebowski']],\n",
       " [[18.35228960809382, 'mars'],\n",
       "  [17.272743160558889, 'horrible'],\n",
       "  [16.193196713023966, 'gorilla'],\n",
       "  [16.193196713023966, 'cindy'],\n",
       "  [13.674255002109119, 'batman'],\n",
       "  [12.95455737041917, 'fault'],\n",
       "  [12.954557370419169, 'bats'],\n",
       "  [12.954557370419169, 'eastwood'],\n",
       "  [12.414784146651701, 'godzilla'],\n",
       "  [11.87501092288424, 'martha']],\n",
       " [[0.0023184565606258548, 'mallory'],\n",
       "  [0.0014769745243560189, 'marie'],\n",
       "  [0.0014371265400795635, 'altman'],\n",
       "  [0.0013892223220769112, 'julianne'],\n",
       "  [0.0011732169010821849, 'dinosaurs'],\n",
       "  [0.0010833532309152843, 'apostle'],\n",
       "  [0.0010441877831829189, \"friend's\"],\n",
       "  [0.0010098516458492538, 'fiona'],\n",
       "  [0.0010056303069322231, 'donkey'],\n",
       "  [0.00094980502289305298, 'mickey']],\n",
       " [[0.0013689842437767507, 'gorilla'],\n",
       "  [0.0013689842437767507, 'cindy'],\n",
       "  [0.0012020041196654269, 'squad'],\n",
       "  [0.0010435049291860903, 'eastwood'],\n",
       "  [0.0010144084962846044, 'suzie'],\n",
       "  [0.00097383215643322021, 'poison'],\n",
       "  [0.00095621563048163311, 'freeze'],\n",
       "  [0.00095406164954156818, 'bats'],\n",
       "  [0.00089343254292882635, 'martha'],\n",
       "  [0.00079630005922090965, 'varsity']]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_table(my_list,my_fname):\n",
    "    f = open(my_fname,\"w\")\n",
    "    for j in range(10):\n",
    "        table_strings = [my_list[i][j] for i in range(4)]\n",
    "        table_strings = [\"{0:.2f} & {1}\".format(my_list[i][j][0],my_list[i][j][1]) for i in range(2)]+[\"{0:.4f} & {1}\".format(my_list[i][j][0],my_list[i][j][1]) for i in range(2,4)]\n",
    "        full_string = \" & \".join(table_strings)\n",
    "        f.write(full_string)\n",
    "        f.write(r\" \\\\\")\n",
    "        f.write(\"\\n\")\n",
    "    f.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_table(mr,\"../figures/NB/output-003-table-MR.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_table(tr,\"../figures/NB/output-003-table-TR.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:9: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:18: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 49910 unique words in this corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:125: RuntimeWarning: invalid value encountered in true_divide\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:126: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "mr,tr = NB_wordshift(q=True)\n",
    "to_table(mr,\"../figures/NB/output-004-table-MR.tex\")\n",
    "to_table(tr,\"../figures/NB/output-004-table-TR.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:9: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:18: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 49910 unique words in this corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:125: RuntimeWarning: invalid value encountered in true_divide\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:126: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "mr,tr = NB_wordshift(q=True)\n",
    "to_table(mr,\"../figures/NB/output-005-table-MR.tex\")\n",
    "to_table(tr,\"../figures/NB/output-005-table-TR.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:9: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:18: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 49910 unique words in this corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:125: RuntimeWarning: invalid value encountered in true_divide\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:126: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "mr,tr = NB_wordshift(q=True)\n",
    "to_table(mr,\"../figures/NB/output-006-table-MR.tex\")\n",
    "to_table(tr,\"../figures/NB/output-006-table-TR.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
