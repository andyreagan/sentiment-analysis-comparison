{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement and test Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/andyreagan/tools/python/labMTsimple/\")\n",
    "from labMTsimple.speedy import *\n",
    "from labMTsimple.storyLab import *\n",
    "\n",
    "import re\n",
    "import codecs\n",
    "from os import listdir,mkdir\n",
    "from os.path import isfile,isdir\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import rc,rcParams\n",
    "rc(\"xtick\", labelsize=8)\n",
    "rc(\"ytick\", labelsize=8)\n",
    "rc(\"font\",**{\"family\":\"serif\",\"serif\":[\"cmr10\"]})\n",
    "# rc(\"text\", usetex=True)\n",
    "figwidth_onecol = 8.5\n",
    "figwidth_twocol = figwidth_onecol/2\n",
    "\n",
    "import numpy as np\n",
    "from json import loads\n",
    "import csv\n",
    "from datetime import datetime,timedelta\n",
    "import pickle\n",
    "\n",
    "error_logging = True\n",
    "sys.path.append(\"/Users/andyreagan/tools/python/kitchentable\")\n",
    "from dogtoys import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadMovieReviews():\n",
    "    posfiles = [\"../data/moviereviews/txt_sentoken/pos/\"+x for x in listdir(\"../data/moviereviews/txt_sentoken/pos\") if \".txt\" in x]\n",
    "    negfiles = [\"../data/moviereviews/txt_sentoken/neg/\"+x for x in listdir(\"../data/moviereviews/txt_sentoken/neg\") if \".txt\" in x]\n",
    "    poswordcounts = dict()\n",
    "    allwordcounts = dict()\n",
    "    for file in posfiles:\n",
    "        f = open(file,\"r\")\n",
    "        postext = f.read() + \" \"\n",
    "        f.close()\n",
    "        dictify_general(postext,poswordcounts)\n",
    "        dictify_general(postext,allwordcounts)\n",
    "    negwordcounts = dict()\n",
    "    for file in negfiles:\n",
    "        f = open(file,\"r\")\n",
    "        negtext = f.read() + \" \"\n",
    "        f.close()\n",
    "        dictify_general(negtext,negwordcounts)\n",
    "        dictify_general(negtext,allwordcounts)\n",
    "\n",
    "    print(\"there are {0} unique words in this corpus\".format(len(allwordcounts)))\n",
    "\n",
    "    # rip those dictionaries into lists for sorting\n",
    "    allwordsList = [word for word in allwordcounts]\n",
    "    allcountsList = [allwordcounts[allwordsList[i]] for i in range(len(allwordsList))]\n",
    "\n",
    "    # sort them\n",
    "    indexer = sorted(range(len(allcountsList)), key=lambda k: allcountsList[k], reverse=True)\n",
    "    allcountsListSorted = np.array([float(allcountsList[i]) for i in indexer])\n",
    "    allwordsListSorted = [allwordsList[i] for i in indexer]\n",
    "\n",
    "    return allcountsListSorted,allwordsListSorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bayes_score(all_counts,pos_params,neg_params,my_counts):\n",
    "    # uses all_counts_1k\n",
    "    # uses pos_params\n",
    "    # uses neg_params\n",
    "\n",
    "    # compute conditional\n",
    "    prob_pos = my_counts*pos_params/(all_counts/sum(all_counts))\n",
    "    prob_neg = my_counts*neg_params/(all_counts/sum(all_counts))\n",
    "    # add these things up carefulls\n",
    "    log_prob_pos = 0.0\n",
    "    for p in prob_pos:\n",
    "        if p > 0:\n",
    "            log_prob_pos += np.log(p)\n",
    "    log_prob_neg = 0.0\n",
    "    for p in prob_neg:\n",
    "        if p > 0:\n",
    "            log_prob_neg += np.log(p)\n",
    "\n",
    "    # normalize post-hoc\n",
    "    p_d = log_prob_pos+log_prob_neg\n",
    "\n",
    "    # return the difference\n",
    "    # if > 0, positive review\n",
    "    return log_prob_pos/p_d - log_prob_neg/p_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_bayes(all_words_1k,poswordcounts,negwordcounts):\n",
    "    \"\"\"Train bayes classifier using sorted word list, and dictionaries of counts from the two classes. Return the parameters\"\"\"\n",
    "    pos_counts = np.array([float(poswordcounts[word]) if word in poswordcounts else 0.0 for word in all_words_1k])\n",
    "    neg_counts = np.array([float(negwordcounts[word]) if word in negwordcounts else 0.0 for word in all_words_1k])\n",
    "    # print(pos_counts[:10])\n",
    "\n",
    "    # smoothing parameter\n",
    "    alpha = 1.0\n",
    "    pos_params = (pos_counts+alpha)/(np.sum(pos_counts)+alpha*len(pos_counts))\n",
    "    # print(pos_params[:10])\n",
    "\n",
    "    neg_params = (neg_counts+alpha)/(np.sum(neg_counts)+alpha*len(neg_counts))\n",
    "    # print(neg_params[:10])\n",
    "\n",
    "    return pos_params,neg_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naive_bayes():\n",
    "    posfiles = [\"../data/moviereviews/txt_sentoken/pos/\"+x for x in listdir(\"../data/moviereviews/txt_sentoken/pos\") if \".txt\" in x]\n",
    "    negfiles = [\"../data/moviereviews/txt_sentoken/neg/\"+x for x in listdir(\"../data/moviereviews/txt_sentoken/neg\") if \".txt\" in x]\n",
    "\n",
    "    print(len(posfiles))\n",
    "    print(len(negfiles))\n",
    "    # randomly select N reviews for training\n",
    "    training_size = 200\n",
    "\n",
    "    pos_files_training = np.random.choice(np.arange(len(posfiles)),size=training_size/2,replace=False)\n",
    "    print(pos_files_training)\n",
    "    # go add up all the words for training\n",
    "    poswordcounts = dict()\n",
    "    for file in [posfiles[i] for i in pos_files_training]:\n",
    "        f = open(file,\"r\")\n",
    "        postext = f.read()\n",
    "        f.close()\n",
    "        dictify_general(postext,poswordcounts)\n",
    "    # select the negative reviews for training\n",
    "    neg_files_training = np.random.choice(np.arange(len(negfiles)),size=training_size/2,replace=False)\n",
    "    print(neg_files_training)\n",
    "    # go add up the words\n",
    "    negwordcounts = dict()\n",
    "    for file in [negfiles[i] for i in neg_files_training]:\n",
    "        f = open(file,\"r\")\n",
    "        negtext = f.read()\n",
    "        f.close()\n",
    "        dictify_general(negtext,negwordcounts)\n",
    "\n",
    "    all_counts,all_words = loadMovieReviews()\n",
    "    # take the top 1000 words\n",
    "    all_counts_1k = all_counts[30:5000]\n",
    "    all_words_1k = all_words[30:5000]\n",
    "    # build this for speed later\n",
    "    all_words_1k_dict = dict()\n",
    "    for i,word in enumerate(all_words_1k):\n",
    "        all_words_1k_dict[word] = i\n",
    "\n",
    "    pos_params,neg_params = train_bayes(all_words_1k,poswordcounts,negwordcounts)\n",
    "\n",
    "    # classify all positive reviews\n",
    "    print(\"positive reviews\")\n",
    "    correct_pos = np.zeros(len(posfiles)-training_size/2)\n",
    "    for i,file in enumerate([posfiles[i] for i in range(len(posfiles)) if i not in pos_files_training]):\n",
    "        f = open(file,\"r\")\n",
    "        postext = f.read()\n",
    "        f.close()\n",
    "        my_poswordcounts = dict()\n",
    "        dictify_general(postext,my_poswordcounts)\n",
    "        my_pos_counts = np.array([float(my_poswordcounts[word]) if word in my_poswordcounts else 0.0 for word in all_words_1k])\n",
    "\n",
    "        score = bayes_score(all_counts_1k,pos_params,neg_params,my_pos_counts)\n",
    "\n",
    "        if score > 0:\n",
    "            correct_pos[i] = 1\n",
    "\n",
    "    pos_accuracy = sum(correct_pos)/len(correct_pos)\n",
    "    print(sum(correct_pos)/len(correct_pos))\n",
    "\n",
    "    # classify all negative reviews\n",
    "    print(\"negative reviews\")\n",
    "    correct_neg = np.zeros(len(negfiles)-training_size/2)\n",
    "    for i,file in enumerate([negfiles[i] for i in range(len(negfiles)) if i not in neg_files_training]):\n",
    "        f = open(file,\"r\")\n",
    "        negtext = f.read()\n",
    "        f.close()\n",
    "        my_wordcounts = dict()\n",
    "        dictify_general(negtext,my_wordcounts)\n",
    "        my_counts = np.array([float(my_wordcounts[word]) if word in my_wordcounts else 0.0 for word in all_words_1k])\n",
    "\n",
    "        score = bayes_score(all_counts_1k,pos_params,neg_params,my_counts)\n",
    "\n",
    "        if score < 0:\n",
    "            # print(\"correct, score={0}\".format(score))\n",
    "            correct_neg[i] = 1\n",
    "\n",
    "    neg_accuracy = sum(correct_neg)/len(correct_neg)\n",
    "    print(sum(correct_neg)/len(correct_neg))\n",
    "\n",
    "    print(\"overall accuracy: {0:.1f}\".format((pos_accuracy+neg_accuracy)/2*100))\n",
    "    \n",
    "\n",
    "    allLengths = [                   1,   2,   3,   5,   7, 10, 15, 25, 40, 60, 80,100,150,250,400,600,900,]\n",
    "    allSamples = [int(np.floor(1000-training_size/2)),2000,1500,1500,1000,900,750,600,500,250,100, 75, 75, 40, 25, 15,  1,]\n",
    "    # allLengths = []\n",
    "\n",
    "    files = [posfiles[i] for i in range(len(posfiles)) if i not in pos_files_training]\n",
    "\n",
    "    # store all of the scores here\n",
    "    pos_results_all = [[] for i in range(len(allLengths))]\n",
    "    # store the mean of those here\n",
    "    pos_means = [0.0 for i in range(len(allLengths))]\n",
    "    # store the std of those here\n",
    "    pos_std = [0.0 for i in range(len(allLengths))]\n",
    "\n",
    "    for k,numReviews in enumerate(allLengths):\n",
    "        numSamples = allSamples[k]\n",
    "        print(\"pos: taking {0} samples of {1} reviews\".format(numSamples,numReviews))\n",
    "\n",
    "        if numReviews == 1:\n",
    "            choose_randomly = False\n",
    "        else:\n",
    "            choose_randomly = True\n",
    "\n",
    "        scores = [0.0 for i in range(numSamples)]\n",
    "        for i in range(numSamples):\n",
    "            # print(\"on sample {0}\".format(i))\n",
    "\n",
    "            if choose_randomly:\n",
    "                my_files = np.random.choice(files,size=numReviews,replace=False)\n",
    "            else:\n",
    "                my_files = [files[i]]\n",
    "\n",
    "            # forget the string expansion\n",
    "            # let\"s store them as a dict\n",
    "            # allwordcounts = dict()\n",
    "            # nah, let's store them as in the vector\n",
    "            allwordcounts = np.zeros(len(all_counts_1k))\n",
    "            for file in my_files:\n",
    "                ########################################\n",
    "                # this loads the files\n",
    "                f = open(file,\"r\")\n",
    "                rawtext = f.read()\n",
    "                f.close()\n",
    "                # add to the full dict\n",
    "                # dictify_general(rawtext,allwordcounts)\n",
    "                # add it to the vector\n",
    "                my_list = listify(rawtext)\n",
    "                for word in my_list:\n",
    "                    if word in all_words_1k_dict:\n",
    "                        allwordcounts[all_words_1k_dict[word]] += 1\n",
    "\n",
    "            scores[i] = bayes_score(all_counts_1k,pos_params,neg_params,allwordcounts)\n",
    "\n",
    "        # now save those scores\n",
    "        pos_results_all[k] = scores\n",
    "        pos_means[k] = np.mean(scores)\n",
    "        print(\"mean score is {0}\".format(pos_means[k]))\n",
    "        pos_std[k] = np.std(scores)\n",
    "\n",
    "    files = [negfiles[i] for i in range(len(negfiles)) if i not in neg_files_training]\n",
    "\n",
    "    # store all of the scores here\n",
    "    neg_results_all = [[] for i in range(len(allLengths))]\n",
    "    # store the mean of those here\n",
    "    neg_means = [0.0 for i in range(len(allLengths))]\n",
    "    # store the std of those here\n",
    "    neg_std = [0.0 for i in range(len(allLengths))]\n",
    "\n",
    "    for k,numReviews in enumerate(allLengths):\n",
    "        numSamples = allSamples[k]\n",
    "        print(\"neg: taking {0} samples of {1} reviews\".format(numSamples,numReviews))\n",
    "\n",
    "        if numReviews == 1:\n",
    "            choose_randomly = False\n",
    "        else:\n",
    "            choose_randomly = True\n",
    "\n",
    "        scores = [0.0 for i in range(numSamples)]\n",
    "        for i in range(numSamples):\n",
    "            # print(\"on sample {0}\".format(i))\n",
    "\n",
    "            if choose_randomly:\n",
    "                my_files = np.random.choice(files,size=numReviews,replace=False)\n",
    "            else:\n",
    "                my_files = [files[i]]\n",
    "\n",
    "\n",
    "            # forget the string expansion\n",
    "            # let\"s store them as a dict\n",
    "            # allwordcounts = dict()\n",
    "            # nah, let's store them as in the vector\n",
    "            allwordcounts = np.zeros(len(all_counts_1k))\n",
    "            for file in my_files:\n",
    "                ########################################\n",
    "                # this loads the files\n",
    "                f = open(file,\"r\")\n",
    "                rawtext = f.read()\n",
    "                f.close()\n",
    "                # add to the full dict\n",
    "                # dictify_general(rawtext,allwordcounts)\n",
    "                # add it to the vector\n",
    "                my_list = listify(rawtext)\n",
    "                for word in my_list:\n",
    "                    if word in all_words_1k_dict:\n",
    "                        allwordcounts[all_words_1k_dict[word]] += 1\n",
    "\n",
    "            scores[i] = bayes_score(all_counts_1k,pos_params,neg_params,allwordcounts)\n",
    "\n",
    "        # now save those scores\n",
    "        neg_results_all[k] = scores\n",
    "        neg_means[k] = np.mean(scores)\n",
    "        print(\"mean score is {0}\".format(neg_means[k]))\n",
    "        neg_std[k] = np.std(scores)\n",
    "\n",
    "    # print(neg_results_all)\n",
    "    # print(pos_results_all)\n",
    "    print(neg_means)\n",
    "    print(pos_means)\n",
    "    print(neg_std)\n",
    "    print(pos_std)\n",
    "\n",
    "    overlapping = np.zeros(len(allLengths))\n",
    "    for i in range(len(allLengths)):\n",
    "        average_score = np.mean(neg_results_all[i]+pos_results_all[i])\n",
    "        overlapping[i] = float(len(np.where(pos_results_all[i] < average_score)[0]) + len(np.where(neg_results_all[i] > average_score)[0])) / ( len( neg_results_all[i] ) + len( pos_results_all[i] ) )\n",
    "\n",
    "    print(overlapping)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_axes([0.15,0.2,0.7,0.7])\n",
    "    ax1.plot(np.log10(allLengths),overlapping,linewidth=2,color=\"#ef8a62\",)\n",
    "    ax1.set_xlabel(\"log10(Number of Reviews)\")\n",
    "    ax1.set_ylabel(\"Fraction overlapping\")\n",
    "    # xlim with a little space\n",
    "    ax1.set_xlim(np.log10([allLengths[0]-.1,allLengths[-1]+.1])) \n",
    "    ax1.set_title(\"Sentiment over many random samples for {0}\".format(\"Naive Bayes\"))\n",
    "    # mysavefig(\"moviereviews-scores-{0}.png\".format(\"naive-bayes\"))\n",
    "    mysavefig(\"moviereviews-scores-{0}.pdf\".format(\"naive-bayes\"),folder=\"../figures/NB\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_axes([0.15,0.2,0.7,0.7])\n",
    "    ax1.errorbar(np.log10(allLengths),pos_means,pos_std,linewidth=2,color=\"#ef8a62\",)\n",
    "    ax1.errorbar(np.log10(allLengths),neg_means,neg_std,linewidth=2,color=\"#2b8cbe\",)\n",
    "    ax1.legend([\"Positive reviews\",\"Negative reviews\"],loc=\"best\")\n",
    "    ax1.set_xlabel(\"log10(Number of Reviews)\")\n",
    "    ax1.set_ylabel(\"Sentiment\")\n",
    "    # xlim with a little space\n",
    "    ax1.set_xlim(np.log10([allLengths[0]-.1,allLengths[-1]+.1])) \n",
    "    # ax1.set_ylim([0,24])\n",
    "    # plt.xticks([float(i)+0.5 for i in range(4)])\n",
    "    # plt.yticks([float(i)+0.5 for i in range(3)])\n",
    "    # ax1.set_xticklabels([1,5,25,50])\n",
    "    # ax1.set_yticklabels([22,28,35])\n",
    "    ax1.set_title(\"Sentiment over many random samples for {0}\".format(\"Naive Bayes\"))\n",
    "    # mysavefig(\"moviereviews-{0}.png\".format(\"naive-bayes\"))\n",
    "    mysavefig(\"moviereviews-{0}.pdf\".format(\"naive-bayes\"),folder=\"../figures/NB\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NB_on_NYT():\n",
    "    \"\"\"Train NB on movie reviews, test it on NYT Society section.\"\"\"\n",
    "\n",
    "    print(\"training\")\n",
    "    posfiles = [\"../data/moviereviews/txt_sentoken/pos/\"+x for x in listdir(\"../data/moviereviews/txt_sentoken/pos\") if \".txt\" in x]\n",
    "    negfiles = [\"../data/moviereviews/txt_sentoken/neg/\"+x for x in listdir(\"../data/moviereviews/txt_sentoken/neg\") if \".txt\" in x]\n",
    "\n",
    "    # randomly select N reviews for training\n",
    "    training_size = 200\n",
    "\n",
    "    pos_files_training = np.random.choice(np.arange(len(posfiles)),size=training_size/2,replace=False)\n",
    "    # go add up all the words for training\n",
    "    poswordcounts = dict()\n",
    "    for file in [posfiles[i] for i in pos_files_training]:\n",
    "        f = open(file,\"r\")\n",
    "        postext = f.read()\n",
    "        f.close()\n",
    "        dictify_general(postext,poswordcounts)\n",
    "    # select the negative reviews for training\n",
    "    neg_files_training = np.random.choice(np.arange(len(negfiles)),size=training_size/2,replace=False)\n",
    "    # go add up the words\n",
    "    negwordcounts = dict()\n",
    "    for file in [negfiles[i] for i in neg_files_training]:\n",
    "        f = open(file,\"r\")\n",
    "        negtext = f.read()\n",
    "        f.close()\n",
    "        dictify_general(negtext,negwordcounts)\n",
    "\n",
    "    all_counts,all_words = loadMovieReviews()\n",
    "    # take the top 1000 words\n",
    "    all_counts_1k = all_counts[30:5000]\n",
    "    all_words_1k = all_words[30:5000]\n",
    "    # build this for speed later\n",
    "    all_words_1k_dict = dict()\n",
    "    for i,word in enumerate(all_words_1k):\n",
    "        all_words_1k_dict[word] = i\n",
    "\n",
    "    pos_params,neg_params = train_bayes(all_words_1k,poswordcounts,negwordcounts)\n",
    "\n",
    "    # above from naive_bayes() function\n",
    "    # now get the NYT section loaded\n",
    "    print(\"loading NYT\")\n",
    "    sections = [\"arts\",\"books\",\"classified\",\"cultural\",\"editorial\",\"education\",\"financial\",\"foreign\",\"home\",\"leisure\",\"living\",\"magazine\",\"metropolitan\",\"movies\",\"national\",\"regional\",\"science\",\"society\",\"sports\",\"style\",\"television\",\"travel\",\"week-in-review\",\"weekend\",]\n",
    "    scores = np.zeros(len(sections))\n",
    "    for i,section in enumerate(sections):\n",
    "        a = pickle.load( open(\"../data/nyt/sections/NYT_{0}.dict\".format(section), \"rb\") )\n",
    "\n",
    "        allwordcounts = np.zeros(len(all_counts_1k))\n",
    "        for word in a:\n",
    "            if word in all_words_1k_dict:\n",
    "                allwordcounts[all_words_1k_dict[word]] = a[word]\n",
    "\n",
    "        print(\"scoring\")\n",
    "        score = bayes_score(all_counts_1k,pos_params,neg_params,allwordcounts)\n",
    "        print(\"{0} score={1}\".format(section,score))\n",
    "        scores[i] = score\n",
    "    indexer = sorted(range(len(scores)),key=lambda k: scores[k],reverse=True)\n",
    "    sections_sorted = [sections[i] for i in indexer]\n",
    "    my_happs_sorted = np.array([scores[i] for i in indexer])\n",
    "    print(sections_sorted)\n",
    "\n",
    "    avg_happs_unweighted = my_happs_sorted.mean()\n",
    "\n",
    "    fig = plt.figure(figsize=(12,10))\n",
    "    ax = fig.add_axes([.2,.2,.7,.7])\n",
    "    # ax.bar(np.arange(len(sections)),np.array(my_happs_sorted),orientation=\"vertical\")\n",
    "    # ax.bar(0,.8,np.array(my_happs_sorted)-avg_happs,bottom=np.arange(len(sections)),orientation=\"horizontal\")\n",
    "    bar_height = 0.8\n",
    "    happs_diff = np.array(my_happs_sorted)-avg_happs_unweighted\n",
    "    rects1 = ax.bar(0,bar_height,happs_diff,bottom=np.arange(len(sections)),orientation=\"horizontal\")\n",
    "    ax.set_ylim([bar_height-1,len(sections)])\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xlabel(\"Happs diff from unweighted average\")\n",
    "    ax.set_title(\"Ranking of NYT Sections by {0}\".format(\"NB\"))\n",
    "\n",
    "    def autolabel(rects):\n",
    "        # attach some text labels\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            width = rect.get_width()\n",
    "            y = int(np.floor(rect.get_y()))\n",
    "\n",
    "            x = rect.get_x()\n",
    "            if happs_diff[y] > 0:\n",
    "                ax.text(-0.0005, y+bar_height/2., \"{0}. {1}\".format(y+1,sections_sorted[y].capitalize()),\n",
    "                        ha=\"right\", va=\"center\")\n",
    "            else:\n",
    "                ax.text(.0005, y+bar_height/2., \"{0}. {1}\".format(y+1,sections_sorted[y].capitalize()),\n",
    "                        ha=\"left\", va=\"center\")\n",
    "\n",
    "    autolabel(rects1)\n",
    "\n",
    "    # mysavefig(\"NYT-sorted-{0}.png\".format(\"NB\"))\n",
    "    mysavefig(\"NYT-sorted-{0}.pdf\".format(\"NB\"),folder=\"../figures/nyt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NB_wordshift(q=False):\n",
    "    print(\"training\")\n",
    "    posfiles = [\"../data/moviereviews/txt_sentoken/pos/\"+x for x in listdir(\"../data/moviereviews/txt_sentoken/pos\") if \".txt\" in x]\n",
    "    negfiles = [\"../data/moviereviews/txt_sentoken/neg/\"+x for x in listdir(\"../data/moviereviews/txt_sentoken/neg\") if \".txt\" in x]\n",
    "\n",
    "    # randomly select N reviews for training\n",
    "    training_size = 200\n",
    "\n",
    "    pos_files_training = np.random.choice(np.arange(len(posfiles)),size=training_size/2,replace=False)\n",
    "    # go add up all the words for training\n",
    "    poswordcounts = dict()\n",
    "    for file in [posfiles[i] for i in pos_files_training]:\n",
    "        f = open(file,\"r\")\n",
    "        postext = f.read()\n",
    "        f.close()\n",
    "        dictify_general(postext,poswordcounts)\n",
    "    # select the negative reviews for training\n",
    "    neg_files_training = np.random.choice(np.arange(len(negfiles)),size=training_size/2,replace=False)\n",
    "    # go add up the words\n",
    "    negwordcounts = dict()\n",
    "    for file in [negfiles[i] for i in neg_files_training]:\n",
    "        f = open(file,\"r\")\n",
    "        negtext = f.read()\n",
    "        f.close()\n",
    "        dictify_general(negtext,negwordcounts)\n",
    "\n",
    "    all_counts,all_words = loadMovieReviews()\n",
    "    # take the top 1000 words\n",
    "    all_counts_1k = all_counts[30:5000]\n",
    "    all_words_1k = all_words[30:5000]\n",
    "    # build this for speed later\n",
    "    all_words_1k_dict = dict()\n",
    "    for i,word in enumerate(all_words_1k):\n",
    "        all_words_1k_dict[word] = i\n",
    "\n",
    "    pos_params,neg_params = train_bayes(all_words_1k,poswordcounts,negwordcounts)\n",
    "\n",
    "    my_counts = np.ones(len(all_counts_1k))\n",
    "    # given a text word vector my_counts, the unweighted probabilities are this\n",
    "    prob_pos = my_counts*pos_params/(all_counts_1k/sum(all_counts_1k))\n",
    "    prob_neg = my_counts*neg_params/(all_counts_1k/sum(all_counts_1k))\n",
    "    # really just each vector multiplied together\n",
    "    # but we take the log, and sum (for floating point-ness)\n",
    "    \n",
    "    # now let's normalize this, per peer request:\n",
    "    prob_pos = prob_pos/prob_pos.sum()\n",
    "    prob_neg = prob_neg/prob_neg.sum()\n",
    "\n",
    "    # using 1's, and taking the max of the ratios will give the most informative\n",
    "    ratio_pos = prob_pos/prob_neg\n",
    "    ratio_neg = prob_neg/prob_pos\n",
    "\n",
    "    indexer = sorted(range(len(ratio_pos)),key=lambda k: ratio_pos[k],reverse=True)\n",
    "    ratio_pos_sorted = [ratio_pos[i] for i in indexer]\n",
    "    words_pos_sorted = [all_words_1k[i] for i in indexer]\n",
    "    indexer = sorted(range(len(ratio_pos)),key=lambda k: ratio_neg[k],reverse=True)\n",
    "    ratio_neg_sorted = [ratio_neg[i] for i in indexer]\n",
    "    words_neg_sorted = [all_words_1k[i] for i in indexer]\n",
    "    \n",
    "    movie_results = [[],[],[],[]]\n",
    "\n",
    "    if not q:\n",
    "        print(\"-\"*80)\n",
    "        print(\"-\"*80)\n",
    "        print(\"top positive words:\")\n",
    "    for i in range(10):\n",
    "        movie_results[0].append([ratio_pos_sorted[i],words_pos_sorted[i]])\n",
    "        if not q:\n",
    "            print(\"{1}: {0}\".format(ratio_pos_sorted[i],words_pos_sorted[i]))\n",
    "    if not q:\n",
    "        print(\"-\"*80)\n",
    "        print(\"top negative words:\")\n",
    "    for i in range(10):\n",
    "        movie_results[1].append([ratio_neg_sorted[i],words_neg_sorted[i]])\n",
    "        if not q:\n",
    "            print(\"{1}: {0}\".format(ratio_neg_sorted[i],words_neg_sorted[i]))\n",
    "    if not q:\n",
    "        print(\"-\"*80)\n",
    "        print(\"-\"*80)\n",
    "\n",
    "    # using 1's, and taking the max of the ratios will give the most informative\n",
    "    diff_pos = prob_pos-prob_neg\n",
    "    diff_neg = prob_neg-prob_pos\n",
    "\n",
    "    indexer = sorted(range(len(diff_pos)),key=lambda k: diff_pos[k],reverse=True)\n",
    "    diff_pos_sorted = [diff_pos[i] for i in indexer]\n",
    "    words_pos_sorted = [all_words_1k[i] for i in indexer]\n",
    "    indexer = sorted(range(len(diff_neg)),key=lambda k: diff_neg[k],reverse=True)\n",
    "    diff_neg_sorted = [diff_neg[i] for i in indexer]\n",
    "    words_neg_sorted = [all_words_1k[i] for i in indexer]\n",
    "\n",
    "    if not q:\n",
    "        print(\"top positive words:\")\n",
    "    for i in range(10):\n",
    "        movie_results[2].append([diff_pos_sorted[i],words_pos_sorted[i]])\n",
    "        if not q:\n",
    "            print(\"{1}: {0}\".format(diff_pos_sorted[i],words_pos_sorted[i]))\n",
    "    if not q:\n",
    "        print(\"-\"*80)\n",
    "        print(\"top negative words:\")\n",
    "    for i in range(10):\n",
    "        movie_results[3].append([diff_neg_sorted[i],words_neg_sorted[i]])\n",
    "        if not q:\n",
    "            print(\"{1}: {0}\".format(diff_neg_sorted[i],words_neg_sorted[i]))\n",
    "    if not q:\n",
    "        print(\"-\"*80)\n",
    "        print(\"-\"*80)\n",
    "    \n",
    "    section=\"society\"\n",
    "    a = pickle.load( open(\"../data/nyt/sections/NYT_{0}.dict\".format(section), \"rb\") )\n",
    "\n",
    "    allwordcounts = np.zeros(len(all_counts_1k))\n",
    "    for word in a:\n",
    "        if word in all_words_1k_dict:\n",
    "            allwordcounts[all_words_1k_dict[word]] = a[word]\n",
    "\n",
    "    prob_pos = allwordcounts*pos_params/(all_counts_1k/sum(all_counts_1k))\n",
    "    prob_neg = allwordcounts*neg_params/(all_counts_1k/sum(all_counts_1k))\n",
    "    \n",
    "    # now let's normalize this, per peer request:\n",
    "    prob_pos = prob_pos/prob_pos.sum()\n",
    "    prob_neg = prob_neg/prob_neg.sum()\n",
    "\n",
    "    # using 1's, and taking the max of the ratios will give the most informative\n",
    "    ratio_pos = prob_pos/prob_neg\n",
    "    ratio_neg = prob_neg/prob_pos\n",
    "\n",
    "    indexer = sorted(range(len(ratio_pos)),key=lambda k: ratio_pos[k],reverse=True)\n",
    "    ratio_pos_sorted = [ratio_pos[i] for i in indexer]\n",
    "    words_pos_sorted = [all_words_1k[i] for i in indexer]\n",
    "    indexer = sorted(range(len(ratio_pos)),key=lambda k: ratio_neg[k],reverse=True)\n",
    "    ratio_neg_sorted = [ratio_neg[i] for i in indexer]\n",
    "    words_neg_sorted = [all_words_1k[i] for i in indexer]\n",
    "    \n",
    "    times_results = [[],[],[],[]]\n",
    "\n",
    "    if not q:\n",
    "        print(\"-\"*80)\n",
    "        print(\"-\"*80)\n",
    "        print(\"top positive words:\")\n",
    "    for i in range(10):\n",
    "        times_results[0].append([ratio_pos_sorted[i],words_pos_sorted[i]])\n",
    "        if not q:\n",
    "            print(\"{1}: {0}\".format(ratio_pos_sorted[i],words_pos_sorted[i]))\n",
    "    if not q:\n",
    "        print(\"-\"*80)\n",
    "        print(\"top negative words:\")\n",
    "    for i in range(10):\n",
    "        times_results[1].append([ratio_neg_sorted[i],words_neg_sorted[i]])\n",
    "        if not q:\n",
    "            print(\"{1}: {0}\".format(ratio_neg_sorted[i],words_neg_sorted[i]))\n",
    "    if not q:\n",
    "        print(\"-\"*80)\n",
    "        print(\"-\"*80)\n",
    "\n",
    "    # using 1's, and taking the max of the ratios will give the most informative\n",
    "    diff_pos = prob_pos-prob_neg\n",
    "    diff_neg = prob_neg-prob_pos\n",
    "\n",
    "    indexer = sorted(range(len(diff_pos)),key=lambda k: diff_pos[k],reverse=True)\n",
    "    diff_pos_sorted = [diff_pos[i] for i in indexer]\n",
    "    words_pos_sorted = [all_words_1k[i] for i in indexer]\n",
    "    indexer = sorted(range(len(diff_neg)),key=lambda k: diff_neg[k],reverse=True)\n",
    "    diff_neg_sorted = [diff_neg[i] for i in indexer]\n",
    "    words_neg_sorted = [all_words_1k[i] for i in indexer]\n",
    "\n",
    "    if not q:\n",
    "        print(\"top positive words:\")\n",
    "    for i in range(10):\n",
    "        times_results[2].append([diff_pos_sorted[i],words_pos_sorted[i]])\n",
    "        if not q:\n",
    "            print(\"{1}: {0}\".format(diff_pos_sorted[i],words_pos_sorted[i]))\n",
    "    if not q:\n",
    "        print(\"-\"*80)\n",
    "        print(\"top negative words:\")\n",
    "    for i in range(10):\n",
    "        times_results[3].append([diff_neg_sorted[i],words_neg_sorted[i]])\n",
    "        if not q:\n",
    "            print(\"{1}: {0}\".format(diff_neg_sorted[i],words_neg_sorted[i]))\n",
    "    if not q:\n",
    "        print(\"-\"*80)\n",
    "        print(\"-\"*80)\n",
    "\n",
    "    score = bayes_score(all_counts_1k,pos_params,neg_params,allwordcounts)\n",
    "\n",
    "    if not q:\n",
    "        print(\"{0} score={1}\".format(section,score))\n",
    "    \n",
    "    return movie_results,times_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bayes_many_samples(n_trials):\n",
    "    posfiles = [\"../data/moviereviews/txt_sentoken/pos/\"+x for x in listdir(\"../data/moviereviews/txt_sentoken/pos\") if \".txt\" in x]\n",
    "    negfiles = [\"../data/moviereviews/txt_sentoken/neg/\"+x for x in listdir(\"../data/moviereviews/txt_sentoken/neg\") if \".txt\" in x]\n",
    "\n",
    "    results = [0.0 for i in range(n_trials)]\n",
    "    n_stopwords = 0\n",
    "\n",
    "    for trial_i in range(n_trials):\n",
    "\n",
    "        print(len(posfiles))\n",
    "        print(len(negfiles))\n",
    "        # randomly select N reviews for training\n",
    "        training_size = 200\n",
    "    \n",
    "        pos_files_training = np.random.choice(np.arange(len(posfiles)),size=training_size/2,replace=False)\n",
    "        print(pos_files_training)\n",
    "        # go add up all the words for training\n",
    "        poswordcounts = dict()\n",
    "        for file in [posfiles[i] for i in pos_files_training]:\n",
    "            f = open(file,\"r\")\n",
    "            postext = f.read()\n",
    "            f.close()\n",
    "            dictify_general(postext,poswordcounts)\n",
    "        # select the negative reviews for training\n",
    "        neg_files_training = np.random.choice(np.arange(len(negfiles)),size=training_size/2,replace=False)\n",
    "        print(neg_files_training)\n",
    "        # go add up the words\n",
    "        negwordcounts = dict()\n",
    "        for file in [negfiles[i] for i in neg_files_training]:\n",
    "            f = open(file,\"r\")\n",
    "            negtext = f.read()\n",
    "            f.close()\n",
    "            dictify_general(negtext,negwordcounts)\n",
    "    \n",
    "        all_counts,all_words = loadMovieReviews()\n",
    "        # take the top 1000 words\n",
    "        all_counts_1k = all_counts[n_stopwords:5000]\n",
    "        all_words_1k = all_words[n_stopwords:5000]\n",
    "        # build this for speed later\n",
    "        all_words_1k_dict = dict()\n",
    "        for i,word in enumerate(all_words_1k):\n",
    "            all_words_1k_dict[word] = i\n",
    "    \n",
    "        pos_params,neg_params = train_bayes(all_words_1k,poswordcounts,negwordcounts)\n",
    "    \n",
    "        # classify all positive reviews\n",
    "        print(\"positive reviews\")\n",
    "        correct_pos = np.zeros(len(posfiles)-training_size/2)\n",
    "        for i,file in enumerate([posfiles[i] for i in range(len(posfiles)) if i not in pos_files_training]):\n",
    "            f = open(file,\"r\")\n",
    "            postext = f.read()\n",
    "            f.close()\n",
    "            my_poswordcounts = dict()\n",
    "            dictify_general(postext,my_poswordcounts)\n",
    "            my_pos_counts = np.array([float(my_poswordcounts[word]) if word in my_poswordcounts else 0.0 for word in all_words_1k])\n",
    "    \n",
    "            score = bayes_score(all_counts_1k,pos_params,neg_params,my_pos_counts)\n",
    "    \n",
    "            if score > 0:\n",
    "                correct_pos[i] = 1\n",
    "    \n",
    "        pos_accuracy = sum(correct_pos)/len(correct_pos)\n",
    "        print(sum(correct_pos)/len(correct_pos))\n",
    "    \n",
    "        # classify all negative reviews\n",
    "        print(\"negative reviews\")\n",
    "        correct_neg = np.zeros(len(negfiles)-training_size/2)\n",
    "        for i,file in enumerate([negfiles[i] for i in range(len(negfiles)) if i not in neg_files_training]):\n",
    "            f = open(file,\"r\")\n",
    "            negtext = f.read()\n",
    "            f.close()\n",
    "            my_wordcounts = dict()\n",
    "            dictify_general(negtext,my_wordcounts)\n",
    "            my_counts = np.array([float(my_wordcounts[word]) if word in my_wordcounts else 0.0 for word in all_words_1k])\n",
    "    \n",
    "            score = bayes_score(all_counts_1k,pos_params,neg_params,my_counts)\n",
    "    \n",
    "            if score < 0:\n",
    "                # print(\"correct, score={0}\".format(score))\n",
    "                correct_neg[i] = 1\n",
    "    \n",
    "        neg_accuracy = sum(correct_neg)/len(correct_neg)\n",
    "        print(sum(correct_neg)/len(correct_neg))\n",
    "    \n",
    "        print(\"overall accuracy: {0:.1f}\".format((pos_accuracy+neg_accuracy)/2*100))\n",
    "        results[trial_i] = (pos_accuracy+neg_accuracy)/2*100\n",
    "        \n",
    "    print(np.mean(results))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "[761 523 507 693  60 597 453  28 853 270 118 127 374 590 754 941 940 248\n",
      " 183 445 117 820 678  43 675 313 526 240 490 432 751 672 312 558  96 111\n",
      " 980 832 959 139 229 615 465 443 730 522 883  65 925 643 942 469 504 391\n",
      " 612 899 887 963 340 474  82 829 145 620 834 399 907 376   9 531  39 278\n",
      " 477 551 901 630 965  31  86 692  62 216 669 244 471 720 156 227 714 848\n",
      " 825  21 564 788  38 992 553 704 798 707]\n",
      "[ 22 579  26 262 880 574 824 416 552 336 785 689 839 157 587 350 343  64\n",
      " 522 413   1 401 868 692 164 179   7 730 435  51 797  29 956 903  66 886\n",
      " 659 279 970 775  34 870 473 632 891 308 863 191 827 667 428 506 404 609\n",
      " 895 483 357 943 739 422 873 961  95  92 407 861 411 679 911 746 440 311\n",
      " 927  61 439 545 380 340 119 563 171 635  76 441 180 625 637 558 509 468\n",
      " 753 293 384 436 788  57 514  80 808 146]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:10: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:20: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 49910 unique words in this corpus\n",
      "positive reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:43: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.797777777778\n",
      "negative reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:62: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.714444444444\n",
      "overall accuracy: 75.6\n",
      "pos: taking 900 samples of 1 reviews\n",
      "mean score is 0.3017582361623197\n",
      "pos: taking 2000 samples of 2 reviews\n",
      "mean score is 0.1403971684898872\n",
      "pos: taking 1500 samples of 3 reviews\n",
      "mean score is 0.11034835924242371\n",
      "pos: taking 1500 samples of 5 reviews\n",
      "mean score is 0.08854013358847249\n",
      "pos: taking 1000 samples of 7 reviews\n",
      "mean score is 0.07357304813942502\n",
      "pos: taking 900 samples of 10 reviews\n",
      "mean score is 0.06461946056958803\n",
      "pos: taking 750 samples of 15 reviews\n",
      "mean score is 0.053409946868337034\n",
      "pos: taking 600 samples of 25 reviews\n",
      "mean score is 0.04152686155050998\n",
      "pos: taking 500 samples of 40 reviews\n",
      "mean score is 0.03107676382717013\n",
      "pos: taking 250 samples of 60 reviews\n",
      "mean score is 0.024137089788770816\n",
      "pos: taking 100 samples of 80 reviews\n",
      "mean score is 0.01991244882912512\n",
      "pos: taking 75 samples of 100 reviews\n",
      "mean score is 0.01651506549022935\n",
      "pos: taking 75 samples of 150 reviews\n",
      "mean score is 0.01186107011427974\n",
      "pos: taking 40 samples of 250 reviews\n",
      "mean score is 0.0079650580909693\n",
      "pos: taking 25 samples of 400 reviews\n",
      "mean score is 0.005616631177398465\n",
      "pos: taking 15 samples of 600 reviews\n",
      "mean score is 0.004460492872191927\n",
      "pos: taking 1 samples of 900 reviews\n",
      "mean score is 0.003745382349495341\n",
      "neg: taking 900 samples of 1 reviews\n",
      "mean score is -0.14336264760713854\n",
      "neg: taking 2000 samples of 2 reviews\n",
      "mean score is -0.08101428172683635\n",
      "neg: taking 1500 samples of 3 reviews\n",
      "mean score is -0.05684999382097677\n",
      "neg: taking 1500 samples of 5 reviews\n",
      "mean score is -0.03348962034382769\n",
      "neg: taking 1000 samples of 7 reviews\n",
      "mean score is -0.02583706101111999\n",
      "neg: taking 900 samples of 10 reviews\n",
      "mean score is -0.020047881016309235\n",
      "neg: taking 750 samples of 15 reviews\n",
      "mean score is -0.012045982149728015\n",
      "neg: taking 600 samples of 25 reviews\n",
      "mean score is -0.006744087907263019\n",
      "neg: taking 500 samples of 40 reviews\n",
      "mean score is -0.0020920126994336003\n",
      "neg: taking 250 samples of 60 reviews\n",
      "mean score is 0.0007869319118198813\n",
      "neg: taking 100 samples of 80 reviews\n",
      "mean score is 0.002550436416977103\n",
      "neg: taking 75 samples of 100 reviews\n",
      "mean score is 0.003212321190145849\n",
      "neg: taking 75 samples of 150 reviews\n",
      "mean score is 0.00427090528988333\n",
      "neg: taking 40 samples of 250 reviews\n",
      "mean score is 0.004638876087552269\n",
      "neg: taking 25 samples of 400 reviews\n",
      "mean score is 0.004449732282286298\n",
      "neg: taking 15 samples of 600 reviews\n",
      "mean score is 0.0040927092949383645\n",
      "neg: taking 1 samples of 900 reviews\n",
      "mean score is 0.0036545957349931046\n",
      "[-0.14336264760713854, -0.081014281726836349, -0.056849993820976767, -0.033489620343827693, -0.025837061011119988, -0.020047881016309235, -0.012045982149728015, -0.0067440879072630192, -0.0020920126994336003, 0.00078693191181988126, 0.0025504364169771031, 0.0032123211901458489, 0.0042709052898833302, 0.0046388760875522694, 0.0044497322822862981, 0.0040927092949383645, 0.0036545957349931046]\n",
      "[0.30175823616231973, 0.1403971684898872, 0.11034835924242371, 0.088540133588472486, 0.073573048139425015, 0.064619460569588033, 0.053409946868337034, 0.041526861550509983, 0.031076763827170131, 0.024137089788770816, 0.019912448829125119, 0.016515065490229349, 0.01186107011427974, 0.0079650580909693004, 0.0056166311773984652, 0.0044604928721919271, 0.0037453823494953409]\n",
      "[3.910065572760054, 0.14901567321260589, 0.074023439343067696, 0.041519893313850011, 0.030965462035154857, 0.022534228432366639, 0.014744354211293323, 0.0091776723263504219, 0.0059551479542071951, 0.0038160703564254985, 0.0026263641265623664, 0.0021174168057061277, 0.0011965083660422622, 0.00071800603944754688, 0.00023057591993113055, 0.00016407018718602844, 0.0]\n",
      "[6.2170047156995576, 0.1367911843807689, 0.077553043103036429, 0.043641590548790826, 0.029859232711699046, 0.021000682260176689, 0.014773848946161634, 0.0091701866215276994, 0.0056456985791142836, 0.0036070939150784692, 0.0025489424483309485, 0.0021224704781292302, 0.0012776051246706709, 0.00052720423985944803, 0.00032293027398278733, 0.0001148979668214445, 0.0]\n",
      "[ 0.25277778  0.161       0.117       0.06866667  0.046       0.02555556\n",
      "  0.01333333  0.00333333  0.004       0.002       0.          0.          0.\n",
      "  0.0125      0.02        0.1         0.        ]\n"
     ]
    }
   ],
   "source": [
    "naive_bayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "[351 897 592 510  91 721 572 273 504 625 735 725 393 930 502 815 131 926\n",
      " 499 871  97 632 200 959 500 828  44 494 127 488 684 800 152  66  60 130\n",
      " 621  14 669  59 156 687 125 432  21  71 448 414 445 412 444  81  57 271\n",
      " 694 506  47 483 336 145 839 465 458 505 492 119 895 980 489  76 927 258\n",
      " 366 123 730 819 854 727 237 670  11 306 641 288 314 101 295 820 887 265\n",
      " 471 223 173 216 186 342  87 629 468 129]\n",
      "[266 347 571 632 597 546 236 134 870 988  46 360 349 230  65 208 452 286\n",
      " 847 907 382 562 184 169 998 155 700 817 164 593 419 840 693 661 880 621\n",
      " 109 899 963 545 885 270 695 190 470 773 218 878 765 576 345 357  38  97\n",
      " 779 523 900 881 552 374 219 915 398  42 104 105  70 557 272 403 390 281\n",
      " 247 911  78 914 492 490 659 363 872  20 305 474 933 554  64 172 778 540\n",
      " 827 944 180 860 809 710 791 775 316 801]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:15: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:25: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 49910 unique words in this corpus\n",
      "positive reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:48: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.644444444444\n",
      "negative reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:67: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.833333333333\n",
      "overall accuracy: 73.9\n",
      "1000\n",
      "1000\n",
      "[ 92 841 177 240 788 134 368 971 918 443 491 929 664 890 305  75 989 774\n",
      " 313  23 952 662 845 172   9 687 358 638 310 775 309 112  15 126 100 579\n",
      "  86 220 583 604 191 471 936 388 705 459 610 797 791  32 422 110  38 562\n",
      " 271 954 927 656 109 916 983  65 334 607 436 718  50 517  45 939 633 407\n",
      "  28 262 965 591 219 500 449 101 234 754 453  10 498  52 683 333  22 131\n",
      " 180  39 510 378 566 995 858 133 367 208]\n",
      "[952 314 431 982 700 838 540 737 375 872 392 589 973 374 155  29 795 593\n",
      " 866 929 996 550 794 117 688 105 141 398 133 736 899 303 305 796 667 342\n",
      " 984  17  76 490 935 294 680 368 658 108 717 601 845 653  90 397 318 918\n",
      "  40 171 317 249 172 287 325 785 770 923 299 465 903 466 798 221 300 112\n",
      " 577 185 439 230 854  33  66  38 168 253 630 384 948 861 946 389 919 636\n",
      " 201 424 817 605 306 528 208 440 265 386]\n",
      "there are 49910 unique words in this corpus\n",
      "positive reviews\n",
      "0.651111111111\n",
      "negative reviews\n",
      "0.874444444444\n",
      "overall accuracy: 76.3\n",
      "1000\n",
      "1000\n",
      "[354 507 175 300   7  47 150 812 526 153 205 780 698 249 348 792 706 722\n",
      " 215 798 254 702 320 531 561 811 629 981 154 555 370 727 369 674 357 744\n",
      " 820 218 682 307 353 332 439 201 735 522 925 891 765 476 777 973 888 247\n",
      " 745 165 520 653 753 182  24 473 837 440 450 402 878 736 790  75 342 552\n",
      " 791  30 662 112 630 605 852 842 293 871  78 475 627 285 684 873 721  59\n",
      " 974 872 892 299 346  74 456 569 928 432]\n",
      "[534 232 806 320 937 228 560 734 492 900 834 588 712 956 528 589 185 394\n",
      " 331 757 111 781 898  68 400  13 760 722 969 894 488 863 466 848 519 106\n",
      " 438 805 134 726 787 539  57 970 840  98 663 505 389 169 310 381 662 756\n",
      "  32 702 300 867 102 397 570 154 911  55 271 178 607 706 214 493 861  89\n",
      " 818 441 751 129 980 518 225 785  94 361 280 657 638 951 342 778 208 794\n",
      " 748 294 599 434 522 183 641 746 796 152]\n",
      "there are 49910 unique words in this corpus\n",
      "positive reviews\n",
      "0.62\n",
      "negative reviews\n",
      "0.852222222222\n",
      "overall accuracy: 73.6\n",
      "1000\n",
      "1000\n",
      "[531 182 888 324 372 890 811 710 776 310 246 546 699 968 790 994 880 277\n",
      " 954 553 978 399 532 384  78 544 127 631 128 292 289 253 164 427 730 612\n",
      " 913 687 520 853 947 507 143 721 558 755 360 652 288 756 271 972 615 504\n",
      " 174 889 607 851  24 527 284 752 771 556 632 717 827 323 290 619 896 700\n",
      "  45  97 502 274  79 287 483 549 148 572 386 923 370 839 233 206 498 636\n",
      " 244 552 551 668 321 826 965 982 595 186]\n",
      "[812 235 908 456 431 309 537 229 541 419 996 474 564 935 650 711 503 723\n",
      " 886 579 804 292 512 863 893 930 657 417 976 890 514 853 741 491 154  67\n",
      " 187 826 835 340 707 949 694 175 672 755 568 658 955 884 167 590  13  73\n",
      " 713 480  58  61 766  57 632 178  11 810 842  20 973 691 362 243  93 385\n",
      " 177 968 878 784  68 285 463 400 439 668 159 261 458 696  85  43  92 436\n",
      " 642 174 286 398 620 936 954 228 958 524]\n",
      "there are 49910 unique words in this corpus\n",
      "positive reviews\n",
      "0.712222222222\n",
      "negative reviews\n",
      "0.806666666667\n",
      "overall accuracy: 75.9\n",
      "1000\n",
      "1000\n",
      "[915 205 521 893 977 991 438 498 313 370 292 968 152 664 822 520 563 844\n",
      " 998 605 199 103 945 624 405 367 504 756 582 475 229 463 921 594 854 277\n",
      " 746  46 819 133 524  55 234 523 433 454  63 296 334 204 953 995 954 654\n",
      " 909  67 514 894 699 411 590 201 333 731 328 457 413  11 661 499 200 190\n",
      " 465 872 226 794 575  96 309 217 969 374 867 838 319 719  22 757 477 339\n",
      " 557 947 414 918 446   8  56 651 828 950]\n",
      "[437 909 414 871 914 629 882 602 229 182 465 596 246 157 510  79 478  61\n",
      " 927 565 323  44 716  41  13 189 351 181 964 886 929 953 392 378 652 908\n",
      " 676 135 979  45 718  30 988 492 550 823  46 400 100 483 977 769 805 552\n",
      " 356 144   6 260 248 404 115 554 816 421 707 757 359 813 584 398 841 592\n",
      " 657 137  35 820 928 230 336 924 104 708 815 867 942 223 604  80 365 783\n",
      " 251 967 799 250 411 620 455 690 685 314]\n",
      "there are 49910 unique words in this corpus\n",
      "positive reviews\n",
      "0.655555555556\n",
      "negative reviews\n",
      "0.852222222222\n",
      "overall accuracy: 75.4\n",
      "1000\n",
      "1000\n",
      "[695  89 923 965 946 782 131 244 272 684 502  46 851 203  65 880 928 559\n",
      " 979 176 898 247 231 534 924 577 748 789 832 938 428 413 196  85 635 433\n",
      "   9 188 546 964 139 234 311 774 531 552 127 344 105 750 918 659 770 642\n",
      " 269 658 910 138 537 372 278 777 179 199 147  53 914 485  59 555 991 154\n",
      "  99 661 894 982 671 367 539 478 309 152 913 929 515 528 877 974 665 187\n",
      " 412 785 778 170 791 649 604 616 947 327]\n",
      "[861 771 578   8 157 372 512  71 256 272 462 429 775 935 665 261 411 544\n",
      " 304  84 706 412 401 232  54 421 955 448 521 185 477  83 314 969 865 980\n",
      " 875  32 790 594 807  73 787 683 300 808 900 274 374 154  44 979 888 717\n",
      " 871 933 235  86 399 618 587  92 686 647 995 611 986 624 289 945 678 970\n",
      "  67  52 735 999 340  91 747 895 297 147  70 623 604 167 884 914  46 517\n",
      " 794 418 631 830 466  39 978 921  87 174]\n",
      "there are 49910 unique words in this corpus\n",
      "positive reviews\n",
      "0.611111111111\n",
      "negative reviews\n",
      "0.893333333333\n",
      "overall accuracy: 75.2\n",
      "1000\n",
      "1000\n",
      "[333 401 337 981 591 336 311 974 523 263  28 927 388 361 719 352 198 353\n",
      " 761 930 615 937 275 452 895 367 321  14 776 157 373 846 318 335 357 199\n",
      " 915 735 678 858  45 467 212 790 273 675 799 437 277 685  66 744  39 827\n",
      " 865 634 717 259 154 117 530 945 143 880 742 826 262 660  41 909 391 242\n",
      " 697 151 999  47 571 229 813 842 148 606 670 123  56 648 713 296 747 611\n",
      " 227 448 988 619 983 857 511  99 599 164]\n",
      "[585 312 194 355 920 655   0 540 801 173 493 391 189 496 177 791 605 577\n",
      " 749  87 537  27 481 412 609 808 974 770 908 263 762 265 614 198 203 421\n",
      " 404 465 771 855 500 520 756 140 711 916 935 383 666 820 874 409 758 782\n",
      " 849 656 546 209 910 223 963 747 754 670 136 976  96 176 424 451 293 846\n",
      " 135  16 739 388 748 186 944 692  21 602 921 545 372 144 610 970 789 335\n",
      " 700  67  85 290 521 158 488 193 710 300]\n",
      "there are 49910 unique words in this corpus\n",
      "positive reviews\n",
      "0.633333333333\n",
      "negative reviews\n",
      "0.866666666667\n",
      "overall accuracy: 75.0\n",
      "1000\n",
      "1000\n",
      "[136 472 609 539 845 760 223 209 593 782  65 956 208  32 706 847 690  71\n",
      " 447  48 477 793 306 686 850 339 950 374 990 835 870 118 550 622 673 943\n",
      " 596 234 738 730 644 800 765 728 566 955 634 451 435 108 935 312 328 764\n",
      "  38 360 653 625 473  31 264 346 754 389 431 549 737 743 171 703 600 823\n",
      " 226 887 413 407 995 874 794 481 843 448 260 881 124 221 954 920 300 853\n",
      " 229 970 966  12 631 832 140 282 362 373]\n",
      "[195 914 584 788 728 672   1 367 215 183 178 286 968 501 256  78 838 114\n",
      " 251 458 233 383 974  81 806 118 309 916  61 524 407 803  93 671 766  66\n",
      " 299 537 880 489 132 566 358 395 512 637 177 732  12 990 928 242 522 691\n",
      " 417 641 405 539 622 712 316 435 763 632 685 249 955 432 737 693 142 856\n",
      "  50 258 388 612 751 765 479 129 456 978  65 976 157 511 466 212 342 659\n",
      " 651 517 840 320  16 896 819 902 170 904]\n",
      "there are 49910 unique words in this corpus\n",
      "positive reviews\n",
      "0.667777777778\n",
      "negative reviews\n",
      "0.863333333333\n",
      "overall accuracy: 76.6\n",
      "1000\n",
      "1000\n",
      "[957 601 858  38 600 786 799 233 974   8 568 299 789 284 327 732 238 509\n",
      " 184 384 524 561 100 763 728 187 413 933 702 635 109 227 445 482 785 328\n",
      " 422 949  16 757 337 654 490 450 987 753 336 998 583 667 839 245 243 677\n",
      " 108 436 222  10 281 721 400 642 462 930 637 897 937 584 248 563 126 478\n",
      " 352   9 870 720 146 607  81 280 752 618 398 140 784 176 139 392 984 983\n",
      " 840 564 430  88 212 278 931 706 236 522]\n",
      "[692 498  12 707 610 494 387 679 360 587 436 416 180 779 355 154 288 985\n",
      " 452 814 590  49 830 454 200 258 880 450 842 267 421 409 722 989 268  45\n",
      " 818 176 166 789 611 793  79 634 940 339 983 576 694  51 105 825 797 994\n",
      " 897 801 916 982 439 556  76 405 121 625 483 429 828 423 395 282 111 909\n",
      " 312 177 240 202 119 432 108 427 826 368 560 502 760 641 484 787 286 471\n",
      " 676 376   1 473 392 445 134 323 318 717]\n",
      "there are 49910 unique words in this corpus\n",
      "positive reviews\n",
      "0.652222222222\n",
      "negative reviews\n",
      "0.865555555556\n",
      "overall accuracy: 75.9\n",
      "1000\n",
      "1000\n",
      "[515 917 771 990 429 241  75 781 430 994 807 572 763  67 987 183 606 186\n",
      " 846 321 452 414 445 438  71 192 670 864 132 136 684 350 893 625 667 849\n",
      " 558 159 137 831 301 829 344 488 440  92 874 631 931 530 283 464 206 502\n",
      " 108  95 423 255 226 200 292 177 591 436 162 118 428  33   8 161 692 651\n",
      " 590 603 155 880 805 204 798  17 544  45 760 729 332 548 708 293 959 521\n",
      " 371 914 945 809 844   3 338 706 228 697]\n",
      "[598 843  28  15  31 720  94 734  14 574 952 422 377 607 758 665 617 453\n",
      " 480 250  67 306 437 821 416 836 169  78 775 463 594  21 448 165 378 761\n",
      " 787 692 354 786 760 372 240 967  83 163  40 982 769 908 944 436 276  55\n",
      "  51 726 571 977 215 740 497 989 906 440 848  50 247 170 592 717 697 213\n",
      " 945 817 819 327 954 562 382 131 664 999 150 447 319 227 831 500 656 896\n",
      " 706 424 474 676 386 101 361 204 672 173]\n",
      "there are 49910 unique words in this corpus\n",
      "positive reviews\n",
      "0.573333333333\n",
      "negative reviews\n",
      "0.872222222222\n",
      "overall accuracy: 72.3\n",
      "75.0055555556\n"
     ]
    }
   ],
   "source": [
    "bayes_many_samples(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:13: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:22: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 49910 unique words in this corpus\n",
      "loading NYT\n",
      "scoring\n",
      "arts score=0.0011159275640186261\n",
      "scoring\n",
      "books score=0.0012200187812716479\n",
      "scoring\n",
      "classified score=0.0015993922267443228\n",
      "scoring\n",
      "cultural score=0.001261138308587706\n",
      "scoring\n",
      "editorial score=0.0012472466907665902\n",
      "scoring\n",
      "education score=0.011667700557933758\n",
      "scoring\n",
      "financial score=0.0011334191940998717\n",
      "scoring\n",
      "foreign score=0.0011420095448818057\n",
      "scoring\n",
      "home score=0.0017423632375913045\n",
      "scoring\n",
      "leisure score=0.0015426128702282216\n",
      "scoring\n",
      "living score=0.001899736991112455\n",
      "scoring\n",
      "magazine score=0.0013857617664865818\n",
      "scoring\n",
      "metropolitan score=0.0010941508039481462\n",
      "scoring\n",
      "movies score=0.001398978836208764\n",
      "scoring\n",
      "national score=0.0011498957790482822\n",
      "scoring\n",
      "regional score=0.001148435935058978\n",
      "scoring\n",
      "science score=0.0016449263342612763\n",
      "scoring\n",
      "society score=0.0016969451543734082\n",
      "scoring\n",
      "sports score=0.0011298850088227175\n",
      "scoring\n",
      "style score=0.0014925479144223819\n",
      "scoring\n",
      "television score=0.07792182991749835\n",
      "scoring\n",
      "travel score=0.038806614494318714\n",
      "scoring\n",
      "week-in-review score=0.0015283724947402488\n",
      "scoring\n",
      "weekend score=0.0014304607439555306\n",
      "['television', 'travel', 'education', 'living', 'home', 'society', 'science', 'classified', 'leisure', 'week-in-review', 'style', 'weekend', 'movies', 'magazine', 'cultural', 'editorial', 'books', 'national', 'regional', 'foreign', 'financial', 'sports', 'arts', 'metropolitan']\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnwAAAIwCAYAAAAcd7ZBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3Xl8zVf+x/HXCbELsWWzxNrYKUV/1YoYIahaS5hBqbbo\ntENbdLRGtdNdJ91S01FCa99aqkhVUEmINkKLEFQ1FaJENGIJ+f7+uFeakFhzk7h9Px+P76P3fr/n\nnO/5npuZfnqW7zGWZSEiIiIizsulsCsgIiIiIo6lgE9ERETEySngExEREXFyCvhEREREnJwCPhER\nEREnp4BPRERExMkp4BP5kzLGlDLGuBV2PST/GGMmGWNeLux6iEjRo4BPpIgzxjxojNlkjNlvjPmn\nMeZlY8wMY0zT2yizGrAECMnlWmVjzGFjTJnbqfcN1qOFMWaqMWa1McbrimsvGmN+MsbMz3buIWPM\nj8aYucaYGsaYD4wx540x4caYqvY0ZYwxMcaYbcaYTsaYDcaYmfby4o0xXxtjJhtjvjTG/CePet1n\njHnXGPOEMeaxvNLdyPNd8f1bY8z9t1LWDZoB/PV2CjDGPGmMiTPGRGY7d78xJtLeZk2MMf+w/zZf\n2/8m/2WM+cIY0/G2n0BEHKJ4YVdARK7NsqyVxphKQHfLsl4FMMa0BCKMMbUty/r9FspMNsa8DQzJ\n5fJJ4BnLstJvq+I3Zgq2AKUXkON+lmW9bIxJBcYbYzpblvW1ZVlfGGPcLcsKsyd70hiTDjS0LOu4\nPV+6MWY58JZlWReNMS0ty3obwBhTB9hmWVao/fuzV1bIGFMMmGZZVjv7dzdgwy0+Xy8gLtv3V4Ad\nt1jWdVmWdcwYc1tv07cs6wNjzFlsbfuYZVkfW5b1rTFmEvCTZVk/Az8aY5qTsy1bAxuMMTUsy0q5\n7YcRkXylHj6RO9MeoBLQ4DbKyDUwsGwW30a5N6OCZVlplmV9ZllWai7XU4GxQKgxpsTlKl6R5t9A\nO2PMA5AVeHxnWdZF+/Vfr3H/o7mcqwRUvvzFsqzTwMLrP8ofjM0jQM3s5y3LWmsvr6i7CDwB/NsY\nU8V+ziKPvxm7eKAMUMvBdRORW6CAT+TO1BmIxN57ZIypaIx5yRjzV2PMm8aYkvbzzxpjDhlj/I0x\ng+xDoeWuLMwY85V9yK6VMaa/MeaAMaa6MaadMWaHfWhzkDHmbWNMt2z5Jhpjhhtjphtjxhhj3sit\nsvZrj9qHC/vYz/UBatnr2zqP57wcfCYAL+SRIBV4GXjTfqqLZVlfZ7s+P7d89muf5XLuOHDGGLPS\nGNPXGOMOzLbX2RjbPLm/2dvCz37+fmPMa/a2eBWoAdwHNLMPedYxxrQ0xkQZYwbY81S1D4X+zRgz\nxRjjbYypZh82fccYM9jeXlPs6WvY27GPMeYTY0yNPB6rlDHmEfvfwjvGmPLGmObGmK3GmC3GmArG\nmCBjTJIxptM12n0rtmH/d/Jqvyv8FYgCdt5gehEpSJZl6dCho4gfwFBs/zJ9GFtwswgoke36g8Aq\n++dngFHZru0Hmts/vwP0sX/uAMwEvICnrrjfBqCm/fNU4A3752bASvvnFsAK++fnsPXEFc+l7n2A\nF7J9XwTUtX+OuHyfPJ57iP2fdbANNTe4fO6KdMWBffbnaXqN8mYBo2+gvX2A+fZ7ngEes58fDjxr\n/1wfWA6Ut9+7JFAMWyBuLrfvFeW+lO2ZvgCq2z9XAcLtnwOALdnyHM72u95v/xx4OW8udT95+XfA\n9h8G/7V/vgf4wv654uW/g7z+3rKlOwL4Aw9k/63sbTnD/jc5GgjPq046dOgo/EM9fCJ3jkTLshZZ\nlvUicAEYlu3aV8CHxpgnsQUiVbNdK8YfvS6pQPYePg9sgcfKK+6VfejuErbhusv5y9o/17N/B0gB\n6ll/DKNm9zDwY7bvCUDfXNLlxgBYlnUQeBv4KLdE9vuGAB6WZf1wg2XnfkNjSgHnLMsKxja0+zfg\nbfs8yp5ARWNMT6CJ/Vk6Ypvbdt6yrEuWZd1nWVZeQ5+Z9nuUBh6wLCvRXv/fgMbGtpjmErA3W57L\nbboGWGiM+RIoeTlvLs5l+x1isc0jxLKsbUA9+5zEvwCrrtcWlmWdwhbMfwSUyCVJrP1vMhQYA3xj\njPG+XrkiUvAU8IncmbaRM2gKBv6ObegxGsAYc/l/39Y1ApBiwHjgvevc71Iu57bwx3yt+th6eHJT\nipzBgqv9uBHZ6/0Wtt7IvFahnrEft8sDW8+YreEsaxm2nsi62AK23ZZlrbAsa7llWeOx/f+ouVaB\nxpi2V5y6sk0gZ7vk1t5nsfVwzgSm2Bfu5Hq7K+6TfXHecmx/N6Utyzp/jSpntbtlWXOBRGDCNdJj\nWVYCtvYfeK10IlI4FPCJ3Dmy/4v8NLYeJowxtYABwCeWbcWulz3tgFzyXemgZVkbgNPGmF43WIfL\n5Z0CthtjhgE7LMv6Io88S4BW2b43BT6/gXtBtmDFsqwMYBSQ17yz/DTqcsBs/2clbAtlFmMbqsV+\nbSC24e+6xpjy9nP32nsD07AN98IVi2ss2yrWaGNMQ3ue6sABy7IuLzDJ/ptd/hwMlLUHoP/kigUh\n2ZS6PIcT2zzC7AtOFmIbfj12nee/8g0Oo+1l5ck+17EOOXtzRaSI0GtZRIo4+yKJYUAdY8xYy7L+\ngy3w6G2MGYWt9+U9oI/9dRpxwNNArP16VWPMS9iGfbsBKcaY3dj+Jd7YGOOPbSXrf+0rYYsBfsAE\nY0xYtjw7sAVcfsaYh4Fl2HrC/IGTxpj6wNQrexMty/rM2N7b9gS24eQwy7J22Rcv+GHrrZpiWdbh\nK577NWCwMaaC/ZmxLGujMWbWDbZR9uuNgcH2ujayD52G2Icsr5QJfA08a4wpji2Afs2yrDRgvjGm\ntjFmIra5ct9alnXKGDMCeM0Y8x1wyrKsaGPMKeCcMeYp4At7L183INUYE26vz9+NMYex9ZQOMLZ3\nCY4GmhtjumAbNq9qjPkXtuHzwcaYY9gWheS1mGKpvSyD7T8Knsv2W/xg/xv5Jo+8GGPGAf8wxtSz\nLOuf9nwJ9t/j8pD0k8D9QHVjWwRUHFtA+LRlWXn19IpIITJ5j/SIiOTNGDMeWGtZ1g77v/THAL9Y\nljWvkKsm12CM+ZtlWZ8Wdj1EpGBpSFdEblU9YDeAvfdrI7ZVnVLEGNvrdsYZY2pjW2giIn8y6uET\nkVtijPHBtoL1F2yLDSoBH1iWdaFQKyZXMcY0wLaoZ6uVy7sHRcT5KeATERERcXL5smjD3ObejSIi\nIiJyYyzLuuaroHKTb3P4CvsN0kXp+Ne//lXodSgqh9pCbaG2UFuoPdQWaov8O26VFm2IiIiIODkF\nfCIiIiJOTgGfA/j7+xd2FYoMtcUf1BZ/UFv8QW2Rk9rjD2qLP6gtbl++7bQxZcqUrM/+/v5/6h9n\n4MBhHDv2c2FXQ0RE5E/Dw6MWR48eKuxq5LsNGzawYcOG2y4nX17LYoyx8qMcZ2Hb0UjtISIiUnDM\nbS1quFMYY7AKc5WuiIiIiBRNBRrwWZbFuHHjrpsuLCyMRx99lI8//hhfX19eeeUV3njjDfr27XtV\n2szMTN5//32+/fbbG65HWloabdu2zfXa3r176d279w2XJSIiIlLU5dscvutJSUkhLCyMTZs2XTdt\nyZIlmTFjBgALFixgyJAh1KxZk08/vXq/bxcXF2rWrElERAT333//DdWlXLlyrFy5Mtdrd911Fx9/\n/PENlSMiIiJyJyiwHj53d3fGjh2Lm5vbddM2aNAg63P28fjGjRvnmr5ixZvbrz0xMZEtW7bkei0h\nIYG4uLibKk9ERESkKCuwHr6b0apVq1zP33333QC8/vrrNGjQgLi4OKZOnZojzZIlS0hNTeWXX37h\nySef5L333uPLL78kIiKCjz76iMOHDzNp0iSee+45evbsyZYtWzhx4gTnz5/n3LlztGnThvHjx7N9\n+3aOHTvGvHnz8PX15ciRI4wZM4bJkydz+PBhHn74YVatWsXzzz9P9erVHd4mIiIiIrfqjlu0sWbN\nGtLS0ujTpw9ubm45eupSUlKYPn06I0aMoEePHsyYMYOpU6dSuXJlKlSoQNOmTQkNDcXHxwcfHx8A\nVq5cScmSJXnwwQdp0qQJ9erVw93dHYCJEycyePBgevfuzenTp4mIiGDEiBEcP36cbt260aJFi5ua\nOygiIiJSGO64gC8uLo709HTCw8Nxc3PDxeWPR9i7dy+WZREeHk5iYiJeXl4AtGvXjqioqFyXa//9\n739nwYIFtGrVinPnzuW4FhMTQ5UqVQCoVq0a27ZtA8DDwwOAEiVKkJGR4ZDnFBEREckvBR7wXRl0\nJSUl3VT+Vq1a4e7uTmBgII899ljWcKplWfj5+VGuXDkCAwPp1asXnTt3BiA4OJjXXnsNX1/fq+qx\ncuVKZsyYQVRUFKtWrcpxrWnTpvzyyy+Abd5fs2bNrnqGP8M7f0REROTOVmAB35kzZwgJCSE+Pp6Q\nkBDS09NJTU2lX79+uaY/ceIEISEhJCQk8O677xIVFQVA586dqVq1KnPnzmXevHmUK1eOefPmsXHj\nRs6cOcPo0aOZPn06S5Ys4fTp0wA0atQId3d3mjRpAsCWLVtISEhg6dKlHDt2jEWLFvHll18SFBTE\nmjVr2L17N99++y0ffvgh8+bNY+HChVSrVo2uXbsyZ84cYmNj2bZtGytWrGDlypX8/vvvBdOIIiIi\nIrdAO204gHbaEBERKWjaaeNa7rg5fCIiIiJycxTwiYiIiDg5BXwiIiIiTk4Bn4iIiIiTK5I7bdzp\nPDxqcezYTc+nFBERkVvk4VGrsKtQpGmVroiIiMgdQqt0RURERCRXCvhEREREnJzm8DmAp6cvx479\nXNjVEHFKHh61OHr0UGFXQ0TkjqI5fA6gnTZEHOnP8TZ9EZHcaA6fiIiIiORKAZ+IiIiIk7tj5vBl\nZGTw8ccfc+7cOU6dOsXLL7+cZ9qwsDA2b95MmzZtePXVV3n00UdxdXUlJiaGpUuX5ludZs2aRZUq\nVXjwwQfzrUwRERGR/HbHBHxLlixh0KBBuLu7079/f2JiYmjTpk2uaUuWLMmMGTMAWLBgAUOGDKFm\nzZp8+umn+Vqnu+++m3fffVcBn4iIiBRpd8yQ7t69e1m4cCEAderUITExMc+0DRo0yPqcfXJ348aN\n87VOFStWzNfyRERERBzhjgn4nn/+eYYOHQrAjh07aNu2bZ5pW7Vqlev5u+++m+joaPz8/Fi6dCkP\nP/wwKSkpDB8+nM8//5xXXnkFgBUrVuDj48OuXbuIioqiY8eO/Pbbb0yfPp358+fz4osvcv78+fx/\nSBEREREHuGMCvpIlS1K6dGk2btxIQEAAPj4+t1TOvffei7e3N61atSI0NBR3d3c+/PBDevXqRWxs\nLMnJyfTs2ZNu3bpRrlw5KlWqxIwZM0hOTiYqKorg4GCaN2/O8uXL8/kJRURERBzjjgn4AE6ePElk\nZCTjx4+/7bJ8fX2pUqUKAD/88AMLFy7kzJkznDt3DoCBAwcyd+5c9u/fT926ddm5cyeXLl0iPDyc\nCxcu4Obmdtt1EBERESkId1TAN2/ePCZOnMjFixf55ptvAEhKSrrpcrLP61u0aBGLFi1iwIABVK1a\nlcOHDwPQsWNHvvnmG/tLlKFFixaUK1eOwMBABg0aRIsWLa4qS0RERKQoumMCvunTp/Piiy/i4eGB\np6cnnp6epKam0q9fv1zTnzhxgpCQEBISEnj33XeJiooCYMuWLezfv5+33noLAG9vb86cOcOaNWuo\nVasWX375JQAuLi60bt2ae+65BwA/Pz/8/f2ZNWsWixYtwrIs5syZw/bt24mPjy+AFhARERG5Ndpa\nzQG0tZqII2lrNRH589LWaiIiIiKSKwV8IiIiIk5OAZ+IiIiIk1PAJyIiIuLk7pi9dO8kHh61OHbs\npudTisgN8PCoVdhVEBG542iVroiIiMgdQqt0RURERCRXCvhEREREnJwCPgfw9PTFGFPoh6enb2E3\nhYiIiBQBmsPnAEVnpw3tSCAiIuJMNIdPRERERHKlgE9ERETEyRVIwHfhwgXCwsJYunQpw4cPJz09\nPc+0MTExdOjQgcGDBzNz5kymTZtG06ZNiYuLy5Fu8uTJjBw5Mt/rOnnyZJYsWZLv5YqIiIgUlgIJ\n+LZt28b69evp27cvp0+fZv369XmmbdOmDQEBAXTp0oXhw4fzzDPPMHfuXH777bcc6UaMGMGlS5fy\npX7ZA7xnnnmGhx56KF/KFRERESkKCiTgu++++3j//fcBOHHiBPfcc89N5W/WrNk1ewVvx7p169i9\ne3fW99jYWPbv3++Qe4mIiIgUhgLbWu3SpUuEhoYyZMgQPDw8bjjfvHnzGDRoEK1btwbgrbfeokmT\nJhw6dAiAzMxMBg8eTMOGDRk2bBj9+/fnrbfe4oEHHuDDDz+kfPnyxMfHM3r0aDZv3syxY8dIS0sj\nICCAVq1aER4ezr59+1i8eDH9+/fnu+++4+zZs0yePJlVq1aRmprKuXPnqF+/PrVr12bw4ME88sgj\nVK1aldWrV/PBBx84orlERERE8k2BBXyVKlVi9OjR9O/fn/r169O+fftrpo+IiODIkSPExsYyaNAg\nvL29mTFjBhUqVCAoKIj4+Hi2bduGi4sLTzzxBBs3bqRmzZp0794dgL179xIZGcm8efP48ssvsSyL\nPn364Orqyu+//86AAQNYvXo13bt3p1y5cvTv3x+wDSlv3LiR33//nTlz5rBw4UIAAgMDCQ8PJyAg\ngLJly9K9e3feeustxzaaiIiISD4o8FW6d911F/Pnz79uuo4dOzJx4kR69eoFwMGDB/nuu++oW7cu\ncPldd1e7/N65uLg4ateuDUCPHj2oUaMGAIsXL2bz5s2cP38+R76zZ8/m+B4fH0/JkiVzXE9OTgbI\n6qF0cdEiZxERESn6CiRief3113nppZcAOHbsGH5+fgAkJSVdN++gQYOwLItNmzbRsmXLrKDrwoUL\nWcFd8eLFsz6fOHECgCZNmnDgwAHAFgTu27ePXr160ahRIwIDA7Esi19++QVXV1cyMjLYsmVL1j0t\ny6Ju3bo5FopkZmZStWrVHHXTS41FRETkTlAgAd/AgQNp0KABs2bNonTp0jz55JOkpqbSr1+/q9LG\nxMQQERFBeHg4M2fO5KOPPqJ37964urry+OOPs3//flasWMG6devYvn07e/bsoXnz5qSmprJy5UrO\nnj3LnDlzaNSoER07dmTGjBnMnz+fatWq0aJFC7799ltWr16Nl5cXe/bsoUWLFhw9epSUlBQyMzOZ\nO3cuGzduBGD06NHMnj2b0NBQ/vOf//Drr78SERHB3Llz2bp1K/v37+ezzz4riCYUERERuWXaWs0B\ntLWaiIiIOIK2VhMRERGRXCngExEREXFyCvhEREREnJwCPhEREREnp4DPATw8agGm0A9bPUREROTP\nTqt0RURERO4QWqUrIiIiIrlSwCciIiLi5Jwy4PP09MUYU2iHp6dvYTeBiIiISBannMNX+DtdaIcL\nERERyX+awyciIiIiuVLAJyIiIuLkFPAVkFOnTjFhwoRrpomJiaFDhw706tWL9evX57jWqVMnkpKS\n8sw7bNgwtm3bli91FREREedSvLAr8Gcxb948jh8/fs00bdq0ISAggNq1axMQEJDj2vz586lWrVqe\neadNm0blypXzpa4iIiLiXNTDVwASEhLw9fW95fxpaWnExsby22+/5Xo9IyOD2NhYfv7551u+h4iI\niDgvBXwFYNeuXTRu3PiW8xtjCAsLY/fu3WzevJkaNWqwceNGEhIS+L//+z8OHz7MV199xcaNG0lM\nTKRDhw6EhYWxatUqnnzySQCSk5OZNGkSS5cuZcKECbz55pv59XgiIiJSxCngc7CoqCjuu+8+gFt6\nVculS5coW7YsDRs2BKB9+/Y8/vjjAFSrVo0333yTunXr0qJFCwCqV69OQEAAZcuWpXv37vz4448A\nbNmyBR8fH/r27cuuXbsYP358fjyeiIiI3AEU8DnY3r17Wb16NUuWLOHAgQNs2bLlpvKvWrXqqnMD\nBw5k3rx5OYLJK3l4eADg4mL7iZs2bcrvv//OihUr1LsnIiLyJ6NFGw72yCOPAPDzzz/z448/0q5d\nOwCSkpLw8vK6bv59+/Zdda5evXocOnSIs2fP2l8ynbfLvYoHDx6kX79+1K1b92YfQURERO5wCvgK\nwLlz53j//ffZtm0bmzZtonnz5vTr14/IyMgc6WJiYoiIiGDfvn1kZGSwfft2jh49SmJiIuvXr+e3\n337jgQceAKBHjx5ZC0FOnz7NF198QbFixfD39yciIoIjR45QsmRJ9u/fz2effUbnzp3p1q0btWrV\nolatWowbN44aNWoUdFOIiIhIIdDWao6pQZHbWu21117jqaeeonTp0uzfv59///vfzJ49u7CrJSIi\nIjfhVrdWU8DnmBoUuYDv66+/JiUlhbJly3L8+HE8PT3p2rVrYVdLREREboICvmwU8ImIiIgzutWA\nT6t0RURERJycAj4RERERJ+eUAZ+HRy3AFNphu7+IiIhI0eCUc/hEREREnJHm8ImIiIhIrhTwiYiI\niDg5pwv4PD19McYU6uHp6VvYzSAiIiKSxenm8BX+O/hA7+ETERERR9AcPhERERHJlQK+AjBu3Dgy\nMjIIDQ0lOTk5z3QxMTF06NCBXr16sWfPHsLDw2ncuDGPPfYYR44cKcAai4iIiDNRwFcAZs+eTe3a\ntXF1daVatWp5pmvTpg0BAQH06dOHhg0bEhgYSNu2bfnrX/+Kt7d3AdZYREREnEnxwq7An8H777/P\noEGDbimv5gKKiIjI7VLAVwAOHjzImjVr2LVrF88888wtlxMdHc2ePXsoXbo0ZcuWpUOHDgwbNox7\n772Xpk2bsnjxYkaMGEFycjK7du3ihRdeIDk5mffee4969epRoUIFevfunY9PJiIiIncCDekWgBde\neIGuXbtSqlQpwsPDr5t+8+bNzJkzh9mzZ3PgwIGs81OnTmX48OEEBwcze/ZsihUrRq9evTDGEBQU\nxPnz5ylRogS9e/cmIiICgKeeeophw4YxbNgwPv30U4c9o4iIiBRd6uFzsLCwMC5dusSIESMoXbo0\nO3fuJDAw8Jp52rdvz5AhQwDYsGEDAMePHyctLS0rTZkyZdi9ezcAHh4eAJQoUSLr8+Wh4Li4OBIS\nEjhw4ACNGzfO12cTERGRO4MCPgerUqUKrVu3BuDQoUP4+/sDkJSUhJeX1w2XU6lSJTIzM7O+nzx5\nkvr167Nnz55c018O+Fq3bk2TJk2oUaMGTZs2vcWnEBERkTuZAj4H6969O++++y4VK1akevXqBAQE\nkJqaSr9+/YiMjMyRNiYmhvXr11OtWjXuueceEhMTiYuL47PPPqNevXq88847hIaGUqZMGcaMGUOx\nYsVYsWIFxhj8/PyIjY3ls88+o3Xr1uzZs4e1a9fywQcf8P7779OwYUPKlCmj1b4iIiJ/QtppwzG1\n0OpaERERyXfaaUNEREREcqWAT0RERMTJKeATERERcXIK+EREREScnAI+ERERESfndAGfh0ctwBTq\nYauDiIiISNHgdK9lEREREXFWei2LiIiIiORKAZ+IiIiIk8u3rdWmTJmS9dnf3z9rz9iC5unpy7Fj\nPxfKvS/z8KjF0aOHCrUOIiIicufbsGEDGzZsuO1ynG4On7ZWExEREWelOXwiIiIikqt8G9KVvK1d\nu5Z9+/bh4uLC8OHDKV26dJ5pP/roI8qUKUN6ejonT55k0qRJN3SPvXv3MnHiRJYvX55f1RYREREn\noR4+Bzt58iRz5szh73//O8nJycTHx+eZdtOmTXh4eDB06FAef/xxEhMTb/g+d911Fx9//HGe15cs\nWXJT9RYRERHnoYDPwRYuXEjbtm0BmDRpEi1btswzbUpKCjt27ADAxcWFgQMH3vB9EhISiIuLy/Va\neno6M2bMuIlai4iIiDPRog3H1CJr0caYMWMoW7Ys/v7+/PDDD0yYMCHPXOfOnaN169a4urry0EMP\nMX78eMqUKUN0dDR79uyhdOnSlC1blp49exIREUFsbCyurq7Ur1+f+vXr079/f7Zv305ycjLvvfce\n9erVo0KFCnh5eTFs2DDGjx9Phw4dOHjwIEOGDGHVqlUUL16cMWPGsGDBAnx8fAqqgUREROQW3Oqi\nDc3hc7DMzEwqVKhAt27d2L17N6tXryYoKCjXtKVKlSI2NpaVK1eyYMEChg8fzoIFC5g6dSqrV68G\noG/fvnTs2JGJEyeyZcsW4uPjSUpKol69eri7uwPw1FNP8corr1CvXj369u3L0qVL8fb2Zvjw4QDU\nrVuXoUOHcvHiRSpWrMjHH3+sYE9ERMSJaUjXwby8vPD29gagUqVK/Pjjj3mmjYyMpESJEvTt25fF\nixdz8uRJjh8/TlpaWlaaMmXKsHv3booXL44xhoYNGxIQEJCjnLi4OBISEli7di2NGjUCyOpxTE9P\nB2Do0KGEhYWxa9cuGjZsmK/PLCIiIkWLAj4HCwgI4NdffwVsCziaNWsGQFJS0lVpDxw4wPbt27O+\n+/n5UalSJTIzM7POnTx5kgYNGnD+/Pms83v27AH+COpat25NkyZN6NKlC6NGjQKgRIkSXLp0iaio\nKAAaNmzIgQMH8vtxRUREpAjSHD7H1CLHi5dffvllqlevzqlTpxg7diypqal069aNyMjIHLkWL17M\nsWPHKFasGOnp6bRv3562bduydetWvv/+e8qUKUO1atXo1q0bUVFRrFu3jmbNmlG3bl1+/fVXhg4d\nypIlS2jatCnvv/8+DRs2pEyZMnTr1o1PPvmEjIwM2rdvT5MmTQD48MMPGTBgAFWqVCnQ1hEREZFb\nc6tz+BTwOaYWRXqnjR07dlCnTh02bNjAgw8+WNjVERERkRukRRtywzZt2kRsbCz33XdfYVdFRERE\nCoB6+BxTiyLdwyciIiJ3Ju2lKyIiIiK5UsAnIiIi4uScLuDz8KgFmEI9bHUQERERKRqcbg6fiIiI\niLPSHD4phdKlAAAgAElEQVQRERERyZUCPhEREREnp4BPRERExMk5XcDn6emLMaZQD09P38JuBhER\nEZEsTrdoQy9eFhEREWelRRsiIiIikivtpetgCQkJfP3114wcORJXV9frpv/oo48oU6YM6enpnDx5\nkkmTJl2VJi0tjU6dOrF161ZHVFlEREScjHr4HOyXX35h7NixVK1aFS8vLx588ME8027atAkPDw+G\nDh3K448/TmJiYq7pypUrx8qVKx1VZREREXEyCvgcLD09nbNnz3Lq1CmWL19OSEhInmlTUlLYsWMH\nAC4uLgwcODDXdImJiWzZssUh9RURERHno0UbjqnFVYs20tLS+PLLL/MM4gDOnTtH69atcXV15aGH\nHuK5556jbNmy/Pjjjyxfvpzq1atjjKFz584EBASwd+9ezp49yyuvvEKLFi04ceIEPXr0YPDgwTzy\nyCNUrVqV1atX88EHH3Dx4kVeeuklmjVrRnR0NO+88w5LliwhNTWVX375hb///e9UrlzZ0Q0jIiIi\nt0GLNoq4kJAQevbsec00pUqVIjY2lhdeeIFdu3YxYsQIAJ588kn+8Y9/0K1bN8qXL4+Pjw8+Pj4A\nvPTSS3Ts2JH+/fuzYcMGvLy8CAgIoGzZsnTv3p1du3YBMGvWLKpXr07//v2pX78+KSkpTJ8+nREj\nRtCjRw8+/vhjxzaAiIiIFBoFfAVk/fr1lClT5pppIiMjKVGiBH379mXx4sWcPHkSgJ9//pny5cvj\n4eFB3759c+SJi4sjMTGRtWvX0rhxY86cOQOAh4cHcLnHE2JjY6lTpw4Ao0aNYu/evQCEh4eTmJiI\nt7d3/j2siIiIFClapVsA9u3bx4ULF3KcS0pKwsvLK8e5AwcOUKZMGVq2bAmAn58fAD4+Ppw+fRo3\nNzf27NlDw4YNs4aMW7duTd26dbn//vtp3bo1pUuXzlHm5XTNmjVj3759dO7cmSNHjuDt7U3ZsmUJ\nDAwE4MiRI/n/4CIiIlIkKOArAOfPn6dGjRpZ31NTU+nXrx+RkZE50pUuXZrIyEi2bNlCeno6gwcP\nBmzDsa+++irt2rXDw8ODLVu2kJCQwNKlS/nXv/7FG2+8QVJSEi4uLrRr146IiAiOHDlCyZIl2b9/\nP5999hmjRo3in//8JwsXLgRgwIABjB49munTp1OlShWaNGmiXj4REREnpUUbjqmFdtoQERGRfKdF\nGyIiIiKSKwV8IiIiIk5OAZ+IiIiIk1PAJyIiIuLknC7g8/CoBZhCPWx1EBERESkanG6VroiIiIiz\n0ipdEREREcmVAj4RERERJ+dUAZ+npy/GmEI/PD19C7spRERERLI41Ry+orHLBminDREREXEEzeET\nERERkVwp4BMRERFxcsULuwLOLjMzk/nz51O6dGmOHTvGqFGjrpv+5ZdfxtXVFW9vbypXrsz27dsZ\nMWIECxcupF+/ftSsWfOW6tK0aVN27NiBi4sLUVFRbN++nZ07dxIdHU1cXBwuLnnH/7t376Z///6s\nXr36lu8vIiIihUM9fA62Zs0amjZtSp8+ffDw8CAuLu6a6SdNmkTlypX55z//ybBhw2jcuDFRUVH4\n+PiQmJjIoUOHbrku33zzTVZQN3PmTMaMGcPYsWNZt27dNYM9gEaNGtG2bdtbvreIiIgUHgV8Dla+\nfHkmT57MmTNnOHLkCLVr184z7ZEjRwgLC+OJJ57IOlenTh06deoEQIUKFW65Hr/99hvbtm3j7Nmz\nAFy6dAmAKlWq5Dh/LVqIIiIicmdSwOdg999/P5UqVaJx48aUK1fumkFbdHQ0NWrUoHjxnCPt2QNA\nsAVeAwYMYO3atUycOBHLskhOTmbu3LmsXbuWN954I+v7mjVreOONN3BxceHNN9/k+PHjxMbGEh0d\nzcyZM0lPT886b1kWkydPZvny5bzxxhsAxMbGEhoaypo1azhw4ED+N5CIiIg4nObwOdjRo0e57777\nuP/++5k8eTKdO3fGx8cn17S218pcrXz58lelCwsLo3Tp0qxdu5Zt27aRnJzMr7/+Ss+ePfH09CQm\nJibru7e3N5UqVaJu3boA3H333Xh7ezN8+HCArPP//e9/qVmzJr179+bpp5/myJEjjBkzhqioKIwx\nzJ49O7+aRURERAqQevgc7H//+x/Dhg3jkUceYc6cOSxcuDDPtPfeey+JiYlcvHgxx/mIiIir0h46\ndIi5c+dy/Phxzp07R9euXUlLS6NDhw5ER0fn+B4VFQVcf0g2Li6OEydOsHbtWnx9fTl06BDFixfP\nMxAVERGRO4MCvgJw/vx5wLZK9nLvXlJS0lXpvLy8eOSRR5g+fXrWuTNnzlwVcG3dupUpU6YwePBg\natWqxalTp1iyZAkTJkwgNjaWffv2sXTp0qzvCQkJV90re/BnWRaWZdG6dWu8vb3p0qULo0ePxs/P\nL8fw8oULF26vIURERKRQaKcNh/hjp41Tp07xv//9D29vb4wxDBo0iNTUVLp160ZkZGSuud9++23O\nnTuHr68v5cqVo1evXiQmJvK3v/2Nu+66i6effpo33niDhx9+mD179nDgwAHuvfdeTp8+jZeXF8YY\nLl68yPHjx/H09MQYg5+fHwMGDGDo0KHcd9999O/fn2eeeYYuXbpknX/mmWd48803qVWrFpcuXWLg\nwIFs3bqV6OhoGjVqxCuvvMJf/vIXpkyZUoBtKSIiIpfd6k4bCvgcQluriYiISP7T1moiIiIikisF\nfCIiIiJOTgGfiIiIiJNTwCciIiLi5Jwq4PPwqAWYQj9s9RAREREpGpxqla6IiIiIM9MqXRERERHJ\nlQI+ERERESfnVAGfp6cvxphCPzw9fQu7KURERESyONUcPu20ISIiIs5Mc/hEREREJFcK+ERERESc\nnAI+B8vMzOTVV19l/vz5/O9//7tm2piYGDp06ECvXr1Yv379NdPOnDmTadOm5WdVRURExEkVL+wK\nOLv58+dTs2ZNgoODmTBhAr/88gs1atTINW2bNm0ICAigdu3aBAQEXLPc4OBgMjMzHVFlERERcTLq\n4XOwyMhIqlevDkCtWrX49ttv86XcnTt3Eh8fny9liYiIiHNTwOdg5cuX5+LFiwBYlsWvv/56S+VM\nnz6d+fPn8+KLL3Lu3DkOHz7Mhx9+CMDChQsJDw/n6aefJjMzk+DgYKZOncrhw4dp27YtmzZtIjo6\nGj8/P5YuXcrDDz+MZVlMnjyZ5cuX88Ybb+Tb84qIiEjRo4DPwf7617+yb98+wNYr5+Jyc01+6dIl\ndu/eTVRUFMHBwTRv3pzPP/+cNm3aZKVZuHAh9erVY+TIkbi4uPDEE08AULNmTbp37w7Avffei7e3\nN61atSI0NJT//ve/1KxZk969e/Prr7+SlJSUT08sIiIiRY3m8DlY06ZNOXHiBKtXr8bHx4cmTZrc\nVP5Vq1aRnp7OpUuXCA8P58KFC1SsWDFHmpEjRzJ69GiMMaxYsSLHtSvfB+jr6wtAXFwctWvXZu3a\ntdSuXZtz587d/MOJiIjIHUE9fA4WHh7OwYMHCQoK4sSJE3Tq1AnghnvU9u3bR8uWLSlXrhyBgYEM\nGjSIFi1aAH8Ec4mJiaxZs4bBgweza9cuihcvnnXtxIkTWWVlD/5at26Nt7c3Xbp0YdSoUVcFkSIi\nIuI8FPA5WP369fn999+ZPn06Dz/8MMWLFyc1NZV+/fpdlTYmJoaIiAjWrl3LJ598wpNPPsmWLVu4\n66678Pf3Z9asWSxatAjLspg9ezbbt2/n0KFDbNy4kZUrV5KamkqjRo1o3rw5qamprFy5krNnzzJn\nzhyioqLYv38/b731FgCPPvooR48eZeHChSxbtowKFSoUdNOIiIhIAdHWag6hrdVEREQk/2lrNRER\nERHJlQI+ERERESengE9ERETEySngExEREXFyThXweXjUAkyhH7Z6iIiIiBQNTrVKV0RERMSZaZWu\niIiIiORKAZ+IiIiIk3OqgM/T0xdjTKEfnp6+hd0UIiIiIlmcag6fdtoQERERZ6Y5fCIiIiKSKwV8\nIiIiIk5OAV8BsCyLcePG5Tj38ssvs2LFCl599dVr5v3888+pXLkykyZNwrIsnn32WapUqcK6desA\nmDBhAj169CAlJeW69Vi3bh2hoaHXTDNz5kymTZt23bJuR1paGm3btnXoPUREROQPmsPnEH/M4UtJ\nSSEsLIy5c+fy3XffAfDNN98QFRXFiy++yEsvvUSnTp1o3759nqU99thjBAYG0q9fPzIzM6lUqRJH\njx6lVKlSbNu2jZo1a+Lh4XHdWmVkZNC+fXu2bt2aZ5qzZ8+SmZlJ2bJlb/KZb05ycjLVqlVz6D1E\nREScjebwFVHu7u6MHTsWNze3rHORkZG0bNkSgJYtW7J+/fprltG1a1e++uorAH744QeaNWvGN998\nA0BSUtINBXsArq6u1w3kdu7cSXx8/A2Vd6sSExPZsmWLQ+8hIiIif1DAV0Cy94AmJydnBV7lypXj\n6NGj18zbuXPnrKDw4MGDjB49OisAtPVq2kyfPp358+fz4osvcv78eeLj43nllVd47733iI6OzlFm\nUFAQzz777FX3Onz4MB9++CGWZTFixAiee+45Vq9ezYgRI0hPT8+RNjU1ld69ezNx4kSGDh3K/v37\nWbJkCZ988glTpkzht99+Y/Lkydx9992kpqby+uuvM3r0aIwxPPfcc4CtR3HSpEksXryY6dOn8/33\n31OrVi1WrVpFcHAwo0aNYv/+/bRt25Y9e/bcaHOLiIhINgr4CkFmZibFihUD4NKlS1mf81K+fHnq\n1KnDjh07cHFxoXPnzqxdu5aTJ09mDYvu3r2bqKgogoODad68OcuWLWPkyJGMGzeOMWPGEBISAtgC\nz+joaCZNmsTbb7991b3atGkD2ALJIUOGkJ6eTlBQEJ6enuzYsSNH2goVKtCrVy9KlSrFu+++S6VK\nlZg+fTojRoygR48ezJgxg6lTp1K5cmUqVKhA06ZNCQ0NxcfHBx8fHwBeeuklOnbsSP/+/dmwYQMt\nWrRg+PDhuLu7M3ToUMqWLUu9evWYMmUKDRs2vL2GFxER+ZNSwFdAsvfEeXh4cObMGQBOnz5N1apV\nr5s/KCiIBQsWULFiRSpXrkyVKlWYPXt2VoC2c+dOLl26RHh4OBcuXMDNzY2ffvqJzZs3s27dOpo3\nbw7A77//zpw5c/j5558BmD9/Pr1796ZPnz4kJydfdd/Lw8UlSpQgIyODd999Nyv9Zb6+vlSsWJF9\n+/YBEB4eTmJiIl5eXgC0a9eOqKioXN9NGBcXR2JiImvXrqVx48akp6fz0EMPsWzZMi5cuMCRI0f4\n/vvvadWq1fUbWURERHKlgK+AZA922rdvz86dOwGIiYmhXbt2gG0+Xl6CgoL45JNP+L//+7+s7z/+\n+GNWINmiRQvKlStHYGAggwYNokWLFjRs2BB/f3+6dOnCsGHDAKhatSqhoaF8+umnnD59muDgYJYv\nX86yZcuyeguz1/XKIO3pp5/OSn/Z5Tr4+flRtmxZAgMD6dWrF507dwYgODiY1157DV9f36vKbd26\nNXXr1qVLly6MGjWK0qVL06JFC77//ntKlChB8+bNWb16tRZ4iIiI3AYFfA525swZQkJCiI+PJyQk\nhPT0dAICAjh+/DhLlizBGENgYCCpqan069cvz3KaNGlCcHAwrq6uAHTv3p2AgICs635+fvj7+zNr\n1iwWLVoEwIwZM5g2bRrLli0jISGBNWvW8MMPPxAREUGDBg0YOHDgVfMHZ8+ezfbt2/npp5+YO3cu\nGzZs4LvvviMiIoK5c+fmCABPnz7NihUrWL58OQcPHqRixYqMHj2a6dOns2TJEk6fPg1Ao0aNcHd3\np0mTJgBs2bKFhIQEli5dyr/+9S82btzIokWL2LBhA8WLFwegbdu2tGvXjoceeohy5crlwy8hIiLy\n56XXsjiEtlYTERGR/KfXsoiIiIhIrhTwiYiIiDg5BXwiIiIiTk4Bn4iIiIiTc6qAz8OjFmAK/bDV\nQ0RERKRocKpVuiIiIiLOTKt0RURERCRXCvhEREREnFzx/CpoypQpWZ/9/f3x9/fPr6JviKenL8eO\n/Vyg98yLh0ctjh49VNjVEBERkTvchg0b2LBhw22X4zRz+IrOLhugnTZERETEETSHT0RERERypYBP\nRERExMkp4CsAlmUxbty4657Ly0cffcTs2bP56KOP+Pe//33D9501axYrV668qbqKiIiI81HA52Ap\nKSmEhISwadOma57Ly6ZNm/Dw8GDo0KE8/vjjJCYmAraAcdmyZdfMe/fdd7N8+fLbewARERG54yng\nczB3d3fGjh2Lm5vbNc/lJSUlhR07dgDg4uLCwIEDAVvvXVpa2jXzVqxY8TZqLiIiIs5CAV8R16VL\nF5YuXUrLli2ZMmUK99xzDydOnCAyMpJNmzaxevVqkpKSCAoKyhoi/stf/sIXX3yRo5xNmzYREhLC\na6+9RkJCQmE8ioiIiBQSBXxFXKlSpYiNjeWFF15g165dDB8+nMqVK/PAAw/wwAMPEBQUhJeXF//5\nz384d+4cAP/4xz946KGHssrIzMzk2Wef5R//+AcjR45k2rRphfU4IiIiUggU8BVxkZGRlChRgr59\n+7J48WJOnjyZ4/rZs2cB8PPz4+DBg8THx1O7du0caZKTk0lPTyc8PJzvv/+eu+66q8DqLyIiIoVP\nAV8Bye1FzFeeS0pKuirNgQMH2L59e9Z3Pz8/AFxdXcnIyCAqKirrWs+ePXn77bdp3LhxjntUq1aN\nKlWqEBgYSJcuXRgwYMBtP4+IiIjcORTwOdiZM2cICQkhPj6ekJAQ0tPTcz2XmppKv379rspfunRp\nIiMj+eijj5g2bRqDBw8GoEOHDsTGxpKRkZGVduDAgTRr1izr+5w5c9i+fTv79u1j2rRp/Oc//2HZ\nsmUcOXLE8Q8uIiIiRYa2VnOIgt1a7cyZMyQkJHDhwgVq165N1apVC+zeIiIiUnC0tdqf2IkTJ5g1\naxanTp1SsCciIiJXUQ+fQxRsD5+IiIj8OaiHT0RERERy5TQBn4dHLcAUicNWFxEREZGiwWmGdEVE\nREScnYZ0RURERCRXCvhEREREnJwCPhEREREn5zQBn6enL8aYInF4evoWdnOIiIiIZHGaRRt6D5+I\niIg4Oy3aEBEREZFcKeArAJZlMW7cuKzvGRkZfPjhh0ybNo0XX3zxmnk///xzKleuzKRJk7Asi2ef\nfZYqVaqwbt06ACZMmECPHj1ISUm5bj3WrVtHaGjo7T0MEB0djbe3922XIyIiIgVDQ7oO8ceQbkpK\nCmFhYcydO5fvvvsOgPnz59O1a1fc3d3p378/zz33HG3atMmztMcee4zAwED69etHZmYmlSpV4ujR\no5QqVYpt27ZRs2ZNPDw8rlurjIwM2rdvz9atW2/7CQMCAli/fv1tlyMiIiI3TkO6RZS7uztjx47F\nzc0t69zevXtZuHAhAHXq1CExMfGaZXTt2pWvvvoKgB9++IFmzZrxzTffAJCUlHRDwR6Aq6srZcuW\nvZXHuEphB/giIiJy44oXdgX+jJ5//nkyMzMB2LFjB0899dQ103fu3DlrSPjgwYOMHj2ar776iu7d\nu9t7Nm2mT59OhQoV2L17Ny+88AI//fQTS5Yswc3NjXvuuYd77703K21QUBCNGzfm7bffzpFv0qRJ\nvPrqqxw+fJiHH36YVatW8fzzz1O9enXWrFlDcnIy1apV4+jRow5oGREREXEE9fAVgpIlS1K6dGk2\nbtxIQEAAPj4+10xfvnx56tSpw44dO3BxcaFz586sXbuWkydPUq1aNQB2795NVFQUwcHBNG/enGXL\nljFy5EjGjRvHmDFjCAkJAWw9c9HR0UyaNIm33377qnyff/45I0aM4Pjx43Tr1o0WLVrw7bffcurU\nKd5++22GDBlC165dqVixosPbSURERPKHAr5CcvLkSSIjIxk/fvwNpQ8KCmLBggVUrFiRypUrU6VK\nFWbPnp0192/nzp1cunSJ8PBwLly4gJubGz/99BObN29m3bp1NG/eHIDff/+dOXPm8PPPP+eZD8ga\nJi5RogQZGRnEx8fj5eWVVZ/sPYsiIiJStGlIt4BcOedt3rx5TJw4kYsXL7Jx40Y6depEUlJSjqAq\nu6CgIAICApg6dWrW9x9//DEr8GrRogUREREEBgYC8Ouvv9KwYUP8/f0pUaIETZs2BaBq1aqEhoYS\nFBTEgw8+eFW+I0eOkJGRcVV9GzRoQFpaWtb38+fP50OriIiISEFQD5+DnTlzhpCQEOLj4wkJCSE9\nPZ3p06fz4osv4uHhgaenJ56enqSmptKvX788y2nSpAnBwcG4uroC0L17dwICArKu+/n54e/vz6xZ\ns1i0aBEAM2bMYNq0aSxbtoyEhATWrFnDDz/8QEREBA0aNGDgwIFUrFgxRz7LspgzZw6xsbFs27aN\nFStWsGLFClxdXRk2bBgff/wxX375JefOnWPmzJmObTwRERHJF3oti0Nopw0RERHJf3oti4iIiIjk\nSgGfiIiIiJNTwCciIiLi5BTwiYiIiDg5pwn4PDxqAaZIHLa6iIiIiBQNTrNKV0RERMTZaZWuiIiI\niORKAZ+IiIiIk3OagM/T0xdjTJE4PD19C7s5RERERLI4zRw+7bQhIiIizk5z+EREREQkVwr4RERE\nRJycAr4CYFkW48aNy/p+4cIFwsLCWLp0KcOHDyc9PT3PvDExMXTv3p1HHnkkx/m4uDjc3Nz45JNP\nbqounTp1Iikp6eYeQERERO5oCvgcLCUlhZCQEDZt2pR1btu2baxfv56+ffty+vRp1q9fn2f+Nm3a\nMGjQIA4fPszFixezzp89e5YqVaowYsSIm6rP/Pnz8fLyuvkHERERkTuWAj4Hc3d3Z+zYsbi5uWWd\nu++++3j//fcBOHHiBPfcc891y+nUqRNff/01YOsxdHG5+Z8uLS2N2NhYfvvtt5vOKyIiIncuBXyF\n5NKlS4SGhjJkyBA8PDyumdYYQ58+fVi6dClgG85t3rx5jjShoaGsWLGCd955h6NHj7JixQp8fHzY\ntWsXUVFRdOzYkVOnThEWFsbu3bsBeP3111m6dCmTJ08GYOHChYSHh/P0009rlbGIiIgTUcBXSCpV\nqsTo0aP56quv2Lx583XT+/n5sW/fPjIzMzl79iylSpXKurZx40bS0tLo2bMnwcHBjB8/np49e9Kt\nWzfKlStHpUqVmDFjBtWrV6dhw4YArFmzhrS0NPr27YubmxvR0dEsWrSIevXqMXLkSPtrbkRERMQZ\nKOArZHfddRfz58+/obT+/v65zveLiYmhSpUqAFSrVo3vvvsOgAEDBjB37lz2799P3bp1c+SJi4sj\nPT2d8PBw3NzcKFasGI8++iijR4/mueeeIyMj4zafTERERIoKBXwFJPsQ6euvv85LL70EwLFjx/Dz\n8wPIc/Xs5by9e/dmypQpWcO5l883adKEX375BYDExESaNWsGQEBAAN98881VvXWWZdG6dWvc3d0J\nDAzkscceo3r16iQmJrJmzRoGDRrErl278uvRRUREpJAp4HOwM2fOEBISQnx8PCEhIaSnpzNw4EAa\nNGjArFmzKF26NE8++SSpqan069fvqvxbt27lgw8+4L///S8tW7bk3nvvpWzZsrz11lucOHGCTz75\nhKCgIKpWrcrChQuZP38+oaGhALi4uNC6deusRSGJiYmsX7+eJUuW8Je//IWqVasyd+5c5s2bR7ly\n5di4cSMrV67k9OnTNGrUqEDbSURERBxHW6s5hLZWExERkfynrdVEREREJFcK+EREREScnAI+ERER\nESengE9ERETEyTlNwOfhUQswReKw1UVERESkaHCaVboiIiIizk6rdEVEREQkVwr4RERERJyc0wR8\nnp6+GGOKxOHp6VvYzSEiIiKSxWnm8GmnDREREXF2msMnIiIiIrlSwCciIiLi5BTwFQDLshg3btxV\n50+dOsWECROumTcmJobu3bvzyCOP5DgfFxeHm5sbn3zySb7WFWDmzJlMmzYt38sVERGRwlG8sCvg\n7FJSUggLC2PTpk1XXZs3bx7Hjx+/Zv42bdowaNAgZs6cycWLFyle3PaTnT17lipVqjBixIh8r3Nw\ncDCZmZn5Xq6IiIgUDvXwOZi7uztjx47Fzc0tx/mEhAR8fX1vuJxOnTrx9ddfA7YeQxcXx/10O3fu\nJP7/27vzuKzK/P/jr0tFxQXBMHBHy3KtLLX6ZYPhpKmVZpilDZlOM07bd9SpXNKxZZomx4nGJadS\nkZQ0MUtTBBcEV8BwS0XcE8Ut9XbBBeT8/rhv70BRUbm54fb9fDzOg3Ofc67rfM7FLX26znXOlZbm\nsvpFRESkeCnhc5NNmzbRrFmzQh1rjKF79+7MmjULsN/Ovffee537LcuiZ8+exMbGMnjwYOcTwoMH\nD2bevHn83//9HyNGjCA7O/uy4xISEhgwYABz5szh7rvvJiMjg19++YVx48ZhWRb9+vXjrbfeIiYm\nhn79+pGVlQXAxx9/zHfffceIESOKuGVERESkqCnhc4OVK1fyyCOPABT69S2NGzcmPT2d3Nxczpw5\nQ8WKFZ1ljTFERETQsWNHzp8/T0pKCsePH2f37t106dKFgwcPMnjwYLy8vC47rkKFCowePZrDhw/T\nr18/6tSpQ5s2bZz1hoWFkZWVRadOnQgMDGT9+vUsWLCAU6dO0b17d3x8fFi9erVrGkpERESKhMbw\nucHWrVvZvn07hw8fZseOHaxevZqHHnromuXatWvHkiVLqFSpEnDx3YN2u3fvJjU1lcOHD3P27Fl8\nfX258847iYmJ4Y9//KOzzKXH/e53v+PgwYNERUURFxfHyZMnLztvQEAAAOXLlyc7O5t169aRlZVF\nXFwcPj4+Lr29LCIiIjdPCV8xyduTd/GJ2z179vDzzz87k73MzExq1qx5xbLPPPMMb7zxBrGxsfm2\nJ3FGQLYAACAASURBVCUl8Z///IcZM2awZcsWbDYbv/76Kw0aNKBTp07Oego6zmazMXDgQD755BMO\nHjzI3r17CQwMzBfvpeutWrVi1apVdOjQwRm3iIiIlFzqmnGx06dPEx4eTlpaGuHh4c4xcGfPnmXM\nmDGkpKSQmJiIzWYjNDT0svJJSUmMHTuW//3vf7Rs2ZKHH36YypUrM2rUKH799VcmTpyIr68v3t7e\nzJ8/H19fX2JiYvDy8mLmzJl07dqVfv36sXLlynzH+fn5ERMTQ0JCAnv27GHXrl0MHDgQf39/pkyZ\nwtq1a9m1axfTpk1j6dKlrFmzhvj4eKKiomjfvj01atQgKiqKqKgoqlSpUtzNKiIiItdBU6u5hPun\nVps6dSqtWrWicePGHDx4kH79+vHjjz+6NSYRERG5OTc6tZpu6XqoNm3asHz5cnbu3MnJkyfp1q2b\nu0MSERERN1EPn0u4v4dPREREPM+N9vBpDJ+IiIiIh1PCJyIiIuLhPCbhCwioD5gSsdhjERERESkZ\nPGYMn4iIiIin0xg+ERERESmQEj4RERERD+cRCV9gYBDGmBKzBAYGubtJRERERJw8YgxfyXoHH+g9\nfCIiIuIKGsMnIiIiIgVSwiciIiLi4ZTwFQPLshg4cGC+bQMHDiQ7O5vx48dz6NChq5bPzc3lvffe\n46OPPiIiIoK5c+fy/vvvX/H4RYsWMX78+CKJ/aItW7YwatQoTpw4UaT1ioiIiOsp4XOxY8eOER4e\nTmJiYr7tU6ZMoUGDBnh5eXH77bdftY5hw4Zx2223MXToUPr06UOzZs1YuXLlFY8PDg5mypQpzs/R\n0dE3dxFAkyZNWLVqFcePH7/pukRERKR4KeFzMT8/PwYMGICPj0++7WPGjCEjI4NXXnnlquX3799P\nREQE/fv3d25r2LAh7du3v2IZLy8vKleuDMDGjRuJj4+/iSv4TbVq1YqkHhERESleSvjcZOfOnSxY\nsIDRo0df9bhVq1ZRt25dypUrl297//792b17Nw8++CCJiYksW7aswJ7CRYsWsWHDBiZNmgTA22+/\nTd++fbHZbDzxxBNERkayefNmWrRowddff81TTz2FZVn07NmT2NhYBg8erCeORURESjklfG7y7rvv\n8sQTT1CxYkXi4uKueJz9lTOXq1q1KkFBQXTp0gWARx99lObNm192XPfu3WnUqBF9+/YF4LXXXgPs\nvXUvvPACAE2bNqV169bUrVuXyZMnY4whIiKCjh07cv78eVJSUm7qWkVERMS9lPC5QUREBBMnTgTA\n29ubDRs2XPHYhx9+mIyMDHJycvJtv3ibNm/v29V64i5cuEB2dvYV91uWRVBQEP7+/gDs3r2badOm\ncfjwYc6ePXvtixIREZESSwlfMcmbjPn7+zt75nbv3s39998PQGZm5mXlatasycsvv8yECROc206f\nPu3s+StXrpyz7l9//fWy83l5eZGdnc2uXbs4cODAFY8HKFPG/nVISkpi5MiR9O7dm/r162Oz2bDZ\nbDfXACIiIuI2Svhc7PTp04SHh5OWlkZ4eDhZWVl06dKF6dOnExERQZ06dQgJCcFmsxEaGlpgHf/4\nxz84e/YsH374IVOnTmXhwoW0a9cOgM6dO5OYmMgPP/yAr68vkyZNYsGCBaSlpREbG0utWrXw9vYm\nKSmJunXrUqtWLXx9ffnxxx85dOgQ33//PatWrSI1NZUxY8Zw7tw5fH198fb2Zv78+fj6+hITE8Oe\nPXtYu3YtkZGRxdh6IiIiUhQ0tZpLaGo1ERERKXqaWk1ERERECqSET0RERMTDKeETERER8XBK+ERE\nREQ8nEckfAEB9QFTYhZ7PCIiIiIlg0c8pSsiIiJyK9BTuiIiIiJSICV8IiIiIh6uXFFVNHLkSOd6\nu3btnDNBFIfAwCAOHtxTbOe7loCA+hw4sNvdYYiIiEgpt3TpUpYuXXrT9XjEGD7NtCEiIiK3Ao3h\nExEREZECKeErBpZlMXDgwHzbYmNjGTNmDOPGjePMmTNXLJucnExwcDC9e/dm0qRJjB49mhYtWrBu\n3bp8x40YMYJXXnnFuR4dHX1DsZ46dYoHH3zwivsXLVrE+PHjb6huERERcQ8lfC527NgxwsPDSUxM\ndG47evQokZGRvPHGGxw6dIi0tLQrlm/Tpg0hISF07NiRvn37MmjQIKZNm8aRI0fyHdevXz8uXLgA\nwKBBg+jatSsAR44cISEhodDxVqlShblz515xf3BwMFOmTCl0fSIiIuJ+SvhczM/PjwEDBuDj4+Pc\nNmPGDGcv2rBhw2jZsuV11XnPPfeQlZV1xf2pqals376d3Nxcxo4d6xjjWDgZGRmsXr36ivu9vLyo\nXLnydcUrIiIi7qWEr5jkfYjj559/JiMjg/nz5/Ppp59eVz1RUVEAtGrVCoBRo0YRExPD/Pnznces\nWbOGmTNnsnfvXtauXcu8efNYvnw5AFOnTmXWrFmMHTuWtLQ0Vq1aRePGjZk1axbPPfccxhjeeust\nANLT0/nrX/9KZGQkEyZMuKnrFxEREfcpsteySOHl5uZSrVo1OnfuzObNm4mJiaFTp05XLRMfH8/+\n/ftJTU2lV69e1KpVi6+++opq1arRqVMn0tLSSElJAey3gRMSEqhfvz73338/jz32GG3btmXHjh0k\nJSUxZswYcnJyeOqpp4iJiaFWrVo88MADBAcH4+/vT+3atQG46667+Ne//kWFChV4+OGH6d+/v8vb\nRkRERIqeeviKSd7bqjVr1qRWrVoAVK9enZ9//vma5R977DEGDx5Mt27dANi5cydr1qzhjjvuuKz+\ngmRlZZGamoqfnx8A5cqVY/v27YC99zEoKAh/f//Lyi1btozvvvvuqg+WiIiISMmmhK+Y5L2lGxIS\nwr59+wD7Axz33HMPAJmZmdesp1evXliWRWJiIi1btuTQoUMAnD9/Pt85Lq57eXmRnZ3NypUradq0\nqfO8586dcyadV4r1k08+IS0tje7du1OlShX27t172bWIiIhIyaeEz8VOnz5NeHg4aWlphIeHk5WV\nRdu2bbEsi8mTJ1O2bFk6duyIzWYjNDT0svLJycnEx8cTFxfHpEmT+Pzzz3nmmWfw8vLiz3/+M9u3\nb2fOnDksWrSItWvXsmXLFqZNm0ZCQgJHjx6lc+fOxMXF4eXlRbNmzWjXrh1RUVGMHTuWyZMns3r1\narZv386oUaMAWL16Ndu2bWPWrFnceeed7NmzhwULFtCsWTPmzp3LggULSEtLIzY2tribUkRERG6Q\nZtpwCc20ISIiIkVPM22IiIiISIGU8ImIiIh4OCV8IiIiIh5OCZ+IiIiIh1PCJyIiIuLhPCLhCwio\nD5gSs9jjERERESkZPOK1LCIiIiK3Ar2WRUREREQKpIRPRERExMN5RMIXGBiEMabELIGBQe5uEhER\nEREnjxjDp6nVRERE5FagMXwiIiIiUiAlfMXAsiwGDhzo/Lxt2zbGjx9Pdnb2NcsmJycTHBxM7969\nmTRpEh9++CGjR4++rvPPmjWLqVOnXnfcIiIi4hmU8LnYsWPHCA8PJzEx0blt7969DBgwgBo1alCz\nZk2eeuqpK5Zv06YNISEhdOzYkb59+/Luu+8ya9Ys1q1bV+gYfv/73zNx4sSbug4REREpvcq5OwBP\n5+fnx4ABA5g7d65zW1ZWFmfOnKFMmTKsXr2aGjVqXFedderUYefOndx3332FOr5atWqOcY4iIiJy\nK1LC5wZPPvkkAKdOnWL37t089NBDhS578uRJ0tLSaNeuHQDjx4+nTp06bN++nd69e1OjRg3Cw8Np\n1KgRW7Zs4fXXX6dSpUrO8vfddx8vvvgiYWFhLFy4EH9/f9atW8c777xTpNcoIiIiJYcSPjcKDw/P\nN7bvajZu3EhsbCyLFy/mb3/7G9WrVychIYFTp07x9NNPk5mZyVtvvUX79u2pU6cOTz31FP7+/nz8\n8ce8//77WJbFggUL+Prrr2nRogU//vgj+/bt4+mnn6ZmzZouvlIRERFxJ43hc6MlS5bk6327mhYt\nWtCxY0c++eQTkpKSiIqKIjk5GX9/fwBuv/12UlJSSElJybdtzZo1AGRkZBAREYHNZgPgiSee4NSp\nUwQHB7Ny5UoXXJ2IiIiUFEr4isml7+VLT0/n/Pnz+bZlZmYWqq46deqwdu1aWrRowd69ewF7Qnfv\nvffSvHnzfNvuueceAO68806++uorPvzwQy5cuEB0dDTvvPMOqampbNu27WYvT0REREow3dJ1sdOn\nT/Pll1+SlpZGeHg4f/rTn6hUqRLnzp2jbt26zuNsNhuhoaGsWLEiX/nk5GTi4+PZtm0bOTk5HD9+\nnM2bNxMeHs5tt93Gzp07mTFjBrt27WL8+PH4+fkxevRovv32W3bu3Ml7773H119/TVpaGlu3bqV8\n+fL06dOHDh06EBERQc2aNWnbtm1xN4uIiIgUI8204RKaaUNERESKnmbaEBEREZECKeETERER8XBK\n+EREREQ8nBI+EREREQ/nEQlfQEB9wJSYxR6PiIiISMngEU/pioiIiNwK9JSuiIiIiBRICZ+IiIiI\nh1PCJyIiIuLhSn3CFxgYhDGmRC2BgUHubhYRERERp1L/0EbJm1YNNLWaiIiIuIIe2hARERGRAinh\nKwaWZTFw4EDn59zcXKZNm8Z3333H559/ftWyKSkpPPPMM3Tq1Mm57d1336Vjx45s27atUOcfMWIE\n0dHRNxZ8HosWLWL8+PE3XY+IiIgULyV8Lnbs2DHCw8NJTEx0bluwYAEtWrSge/fuBAQEsG7duiuW\nb926NaGhoVSuXJkff/wRgA8//JCXXnqJRo0aXbFc3gRv0KBBdO3a9aavJTg4mClTptx0PSIiIlK8\nlPC5mJ+fHwMGDMDHx8e5rWrVqowYMYLTp0+zf/9+GjRocNU6jDGMHj2aoUOHkpOTA3DVMYKLFi1i\n8+bNzs+pqals3779Jq8EvLy8qFy58k3XIyIiIsVLCZ8bPProo1SvXp1mzZpRpUoVqlWrds0y9evX\np2vXrnz66afAxYdVYPr06Xz22Wf84x//YNWqVZw/f564uDhSU1OZOXMmAGvWrHGuz5s3j6ioKCZN\nmsSyZcvIyMggODiYiIgI5s2bx+uvvw5Aeno6f/3rX4mMjGTChAmuaAYREREpJuXcHcCt6MCBAzzy\nyCM8+uijjBgxgscff5zatWtf8fiLvXlDhgyhdevWvPTSS85t3bt3x8vLi5MnT9KzZ09iYmLo0qUL\nVapUoUePHgC0adOGhIQETp48SWRkJDNmzACgQ4cOxMXFERISQuXKlenSpQujRo0C4K677uJf//oX\nFSpU4OGHH6Z///6ubBIRERFxISV8bvDll18ydOhQypYtS4MGDZgxY0a+hzoudbE3r1KlSgwbNowh\nQ4bQvn175/6ZM2dSpUoVzp07l6/cmTNn8Pb2dn5OS0ujQoUK+fYfOnQIgICAAADKlPmt03fZsmWc\nOHGCM2fO3MTVioiIiLvplm4xuXTM3cXkrEWLFs7evczMzGuW7dWrF+np6c7P3bp1o2nTpnTo0AHL\nsti7dy9eXl5kZ2ezevXqfHXccccdHDlyxLktNzeXGjVqFHiuTz75hLS0NLp3706VKlXYu3dvgdch\nIiIiJZ8SPhc7ffo04eHhpKWlER4eTlZWFm+88Qbjxo1j2rRpxMbG0rNnT2w2G6GhoZeVT0pKYuzY\nsfzvf/9zbgsPD6ds2bIA3HfffSxbtoyYmBhq1qzJli1buO+++zhw4ADHjh1zvgImISEBgFdffZUp\nU6Ywfvx4Pv30U/bt20d8fDzTpk0jKSmJ7du3M3XqVO6880727NnDggULaNasGXPnzmXBggWkpaUR\nGxtbPI0nIiIiRUIzbbiEZtoQERGRoqeZNkRERESkQEr4RERERDycEj4RERERD6eET0RERMTDlfqE\nLyCgPmBK1GKPSURERKRkKPVP6YqIiIjcKvSUroiIiIgUSAmfiIiIiIcr9QlfYGAQxpgStQQGBrm7\nWUREREScSv0YPs20ISIiIrcKjeETERERkQIp4RMRERHxcEr4ioFlWQwcOND5OTc3l48++ohvvvmG\nL7/88qplk5OTCQ4OJiwsjIkTJzJ8+HCmT59+XedfvHgxb7755g3FnteWLVsYNWoUJ06cuOm6RERE\npPiUc3cAnu7YsWNERESQmJjo3PbNN99Qr149XnjhBd555x327t1L3bp1Cyzfpk0bQkJCaNCgAWFh\nYViWhb+/Pw888ACNGjUqVAzt2rXj/vvvv+lradKkCcOGDaNnz574+PjcdH0iIiJSPNTD52J+fn4M\nGDAgX4K0YsUK6tSpA0D9+vVZtmxZoeuz2WxUqVIFPz+/QpfZtGkTmzZtKnzQV1GtWrUiqUdERESK\nj3r4iknep3arVq1KTk6Oc/u+ffuuWX7jxo3Mnj2byMhIli9fjr+/PwDR0dHYbDb27t3L66+/zpEj\nR5g5cyZ16tQhMzOTqlWr8tBDDzF8+HDi4+NJT09n4cKF3H777Zw5c4YXX3yRV155herVqxMSEkJ0\ndDRjxozB29ub559/nr59+xIfH88///lPxxPRIiIiUtqoh6+Y5E2WXnzxRdLT0wHYsGEDZcpc+9fQ\nokULnnnmGTp06MC3334L2G8XT5gwgX79+vHkk0/y5ZdfsnDhQh544AF69OjBxo0beeONN2jdurXz\n/IMGDaJ///706NGDVatWsWvXLsLCwsjKyqJTp04EBgayfv16jDFERETQsWNHzp8/T0pKigtaRURE\nRIqDEj43aNGiBU2bNiUmJobatWvTvHnzQpdt2rQpsbGxAGzduhWAuLg49u3bR61atbjnnns4cOAA\ncXFxjBo16rLyW7dupWzZsgD4+/vz008/ARAQEABA+fLlyc7OBmD37t1MmzaNw4cPc/bs2Ru/YBER\nEXEr3dItJnlv6cbFxZGRkUHfvn1ZsGAB7du3ByAzM5OaNWtetZ6qVauyc+dOABo2bEjlypXp0KED\nAPv27WPx4sX07t2bChUqFHj+mjVrcu7cOSpUqMD+/ft54YUXOHz48GUvik5KSuI///kPM2bMYMuW\nLdhsNmw22801goiIiLiFevhc7PTp04SHh5OWlkZ4eDhZWVk0atSIkydPMmHCBJ577jnKlSuHzWYj\nNDT0svLJycksWbKE7777jq1bt9KyZUsefvhhpk+fTmZmJq+++ioTJkwgOjqakydP0rx5c4KDg3n2\n2WcZNmwYR48eJTIykh07drBhwwYmTZrEmDFjiIqKIiQkhMaNGzNt2jSWLl1KSkoK8fHxTJs2DR8f\nH7y9vZk/fz6+vr7ExMSwZ88e1q5dS2RkpBtaUkRERG6UplZzCfdNrfbee+8xdOhQAH766SdmzpzJ\n6NGj3RKLiIiIFK0bnVpNCZ9LuC/hmz17NpZlUb58eTIyMnjggQdo3bq1W2IRERGRoqWEr0RxX8In\nIiIinutGEz6N4RMRERHxcEr4RERERDxcqU/4AgLqA6ZELfaYREREREqGUj+GT0RERORWoTF8IiIi\nIlIgJXwiIiIiHq7UJ3yBgUEYY0rUEhgY5O5mEREREXEq9WP49B4+ERERuVVoDJ+IiIiIFEgJn4iI\niIiHK+fuADxddnY2X3zxBWfPnuX48eN88MEHAHzwwQfce++9/PzzzwwdOvSK5ZOTk3nrrbeoW7cu\nISEhHD16lP/+979s3boVb2/v645n8eLF/PDDD/z3v/+94WsSERGR0kUJn4tFR0fTq1cv/Pz86NGj\nB8nJyZw8eRKAp59+mrVr17J8+XLatm1bYPk2bdoQEhJCgwYNCAsLA6BZs2YcOnSI+vWv/wXP7dq1\n4/7777/xCxIREZFSR7d0XWzr1q3MmDEDgIYNG5KRkcGKFSto2bIlAC1btmTJkiXXVWerVq04dOjQ\nDcWzadMmNm3adENlRUREpHRSD5+LDRkyhNzcXADWr1/Pm2++yZIlS6hcuTIAVapU4cCBA4Wub/r0\n6Tz//POUL1+e9PR0Fi5cSEBAAFlZWXTt2pU+ffpw9913k5mZyfDhw1m3bh02m429e/fy+uuvc+7c\nOYYPH058fDzLli1jzZo1lC1bFmMMFStWZO/evfzyyy8899xzzJs3jyFDhlCnTh2XtI2IiIgUD/Xw\nuViFChXw9vYmISGBkJAQateuTW5uLmXLlgXgwoULzvWrWb58OaNGjWLRokUAVKtWjUGDBtG/f39C\nQ0NZtWoVR44coVu3blSsWJHPPvuM6tWrM2HCBPr168eTTz7JV199RevWrR2vsoHZs2fTsWNHQkJC\n2L9/P6+88gr9+vXj8OHDdO7cmfvuu49ly5a5rnFERESkWKiHrxgcPXqUFStWOB/OCAgI4PTp0wCc\nOHGCGjVqXLOOtm3bEhYWxvTp053btm7d6kwW/f39+emnnwAICgrC19eX1atXAxAXF0dWVhY1a9bM\nV2dwcDCpqalUqFCBd955x7k9ICAAgPLly5OdnX2jly0iIiIlhHr4ikFUVBSDBw8mJyeHxYsX8+ij\nj7JhwwbA/hTuQw89BEBmZuY163r++eed6zVr1uTcuXMA7N+/n+bNmwM4e/AaN25M5cqV6dChA926\ndePxxx8HcL4U+sCBA7z44ov06NEDX19fZ715XxqtF0iLiIiUfkr4XGzChAkMHz6cgIAAAgMDCQwM\n5LHHHuPw4cNER0djjKFDhw7YbDZCQ0MvK5+cnEx8fDzff/89cXFx+fZNmjSJMWPGEBUVRUhICHXq\n1GHOnDnMnj2bnTt34uvry6uvvsqECROIjo7mxIkTREZGsmPHDjZs2ECNGjUICQnhueee46OPPuLM\nmTNERkaSmppKSkoKc+bMYe7cuc6nikVERKR00tRqLlE6plYbPnw477//PufPn2fOnDkcOHCAN954\nw91hiYiIyBXc6NRqGsN3C7v77rv54YcfKFeuHPv27ePJJ590d0giIiLiAurhc4nS0cMnIiIipcuN\n9vBpDJ+IiIiIh1PCJyIiIuLhSn3CFxBQHzAlarHHJCIiIlIylPoxfCIiIiK3Co3hExEREZECKeET\nERER8XBF9h6+kSNHOtfbtWtHu3btiqrqqwoMDOLgwT3Fcq7CCgioz4EDu90dhoiIiJRyS5cuZenS\npTddT6kfw6f38ImIiMitQmP4RERERKRASvhEREREPJwSPhfLzs5m3LhxjB49muHDhzu3W5bFwIED\nr1k+OTmZ4OBgXnzxRSZNmsS///1v6tWrx5kzZ5g0aRKjR492Wezt27cnMzOzwH1btmxh1KhRnDhx\nwmXnFxERkaJRZA9tSMGio6Pp1asXfn5+9OjRg+TkZBo1akRERASJiYnXLN+mTRtCQkJo0KABYWFh\nADRr1oxDhw7xwgsvkJub67LYv/nmG26//fYC9zVp0oRhw4bRs2dPfHx8XBaDiIiI3Dz18LnY1q1b\nmTFjBgANGzYkIyMDPz8/BgwYcMOJUqtWrTh48CAbNmwgLS2tKMN1OnXqFKmpqRw5cuSKx1SrVs0l\n5xYREZGipYTPxYYMGcJLL70EwPr163nwwQdvqr7p06dTo0YN7rrrLn755RfGjRuHZVn069ePt956\ni5iYGPr160dWVhaWZdGzZ09iY2MZPHgwlmWxefNmWrRoQUxMDNHR0c7bzCdPnmTo0KHMmjWLYcOG\nYYwhIiKCzZs3F1iPiIiIlB5K+FysQoUKeHt7k5CQQEhICLVr176hepYvX86oUaNYtGgRAL6+vrRp\n0wawP6IdFhZGVlYWnTp1IjAwkPXr1zuTto4dO3L+/HlSUlJo2rQprVu3xt/fn9DQUJYvXw7Axx9/\nTHBwMM8++yy1atWicuXKNGnSxFn/pfWIiIhI6aExfMXg6NGjrFixgqFDh95wHW3btiUsLIzp06df\n8ZiAgAAAypcvT3Z2NgC7d+8mNTWVw4cPc/bsWcD+wMjFY+3vMYTU1FT69OkDwGuvvXZZ3QXVIyIi\nIqWDeviKQVRUFIMHDyYnJ4fFixc7t196a/RKT8Tm9fzzz+f7nLeOS+tLSkpi5MiR9O7dm/r162Oz\n2bDZbAWWv/fee0lPTwfs4w6vtx4REREpuZTwudiECRMYPnw4AQEBBAYGEhgYyOnTpwkPDyctLY3w\n8HCysrKw2WyEhoZeVj45OZn4+Hi+//574uLi8u2bMmUKa9euZdeuXUybNo2lS5eSkpJCfHw806ZN\nw8fHB29vb+bPn4+vry8xMTHs2bOH1NRUpk6dSlxcHFu2bCE2NpaRI0cSHx/PrFmzSE9PJyMjgyVL\nljBz5kz8/PwKrGft2rVERkYWV1OKiIjIDdLUai6hqdVERESk6GlqNREREREpkBI+EREREQ+nhE9E\nRETEwynhExEREfFwpT7hCwioD5gStdhjEhERESkZSv1TuiIiIiK3Cj2lKyIiIiIFUsInIiIi4uFK\nfcIXGBiEMaZELYGBQe5uFhERERGnUj+GTzNtiIiIyK1CY/hEREREpEBK+EREREQ8XDl3B+DpsrOz\n+eKLLzh79izHjx/ngw8+KHDb1Xz++edUqlSJrKwsjh49yrBhw4opehEREfEESvhcLDo6ml69euHn\n50ePHj1ISkpi586d+bYlJyfTpk2bAssnJiYSEBBA9+7dyc3N5bXXXiuSmEJDQ2+6HhERESkddEvX\nxbZu3cqMGTMAaNiwIfv27SM9PT3ftoyMjCuWP3bsGOvXrwegTJkyPP/88zcVz8aNG4mPj7+pOkRE\nRKR00VO6LvHbU7rnzp0jNzcXb29vnnjiCSZOnIi/v/9l22rXrl1gTWfPnqVVq1Z4eXnRtWtX3n77\nbbKzs/nDH/5AvXr1CAkJIT4+nk8++YQKFSoQHh5Oo0aNSEtL49VXX2X69Ol88803dOzYkZycHCpW\nrMisWbPo27cvffv2Ze7cuZQvX56ffvqJF154gQYNGhRnQ4mIiMh1uNGndJXwucTlr2VJSEgg78NO\nLQAAFoNJREFUKSmJt99++6rbCnL+/Hnmzp3L9OnTKVu2LNOnT2fKlClYlkWfPn2YPXs2SUlJNGnS\nBG9vb5577jlWrVpFTEwM77//Pg0aNGDnzp0cP36cEydO8N577zFp0iQAwsLCeOedd6hevTpeXl74\n+/sXfXOIiIhIkdBrWUqwo0ePsmLFinyJXUHbCrJixQrKly/Ps88+y8yZMzl69KhzX5ky9l9fkyZN\nWL9+PSkpKc6E7fbbb2fNmjUA1K9fH2MMfn5+zrIXLlwgOzub1157jXfffZfQ0FDKli1bZNcsIiIi\nJYcSvmIQFRXF4MGDycnJYfHixVfclpmZeVnZHTt2sHbtWufnxo0bO9cvXLgAQFpaGq1ataJ58+bs\n3bsXgIyMDO655x7gYi+onZeXF9nZ2ezatYvMzEw2bNjA7Nmz+fjjj0lISCjiKxcREZGSQAmfi02Y\nMIHhw4cTEBBAYGAggYGBBW6z2WwFPjnr7e3NihUr+Pzzzxk9ejS9e/d27lu1ahVz5sxhxYoVjBgx\ngj//+c8cPnyYb7/9llWrVvHee+/x9ddfk56ezty5cwGoVasW3t7eJCUlUbduXX766Sd++OEHNm3a\nxKOPPlps7SIiIiLFR2P4XML1U6tNmTIFYwxhYWEuPY+IiIiUHBrDdws5ceIE8+bNY968eWRlZbk7\nHBERESnh1MPnEq7v4RMREZFbj3r4RERERKRASvhEREREPFypT/gCAuoDpkQt9phERERESoZSP4ZP\nRERE5FahMXwiIiIiUiAlfCIiIiIertQnfIGBQRhjStQSGBjk7mYRERERcSr1Y/j0Hj4RERG5VWgM\nn4iIiIgUqJy7A/B02dnZfPHFF5w9e5bjx4/zwQcfcP78eaKioqhatSrz5s1j7NixVKpU6Zp1jR8/\nnhYtWvDoo48WQ+QiIiLiKdTD52LR0dH06tWLQYMGkZaWRnJyMikpKSxZsoRnn32WEydOsGTJkkLV\ntXDhQr7//vurHnPkyBESEhKKInQRERHxEEr4XGzr1q3MmDEDgIYNG5KRkcEjjzzCmDFjAPj1119p\n3br1Neux2Wy0bduWOXPmXPGY3Nxcxo4d6xjXKCIiImKnW7ouNmTIEHJzcwFYv349b775JgAXLlxg\n/PjxhIWFERAQcM164uLiePnll5k6dSqbN2+madOm2Gw2+vTpw913301mZibvvvsua9eu5cyZM5Qp\nU4bWrVvzzTffUKtWLZYvX87777/v0msVERGRkkk9fC5WoUIFvL29SUhIICQkhNq1awNQvXp1Xn31\nVebPn8/y5cuvWc/BgwepXr06nTt35ocffgCgWrVqdOvWjYoVK/LZZ59xxx13cP/999OlSxfatm3L\ntm3bWLduHa1atSI0NNSl1ykiIiIllxK+YnD06FFWrFjB22+/fdm+u+++m2+++eaq5XNycti4cSOR\nkZGULVvWmfBdFBQUhK+vL2XK/PbrzMrKonnz5gQFBdG5c2e+/fbborkYERERKXV0S7cYREVFMXjw\nYC5cuMDSpUtJSUnh3Llz/P3vf+fgwYPcc889AGRmZlKzZs3LyickJDBs2DDq1asHQGRkJAcPHnTe\nCs47Zq98+fJkZ2ezcuVKKlSoQOfOnfnrX//KwIEDi+FKRUREpCRSD5+LTZgwgeHDhxMQEEBAQACB\ngYE8//zz3HXXXUyePBlvb29ef/11bDZbgbddV65cyd/+9jdWrVoF2McB+vj48Oabb7JhwwbmzJnD\n7Nmz2blzJwCdOnUiLi4OLy8vjDHMnDmT77//nvvvv79Yr1tERERKDs204RKaaUNERESKnmbaEBER\nEZECKeETERER8XBK+EREREQ8nBI+EREREQ9X6hO+gID6gClRiz0mERERkZKh1D+lKyIiInKr0FO6\nIiIiIlIgJXwiIiIiHk4Jn4iIiIiHK/UJX2BgEMaYErUEBga5u1lEREREnEr9QxuaWk1ERERuFXpo\nQ0REREQKpITPxbKzsxk3bhyjR49m+PDh+fYdP36cd95556rlk5OT6dKlCy+//HK+7evWrcPHx4eJ\nEycWecxXk5uby5gxY1i2bBkAffr0ISUlpVhjEBERkeujhM/FoqOj6dWrF4MGDSItLY3k5GTnvqio\nKA4fPnzV8m3atKFXr1788ssv5OTkOLefOXMGf39/+vXrV2A5y7L47rvviuYi8ihTpgz16tUjPj4e\ngNGjR9O6dWsA1qxZwy+//FLk5xQREZGbo4TPxbZu3cqMGTMAaNiwIRkZGQBs27aNoKCgQtfTvn17\nFi5cCNiTuTJlrv6rmzx5MqdOnbqxoK/B19cXgJycHFJTU/nll184f/48n3/+uUvOJyIiIjdHCZ+L\nDRkyhJdeegmA9evX8+CDDwKwadMmmjVrVqg6jDF0796dWbNmAfbbuffee2++Yz7++GO+++47RowY\nwa+//sqKFStITEwkJiaGjIwMgoOD+fe//80zzzzDqVOnmDp1KrNmzWLs2LGkpaWxYsUKGjRoQFRU\nFJMnT+azzz4DID09nXHjxhEdHU1kZORlsc2fP5+lS5eSnp7uTG7XrVuHZVn07NmT2NhYBg8ejGVZ\nbN68mRYtWhATE0N0dPRlt7hFRETENcq5OwBPV6FCBQASEhIICQmhdu3arFy5kkceeYSsrKxCP83b\nuHFj0tPTyc3N5cyZM1SsWNG5b8GCBZw6dYru3buzc+dOtm3bxu9+9zuMMXTq1AmAkJAQ/P39+eqr\nrzh48CBJSUmMGTOGnJwcnnrqKWJiYmjQoAG9evUCoHv37jz++OO88847fP/995QtW5a//OUvPPLI\nI87zlitXjvvuuw+A5s2b06hRI3r27Em9evUAiIiIwNvbm9jYWFJSUmjTpg2tW7fG39+fTp06MXbs\n2JtvYBEREbkmJXzF4OjRo6xYsYKhQ4cC9tu827dv5/Dhw+zYsYPVq1fz0EMPXbOedu3asWTJEipV\nqpRv+7p168jKyiIuLg4fH598t3vPnDmDt7c3AEFBQdx2220sWbIEPz8/wJ60bd++HSBf8nnXXXex\nceNG0tPTKVu2LAD+/v789NNPBAQEXDXOrKwsKlWqxO7du0lNTeXw4cOcPXvWeY6L5a91W1pERESK\nhv6LWwyioqIYPHgwOTk5LF68mJdffpmwsDBCQ0O54447nMleZmZmgeUvJmLPPPMMI0eOdN7Ovbj9\ngQcewM/Pjw4dOvCnP/2JOnXq4OXlRXZ2NitXrnTWczHBatq0Kfv27QPg3Llz1KpV67Jzbd++nZYt\nWxIYGMi5c+cA2L9/P82bN893XF7ly5d3njMpKYmRI0fSu3dv6tevj81mw2azFXhdIiIi4lpK+Fxs\nwoQJDB8+nICAAAIDAwkMDATg7NmzjBkzhpSUFBITE7HZbISGhl5WPikpibFjx/K///2Pli1b8vDD\nD1O5cmVGjRrFr7/+ysSJE3n88cepUaMGUVFRREVFUbVqVYKDg0lNTSU7O5uMjAzi4+OJiIjgyJEj\nNGvWjHbt2hEVFcXYsWOZPHmy83yzZs3iyy+/pEuXLtx1111MmjSJMWPGEBUVRUhICI0bNyYqKoqE\nhAT27t3LDz/8wNy5c8nOzqZbt258++23+Pj44Ovri7e3N/Pnz8fX15eYmBj27NlDamoqU6dOJS4u\nji1bthAbG1tsvwsREZFblWbacInSOdPGY4895nzdioiIiJQ8mmlDbkpKSgo7duzg22+/dXcoIiIi\nUsTUw+cSpbOHT0REREo29fCJiIiISIGU8ImIiIh4uFKf8AUE1AdMiVr8/K7+nrpbydKlS90dQomh\ntviN2uI3aov81B6/UVv8Rm1x80p9wnfgwG4syypRy5tv9nd3s5QY+kf6G7XFb9QWv1Fb5Kf2+I3a\n4jdqi5tX6hM+EREREbk6JXwiIiIiHq7IXstSBLGIiIiIyDXcyGtZiiThExEREZGSS7d0RURERDyc\nEj4RERERD6eET0RERMTDlbuRQsaYu4F2QCVgomVZJxzbywMDgL3AdiAFCAb+ZFlWr6IIuKQoqA0K\nuP51eT9blpXspnBdqpBt4bHfhbwK2RY/A12BAGCTZVkL3RSuSxWyLdYCLwBVgG23cltc/PtgjOkE\nnLQsa7m74nW1wraHMWYjsAZ417KsfW4L2IWuoy06ArlAK8uy/um2gF2okH8zDgLRwGEgB+hqeeDD\nCNfxvXgKOAvkWpa1+KqVXuslwsAoYCgwDPjEse0zoCpwD/B/eY7tDjzlWJ+YZ/scd78MuaiXgtrg\n0uu/Unt42lKYtvDk78INfC8eAv7i+Bzt7pjd3BZ3OP6AtQIGujtmd7aF42c5YDzwO3fHXELaI8Td\nsZaEtgD8L/7N8OSlkG3R2LFeDXjI3TG7uS2qAm86Pve9Vp2FuaU7xrKsjyzL+gcw1rGtlmVZJ7Fn\n2nflObYBcMCx7leIukuzmgW0waXXH5Tns2+xRle8CtMWt4prtYWvZVmrLcv63BjjA/zqjiCLSWHa\nYgcwDegMTCn+EIvNNdvC8fNhIKmYY3OHwrbHI8aYrsaY+4s7wGJUmL+fjwKBxphHjDFd3RBjcSnM\n34w0x3qwZVmrizvAYlSYtjgJtDbGfAtcsy1udAxfbp7yF/JsL3vJZ092sQs5bxtcev15P1/3O3NK\nkcK0xa3iWm2R93vwB2BEMcXlDoVqC8uyDgETsPdseaprtoUxpjpwrLgDc5PC/jv5p2VZPwCvFmNs\nxa0wfz+9sN/CWwE8W4yxFbdCfS+MMd7Yh8R4ssL+zfgGWAD0vVaFhUn4TAHr+x0NXhvYnWd/OnC7\nMaYMkHWFOjxFQW2wjfzXf+lnT1WYtrjIE78LeRWqLYwxDwJzsXfJe6prtoUx5iljzAvAeey3dz1V\nYb4XzbC3wf1AW3cEWYwK891oD7Q3xlTBs/9uFOa7sTnP8TnFG16xKux/Sx4EThV/eMXqWm1xBnvP\n7zLLsiZhf2bgqq754mVjzCjs/9dpgGqWZb1tjLkL+wB8f+AroC7QGogA3sTeBbnbsqxEY0xfYBD2\nwforrutyS7BL2mAR9j/SEeS5fuy3ZvK1hztidbXCtIUnfxfyKuT3Yh/wLfbbuT9bljXQHbG6WiHb\nYjvwCBAIbLauNei4lLqOfyOVgU+BQ5ZlveumcF2ukN+N9di/G3WBRY7b/x7nOr4brwGHgBOWZcW6\nKVyXuo626Aac8dR2gEL/G1kH9AR+AX61LGvNVeu8VsInIiIiIqWb3sMnIiIi4uGU8ImIiIh4OCV8\nIiIiIh5OCZ+IiIiIh1PCJyIiIuLhlPCJiIiIeDglfCKlmDHm98aYDcaYScaYOsaY54wx+40x/zTG\nFMtLnY0xZRznf9fxOcIY09qx7m+MecsY870xppIxZogx5htjjP8ldTxtjBlgjClVM4/kvdYC9tU2\nxgw0xtS7ifq7GGN+vvEIRUTs9B4+kVLOGDMJiLj4Ym9jzBKgj2VZvxRjDMHY57Z83xhzm2VZvzq2\nD8A+7Y8FNAHOYn/Z8nYrzx8fY8xXQH+gnmVZO4sr7puV91qvsP8/wPeXvnTdGBNqWVZ0Ic+xxLKs\nkJsMVURucerhEyn9Lp12ym3TUBljygH35+nV8gFOOyY8v7i+zbr8/zTLWpaVU8qSPS/s11r/KofZ\nCihXF3jmek51vbGJiFyqnLsDEBHXMcaMBpZin3Px30AFIAr7VD0/Y5+zdRDwR+AvwAdAcyDRsqyl\nxpg+wH6gg2VZf7uk7vLAW0AK8P+AXMeuzkBNY8xC7NMBVTLGTAN+BzxqjDlhWda6PPUEAw87pt47\nCPwd+BxoA/wNe8/fduBO7NOOvYw9YRqPfeqhJOzzSj4J/M2yrDN56i4DTAO2YJ+WaKYj5iPADOBt\noDJwr2VZw40x44DGwCfAOOBp4BXsc/0OBt4D1gJ3AaMvXisQ6bilvQFoh33avMmOMFo6bq93sizr\ndaAV8IDjeqcDVbBPl7QdsFmWNdsY0xt7b+g5x/58jDHGUXYS8BgwBPs0ZN8ALzp+Z1OAPwAtgWrY\npygb6zhXM+zzs6Zhn4w9wHGeJZZlrTLGPOqI8wL23tkzwHIgFDgBpFiWterSuESkBLMsS4sWLaV4\nwZ5YfASEAS9h/w95Pcc+b8fPp4G3Het/B9o51gcAf3asL3X89MI+j2lV7ElRLewJ0aXn/RB7Igjw\nBDDCsf4SEJbnXL+7dHsBdS3Js74Fe2J6m6N8G8f2F4CXHeubHD8fB8Y51v8BPFhA3cF5YhuRJ55J\nQOtLrt0b+MGxngj4YU/UwJ54/tGx/l/siV7ea41z/Pw3cHee6/9D3nMUcL3TgTsd698BdwNT8+xf\ndYU2u/i7/U+eNnrXcb3VsCfzftjnoQV7AjcYqIf9fwK8sCd55bH3IvoAMXnqbIo9+f+nY9syoBJQ\nFpjh7u+9Fi1arm/RLV0Rz7DAsqxIy7KmAAfybK9sjHkR+/i5inm2X+yN2wLc61i3ACzLysbeo+MN\nzMXeazSwgHO2AnbkLVtEDlqWdc6yj41rg703DuwTx198QOKg4+f5S9YrXKNuc8n6xbK5AJa9d/Cc\nMaYlMAd7onzx7+R9wG3GmI7ALvK3J0C8MeZJIMGyrK15ryfvOfLGYYyp5Ki3kaPeTdh75HblOfZK\nbRvk6AmskSeW6UAv7D2uK7AnjxhjOgB1gEzHuXdblpVtWdYpR7ke2BPEi+2XgL13tgnwL8e2ho5j\nfo/9fwhEpBTRLV0Rz3MxmfDG/sBEMPAw8EiesXVlHT8bAz/l3eYYh+eFvefnkGVZwcaYd40xvpZl\nHc9znrXA7diTvvLXE9s1tudNcDZivxW5E3vCsqGA4681xi0nzzG3FeL8Mdhv9YZhvwU82rF9DXDO\nsqxYY0wC9t6uvI5ZlvXjVeLId42Odm7lqPdny7L2GmM2Ar7Yf2cXeV1WkTEPAgMty+ppjGkCVDPG\n+FiWtd0YE4S9988yxqRhHzcZ5yhXy1Ff3jb+3nG9acBbju9IgGVZX1xy2s3YeynPO+IUkVJECZ9I\nKWaM+T3wAPZeqZ3AQ9h7Yv4C/BPYhv12K9jHnVV3rIcYY6oD/pZlhTu2BTp6qFoBf8bes9XJkTge\nviTZA/t4tqHGGD/sPUoPGWMmA12BC8aYpdjHl9UyxuzG3ltmGWMSLcty9mAZY54AGjt6Io8ATY0x\nLzl6K0cCA4wxgcBtlmX9xxjzB+BOY0wboLdjvXWecy2zLCtvQrMeeNYY8xT2XsswY8wR7D1YLxpj\n1gBNjDEdLcuKxd6readlWdnGmAxgJYBlWV85XjHTE3ty/GOea52OfRxiJ+wPasxwnLcdUNcYY3PE\n+aJlWVMd+57Hnmy9DrxhjNkCZFmWNd8Yk+Joj9NADWNMZ8uy5ue5puPAGWNMZ8d6J+y3aXHEtdsR\n83FjzHhjTH9H2/6MvTevtTHmd5b96eF12Md4NsDeA9gYOOJ42vuIY/+n2Md5DjLGbMU+RnE/IlJq\n6LUsIrcYY8zfgXjr8leFxFuW9ZibwirVHA+elLcsa6HjHYNjgP6WZV32lG5pYIz5APt4x/LYE/VA\ny7LGuDcqEbkZ6uETuYUYY2pjv10YgP2hhIvbewF3GGNaWpa11l3xlWJpQDdHj1slIKm0JnsOW7H3\nXuYAtbH3GopIKaYePhEREREPp6d0RURERDycEj4RERERD6eET0RERMTDKeETERER8XBK+EREREQ8\nnBI+EREREQ/3/wG2skITfxGAsgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x107b833c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NB_on_NYT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:9: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:18: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 49910 unique words in this corpus\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "top positive words:\n",
      "trek: 22.23155849829559\n",
      "rocky: 22.231558498295588\n",
      "mallory: 19.452613686008643\n",
      "shrek: 17.599983811150675\n",
      "contact: 16.673668873721688\n",
      "marie: 16.673668873721688\n",
      "altman: 14.821038998863726\n",
      "julianne: 14.821038998863724\n",
      "animated: 11.115779249147796\n",
      "lebowski: 11.115779249147796\n",
      "--------------------------------------------------------------------------------\n",
      "top negative words:\n",
      "mars: 18.35228960809382\n",
      "horrible: 17.27274316055889\n",
      "gorilla: 16.193196713023966\n",
      "cindy: 16.193196713023966\n",
      "batman: 13.674255002109119\n",
      "fault: 12.95455737041917\n",
      "bats: 12.954557370419169\n",
      "eastwood: 12.954557370419169\n",
      "godzilla: 12.414784146651701\n",
      "martha: 11.87501092288424\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "top positive words:\n",
      "mallory: 0.0023184565606258548\n",
      "marie: 0.001476974524356019\n",
      "altman: 0.0014371265400795635\n",
      "julianne: 0.0013892223220769112\n",
      "dinosaurs: 0.001173216901082185\n",
      "apostle: 0.0010833532309152843\n",
      "friend's: 0.001044187783182919\n",
      "fiona: 0.0010098516458492538\n",
      "donkey: 0.001005630306932223\n",
      "mickey: 0.000949805022893053\n",
      "--------------------------------------------------------------------------------\n",
      "top negative words:\n",
      "gorilla: 0.0013689842437767507\n",
      "cindy: 0.0013689842437767507\n",
      "squad: 0.0012020041196654269\n",
      "eastwood: 0.0010435049291860903\n",
      "suzie: 0.0010144084962846044\n",
      "poison: 0.0009738321564332202\n",
      "freeze: 0.0009562156304816331\n",
      "bats: 0.0009540616495415682\n",
      "martha: 0.0008934325429288263\n",
      "varsity: 0.0007963000592209096\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "top positive words:\n",
      "trek: 20.65798228822203\n",
      "rocky: 20.65798228822203\n",
      "contact: 15.493486716166519\n",
      "younger: 8.607492620092511\n",
      "goodman: 8.607492620092511\n",
      "masterpiece: 8.60749262009251\n",
      "subtle: 7.74674335808326\n",
      "mature: 7.74674335808326\n",
      "cuba: 7.74674335808326\n",
      "religion: 7.746743358083259\n",
      "--------------------------------------------------------------------------------\n",
      "top negative words:\n",
      "mars: 19.75023476676218\n",
      "horrible: 18.58845625107029\n",
      "batman: 14.715861198763976\n",
      "schumacher: 11.617785156918933\n",
      "product: 11.61778515691893\n",
      "poorly: 11.61778515691893\n",
      "arnold: 10.456006641227036\n",
      "blade: 9.87511738338109\n",
      "decent: 9.294228125535145\n",
      "coach: 9.294228125535145\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "top positive words:\n",
      "performed: 0.01123388374186558\n",
      "mrs: 0.010485540655490002\n",
      "degree: 0.009588007754898523\n",
      "president: 0.007565684593222708\n",
      "father: 0.00723685511905485\n",
      "j: 0.0058295708952366965\n",
      "college: 0.0046096088996417095\n",
      "church: 0.004150110245260737\n",
      "st: 0.004137585853175096\n",
      "mother: 0.003540597850654588\n",
      "--------------------------------------------------------------------------------\n",
      "top negative words:\n",
      "york: 0.018547998873680714\n",
      "n: 0.017382963511169386\n",
      "university: 0.015043267117001272\n",
      "received: 0.00890507909363634\n",
      "mr: 0.005026620895149894\n",
      "manhattan: 0.004572419673205527\n",
      "new: 0.004540998315428337\n",
      "dr: 0.004331090216340459\n",
      "medical: 0.00402728199709851\n",
      "evening: 0.0033536739166692927\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "society score=0.002358306454522563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:117: RuntimeWarning: invalid value encountered in true_divide\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:118: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "mr,tr = NB_wordshift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[22.231558498295591, 'trek'],\n",
       "  [22.231558498295588, 'rocky'],\n",
       "  [19.452613686008643, 'mallory'],\n",
       "  [17.599983811150675, 'shrek'],\n",
       "  [16.673668873721688, 'contact'],\n",
       "  [16.673668873721688, 'marie'],\n",
       "  [14.821038998863726, 'altman'],\n",
       "  [14.821038998863724, 'julianne'],\n",
       "  [11.115779249147796, 'animated'],\n",
       "  [11.115779249147796, 'lebowski']],\n",
       " [[18.35228960809382, 'mars'],\n",
       "  [17.272743160558889, 'horrible'],\n",
       "  [16.193196713023966, 'gorilla'],\n",
       "  [16.193196713023966, 'cindy'],\n",
       "  [13.674255002109119, 'batman'],\n",
       "  [12.95455737041917, 'fault'],\n",
       "  [12.954557370419169, 'bats'],\n",
       "  [12.954557370419169, 'eastwood'],\n",
       "  [12.414784146651701, 'godzilla'],\n",
       "  [11.87501092288424, 'martha']],\n",
       " [[0.0023184565606258548, 'mallory'],\n",
       "  [0.0014769745243560189, 'marie'],\n",
       "  [0.0014371265400795635, 'altman'],\n",
       "  [0.0013892223220769112, 'julianne'],\n",
       "  [0.0011732169010821849, 'dinosaurs'],\n",
       "  [0.0010833532309152843, 'apostle'],\n",
       "  [0.0010441877831829189, \"friend's\"],\n",
       "  [0.0010098516458492538, 'fiona'],\n",
       "  [0.0010056303069322231, 'donkey'],\n",
       "  [0.00094980502289305298, 'mickey']],\n",
       " [[0.0013689842437767507, 'gorilla'],\n",
       "  [0.0013689842437767507, 'cindy'],\n",
       "  [0.0012020041196654269, 'squad'],\n",
       "  [0.0010435049291860903, 'eastwood'],\n",
       "  [0.0010144084962846044, 'suzie'],\n",
       "  [0.00097383215643322021, 'poison'],\n",
       "  [0.00095621563048163311, 'freeze'],\n",
       "  [0.00095406164954156818, 'bats'],\n",
       "  [0.00089343254292882635, 'martha'],\n",
       "  [0.00079630005922090965, 'varsity']]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_table(my_list,my_fname):\n",
    "    f = open(my_fname,\"w\")\n",
    "    for j in range(10):\n",
    "        table_strings = [my_list[i][j] for i in range(4)]\n",
    "        table_strings = [\"{0:.2f} & {1}\".format(my_list[i][j][0],my_list[i][j][1]) for i in range(2)]+[\"{0:.4f} & {1}\".format(my_list[i][j][0],my_list[i][j][1]) for i in range(2,4)]\n",
    "        full_string = \" & \".join(table_strings)\n",
    "        f.write(full_string)\n",
    "        f.write(r\" \\\\\")\n",
    "        f.write(\"\\n\")\n",
    "    f.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_table(mr,\"../figures/NB/output-003-table-MR.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_table(tr,\"../figures/NB/output-003-table-TR.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:9: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:18: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 49910 unique words in this corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:125: RuntimeWarning: invalid value encountered in true_divide\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:126: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "mr,tr = NB_wordshift(q=True)\n",
    "to_table(mr,\"../figures/NB/output-004-table-MR.tex\")\n",
    "to_table(tr,\"../figures/NB/output-004-table-TR.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:9: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:18: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 49910 unique words in this corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:125: RuntimeWarning: invalid value encountered in true_divide\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:126: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "mr,tr = NB_wordshift(q=True)\n",
    "to_table(mr,\"../figures/NB/output-005-table-MR.tex\")\n",
    "to_table(tr,\"../figures/NB/output-005-table-TR.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:9: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:18: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 49910 unique words in this corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:125: RuntimeWarning: invalid value encountered in true_divide\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel/__main__.py:126: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "mr,tr = NB_wordshift(q=True)\n",
    "to_table(mr,\"../figures/NB/output-006-table-MR.tex\")\n",
    "to_table(tr,\"../figures/NB/output-006-table-TR.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
